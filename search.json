[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In this section, we are presented with the opportunity to leverage more intricate models, such as ARIMAX, SARIMAX, or VAR, to better understand the time series of greenhouse gas concentrations. We will persist in utilizing the variables of carbon dioxide, methane, nitrous oxide, and sulphur hexafluoride, as provided by the Global Monitoring Laboratory at NOAA. As previously indicated, these time series are not isolated phenomena. Numerous greenhouse gas emitters discharge more than one of the aforementioned gases, and all are generally associated with increased industrialization. Therefore, to simultaneously predict alterations in these interconnected variables over time, we will employ a VAR model in this segment.\n\n\nVector Autoregression (VAR) is a multivariate forecasting algorithm widely used in time series data analysis. It enables predictions of changes in several interdependent variables over time. Unlike univariate autoregressive models, which analyze one series independently, VAR explores multiple series in unison, encapsulating the linear interdependencies among them.\n\n\n\n\n\nIn a VAR model, each variable is represented by an equation that illustrates its evolution over time. This equation is derived from the variable’s own prior values (lags), as well as the lags of all other variables under consideration. The model also incorporates an error term to account for variations not explained by the time-lagged values. This integration allows the VAR model to comprehend and encapsulate the dynamic interplay between different variables.\nVAR models are particularly advantageous in fields where numerous variables interrelate over time, such as economics and natural sciences. They offer simplicity and adaptability, necessitating minimal prior knowledge about the variables’ influencing factors. The sole prerequisite is a set of variables hypothesized to affect one another over time.\nWhile the primary aspects of VAR are underlined above, it’s crucial to note that VAR models possess certain limitations. As linear models, they may struggle to capture intricate non-linear relationships. They also presume that the relationships between variables remain constant over time, which might not always hold true. Lastly, VAR models, when handling a multitude of variables and lags, can involve numerous parameters, leading to potential overfitting issues. Despite these constraints, VAR models remain an instrumental tool in multivariate time series analysis.\n\n\n\n\n\n\n\n\n\nBased on the plots and background information, we can expect CO2, CH4, N2O, and SF6 to have interrelationships since they are all greenhouse gases and their concentrations may be influenced by similar factors such as human activity and natural processes. From my domain knowledge, I know that most pollution sources contribute more than one type of greenhouse gas to the atmosphere. Thus, it is reasonable to include all four of these time series in the VAR model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot all time series together\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nplot.ts(x , main = \"\", xlab = \"\")\n\n\n\nUsing VAR selection methods, we have found several possible values for p. These are 6 and 10. This is surprising as I would have thought 12 would be a good choice, as that is the length of the seasonal lag. We will test these new p choices along with the standard p=1 and p=2 choices that will give us something to compare to.\n\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nvar_lag_selection <- VARselect(x, lag.max=12, type=\"both\")\nprint(var_lag_selection)\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10     10      6     10 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -1.477896e+01 -1.664075e+01 -1.780180e+01 -1.809010e+01 -1.853678e+01\nHQ(n)  -1.464208e+01 -1.641262e+01 -1.748243e+01 -1.767947e+01 -1.803490e+01\nSC(n)  -1.443895e+01 -1.607407e+01 -1.700845e+01 -1.707007e+01 -1.729008e+01\nFPE(n)  3.815889e-07  5.930499e-08  1.857741e-08  1.393205e-08  8.920559e-09\n                   6             7             8             9            10\nAIC(n) -1.884106e+01 -1.887424e+01 -1.916644e+01 -1.941668e+01 -1.954365e+01\nHQ(n)  -1.824794e+01 -1.818987e+01 -1.839081e+01 -1.854980e+01 -1.858552e+01\nSC(n)  -1.736769e+01 -1.717420e+01 -1.723972e+01 -1.726329e+01 -1.716358e+01\nFPE(n)  6.588292e-09  6.383928e-09  4.776846e-09  3.729662e-09  3.296301e-09\n                  11            12\nAIC(n) -1.952806e+01 -1.953284e+01\nHQ(n)  -1.847869e+01 -1.839222e+01\nSC(n)  -1.692133e+01 -1.669943e+01\nFPE(n)  3.362150e-09  3.362979e-09\n\n\n\n# Fit different VAR models based on the selected lags\nfitvar1 = VAR(x, p=1, type=\"both\")\nfitvar2 = VAR(x, p=2, type=\"both\")\nfitvar6 = VAR(x, p=6, type=\"both\")\nfitvar10 = VAR(x, p=10, type=\"both\")\n\n\n\n\nUsing cross-validation, we see that the model with the lowest MSE is the p=10 model. Thus, we will be selection this as our best model\n\ncv_var <- function(ts_data, p, k) {\n  n <- length(ts_data[, 1])\n  start_ts <- tsp(ts_data)[1] + (k - 1) / 12\n  rmse <- numeric(n - k)\n  \n  for (i in 1:(n - k)) {\n    train_data <- window(ts_data, end = start_ts + (i - 1) / 12)\n    test_data <- window(ts_data, start = start_ts + (i - 1) / 12 + 1 / 12, end = start_ts + i / 12)\n    var_model <- VAR(train_data, p = p, type = \"const\")\n    forecast <- predict(var_model, n.ahead = 1)$fcst\n    \n    k_step_forecast <- matrix(0, nrow = 1, ncol = ncol(ts_data))\n    for (j in 1:ncol(ts_data)) {\n      k_step_forecast[, j] <- forecast[[j]][1, 1] # Extract the 1-step ahead forecasts\n    }\n    \n    rmse[i] <- sqrt(mean((test_data - k_step_forecast)^2))\n  }\n  \n  return(mean(rmse, na.rm = TRUE))\n}\n\n\n# Calculate RMSE for each model using cross-validation\nk <- 12 # Number of steps ahead for the forecasts\nRMSE_lag1 <- cv_var(x, p = 1, k = k)\nRMSE_lag2 <- cv_var(x, p = 2, k = k)\nRMSE_lag6 <- cv_var(x, p = 6, k = k)\nRMSE_lag10 <- cv_var(x, p = 10, k = k)\n\n\n\n\n\n\n\n\n\nThe selected model, VAR(10), was chosen based on its lowest RMSE value obtained through cross-validation, which suggests that this model provides the best fit for the data among the tested models. This model can be utilized to predict future concentrations of CO2, CH4, N2O, and SF6, all of which are greenhouse gases. By analyzing these forecasts, we can gain insights into the potential future trends of these greenhouse gas concentrations. It is important to note, however, that the accuracy of these forecasts depends on the underlying assumptions of the VAR model and the quality of the historical data. As such, the results should be interpreted with caution, considering possible uncertainties and changes in factors influencing greenhouse gas concentrations."
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this section, we will delve into the application of ARMA, ARIMA, and SARIMA models to better understand the time series of greenhouse gas concentrations. We will continue to focus on variables such as carbon dioxide, methane, nitrous oxide, and sulfur hexafluoride, provided by the Global Monitoring Laboratory at NOAA. These time series are interconnected, as many greenhouse gas emitters release multiple gases, and all are generally associated with increased industrialization. To effectively analyze the relationships and trends in these time series, we will employ these models to capture temporal dependencies and seasonal patterns."
  },
  {
    "objectID": "arma.html#background",
    "href": "arma.html#background",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Background",
    "text": "Background\nARMA (Autoregressive Moving Average) and ARIMA (Autoregressive Integrated Moving Average) are widely used forecasting algorithms in time series data analysis, enabling the prediction of changes in a single variable over time while considering its past values and associated error terms. The ARMA model comprises the autoregressive (AR) part, representing the dependency on previous values, and the moving average (MA) part, accounting for the dependency on previous error terms. The ARIMA model extends the ARMA framework by incorporating the Integrated (I) component, which involves differencing the data to achieve stationarity, making it suitable for handling non-stationary time series data.\nThe Seasonal ARIMA (SARIMA) model further expands the capabilities of the ARIMA model by addressing seasonality in the data. It includes additional seasonal autoregressive, differencing, and moving average components, enabling it to capture both regular and seasonal patterns in the time series. Despite some limitations, such as the assumption of constant relationships over time and potential difficulties in capturing complex non-linear relationships, ARMA, ARIMA, and SARIMA models remain essential tools in univariate time series analysis. They offer a solid foundation for understanding the behavior of greenhouse gas concentrations over time, providing valuable insights into the evolving state of our environment."
  },
  {
    "objectID": "arma.html#arima-sf6-concentration",
    "href": "arma.html#arima-sf6-concentration",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ARIMA: SF6 Concentration",
    "text": "ARIMA: SF6 Concentration\n\n\n\nFor this section, we will be using an ARIMA model to forecast future atmospheric concentrations of sulfur hexaflouride.\n\n\n\n\n\n\n\n\n\n\nSF6 Concentration (No differencing)\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\nSF6 Concentration (1st order difference)\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\nSF6 Concentration (2nd order difference)\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot.\n\n\nParameter Selection\nBased on the PACF and ACF plots, I will be modeling the twice-differenced \\(SF_6\\) concentration because I want to ensure that our series is stationary. Based on the ACF and PACF plots of the twice differenced \\(SF_6\\) concentration shown above, I will select 1 and 3 as possible choices for \\(p\\) and 1,2,3, and 4 as possible choices for \\(q\\). Since this data is twice differenced, I will be using a \\(q\\) value of 2.\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n-1926.281\n-1922.574\n-1926.268\n\n\n0\n2\n1\n-1972.627\n-1965.213\n-1972.587\n\n\n0\n2\n2\n-2017.168\n-2006.047\n-2017.087\n\n\n0\n2\n3\n-2024.154\n-2009.325\n-2024.019\n\n\n2\n2\n0\n-1943.242\n-1932.120\n-1943.161\n\n\n2\n2\n1\n-2021.723\n-2006.894\n-2021.588\n\n\n2\n2\n2\n-2042.100\n-2023.564\n-2041.896\n\n\n2\n2\n3\n-2026.081\n-2003.839\n-2025.796\n\n\n\n\n\nBased on the table, the model with the lowest AIC and BIC scores is the 2,2,2 model. None of the other models are that close to this one in terms of AIC and BIC scores. For the sake of comparison, I will select the worst model of the bunch (0,2,0) to show that even this is very similar to our best model. All of the option here would be adequate for forecasting this series.\n\n\nModel Diagnostics\nFor a (2,2,2) ARIMA model, the equation is:\n\\[y(t) = c + ϕ(1)*y(t-1) + ϕ(2)*y(t-2) - θ(1)*ε(t-1) - θ(2)*ε(t-2) + ε(t)\\]\nwhere:\n\n\\(y(t)\\) is the value of the time series at time t\n\\(c\\) is a constant term (i.e., the mean of the series)\n\\(ϕ(1)\\) and \\(ϕ(2)\\) are the autoregressive coefficients for lags 1 and 2, respectively\n\\(θ(1)\\) and \\(θ(2)\\) are the moving average coefficients for lags 1 and 2, respectively\n\\(ε(t)\\) is white noise (i.e., a random error term) with mean zero and constant variance\n\nNow, lets consider the model diagnostics using standardized residuals.\n\n\n\n\n\nFor the most part, the residuals plots looks as we would expect. The residuals have a constant mean and variance, and the lagged p-values for the Ljung-Box statistic are under the 0.05 threshold for all except two of the lags. Something that does still appear is a spike in ACF residuals at lags 12 and 24. This would suggest that the series may not have been adequately stationary prior ro modeling. However, we mentioned this earlier, and it wsa determined that twice-differenced was the proper metric to use, as three-times differences would over difference the data and lose valuable insights.\nNext, we can plot the raw data, our chosen model (2,2,2), and the second model (0,2,0) on the same plot to see how they compare on the training data. As expected, both models are nearly identical to the SF6 data. The lines are directly on top of each other and it is impossible to make out any difference between the models and the data.\n\n\n\n\n\n\n\nAuto-arima\nNext, we can compare our ARIMA(2,2,2) model to the auto-arima model. Thankfully, the auto-arima function provides the same model as the one we came to through traditional means. This is validating because there were many choices for p,d, and q, so it is nice to know that the best model was selected in both methods.\n\n\nSeries: SF6_Concentration \nARIMA(2,2,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      1.2422  -0.5007  -1.8154  0.8326\ns.e.  0.0653   0.0550   0.0495  0.0499\n\nsigma^2 = 6.44e-05:  log likelihood = 1026.05\nAIC=-2042.1   AICc=-2041.9   BIC=-2023.56\n\n\n\n\n10-year Forecast\nFinally, we can use the ARIMA model to forecast the SF6 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(2,2,2) model.\n\n\n\n\n\n\n\nBenchmark Methods\nIn order to determine the effectiveness of our model, we need to compare to other benchmark methods. Based on the graphs and accuracy tests below, it is clear that the ARIMA(2,2,2) model far outperforms any of the benchmark methods. Thus, we can be confident when using this model going forward.\n\n\n\n\n\n\npred1=forecast(fit1, h=120);\naccuracy(pred1)\n\n                       ME       RMSE         MAE       MPE       MAPE\nTraining set 0.0007896781 0.00794485 0.006303769 0.0115696 0.09585017\n                   MASE        ACF1\nTraining set 0.02238707 -0.04714439\n\n\n\nf1 <- meanf(SF6_Concentration, h=120)\naccuracy(f1)\n\n                       ME     RMSE     MAE       MPE    MAPE     MASE     ACF1\nTraining set 1.282536e-16 2.071243 1.79491 -9.131477 27.7452 6.374407 0.990158\n\n\n\nf2 <- naive(SF6_Concentration, h=120)\naccuracy(f2)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292\n\n\n\nf3 <- rwf(SF6_Concentration, h=120)\naccuracy(f3)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292"
  },
  {
    "objectID": "arma.html#sarima-co2-concentration",
    "href": "arma.html#sarima-co2-concentration",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA: CO2 Concentration",
    "text": "SARIMA: CO2 Concentration\nIn this section, we will use a Seasonal Autoregressive Integrated Moving Average (SARIMA) model to forecast future atmospheric concentrations of carbon dioxide. As the graph below illustrates, the CO2 concentration, though representing a global mean, exhibits a strong seasonal pattern. Therefore, we will employ a SARIMA model for our analysis.\n\n\n\n\n\n\n\n\n\n\nCO2 Concentration (No differencing)\n\n\n\n\n\nThe above plots display the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for the CO2 concentration time series. As anticipated, the ACF plot reveals a strong correlation between values, signifying that this series is non-stationary. This aligns with our previous assumption that the series is non-stationary due to its pronounced seasonality. To achieve stationarity, we will apply differencing.\n\nautoplot(decompose(CO2_Concentration))\n\n\n\n\n\n\nMonthly Lags\nBefore performing differencing, we examine the monthly lag plots to determine the length of the seasonality. Based on domain knowledge, we suspect a 12-month lag, as the seasonal pattern recurs annually. The lag plots below confirm this hypothesis. The Month 12 lag plots exhibit an almost perfect y=x line, which is superior to all other lags up to 12 months.\n\ngglagplot(CO2_Concentration, do.lines=FALSE, lags=12)+xlab(\"Xi\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Monthly CO2_Concentration (1958-2023)\")\n\n\n\n\n\n\nTwice Differences (Once Seasonal and Once Standard)\nHaving identified the seasonal lag, we can now use differencing to make the series stationary. We apply one seasonal differencing and one standard differencing to achieve stationarity. The resulting ACF and PACF plots exhibit no seasonal pattern or serial autocorrelation, indicating that our data is now differenced and ready for parameter selection and model creation.\n\nCO2_Concentration %>% diff(lag=12) %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\nParameter Selection\nUsing the ACF and PACF plots of the differenced data, we can identify potential values for the parameters p, P, q, and Q. Note that d and D have already been determined based on the differencing performed in the previous step. The ACF and PACF plots suggest the following possible values for each parameter:\n\nq = 0,1\nQ = 0,1\np = 0,1,2,3\nP = 0,1,2,3\nd = 1\nD = 1\n\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*25),nrow=25)\n  \n  for (p in p1:p2){\n    for(q in q1:q2){\n      for(P in P1:P2){\n        for(Q in Q1:Q2){\n          if(p+d+q+P+D+Q<=9){\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=2,P1=1,P2=4,Q1=1,Q2=2,data=CO2_Concentration)\nknitr::kable(output)\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n901.4395\n906.0794\n901.4448\n\n\n0\n1\n0\n0\n1\n1\n502.4804\n511.7602\n502.4962\n\n\n0\n1\n0\n1\n1\n0\n708.5527\n717.8325\n708.5685\n\n\n0\n1\n0\n1\n1\n1\n503.8839\n517.8035\n503.9154\n\n\n0\n1\n0\n2\n1\n0\n638.8982\n652.8178\n638.9297\n\n\n0\n1\n0\n2\n1\n1\n505.8830\n524.4425\n505.9356\n\n\n0\n1\n0\n3\n1\n0\n608.7436\n627.3032\n608.7963\n\n\n0\n1\n1\n0\n1\n0\n808.8603\n818.1401\n808.8761\n\n\n0\n1\n1\n0\n1\n1\n413.9145\n427.8341\n413.9460\n\n\n0\n1\n1\n1\n1\n0\n610.7566\n624.6763\n610.7882\n\n\n0\n1\n1\n1\n1\n1\n415.9110\n434.4705\n415.9636\n\n\n0\n1\n1\n2\n1\n0\n524.8992\n543.4587\n524.9518\n\n\n1\n1\n0\n0\n1\n0\n828.5434\n837.8231\n828.5591\n\n\n1\n1\n0\n0\n1\n1\n433.4853\n447.4049\n433.5168\n\n\n1\n1\n0\n1\n1\n0\n630.4665\n644.3862\n630.4981\n\n\n1\n1\n0\n1\n1\n1\n435.3995\n453.9590\n435.4522\n\n\n1\n1\n0\n2\n1\n0\n551.4616\n570.0211\n551.5143\n\n\n1\n1\n1\n0\n1\n0\n806.1513\n820.0710\n806.1829\n\n\n1\n1\n1\n0\n1\n1\n410.7134\n429.2729\n410.7660\n\n\n1\n1\n1\n1\n1\n0\n609.6100\n628.1695\n609.6627\n\n\n2\n1\n0\n0\n1\n0\n819.1535\n833.0731\n819.1850\n\n\n2\n1\n0\n0\n1\n1\n423.5954\n442.1550\n423.6481\n\n\n2\n1\n0\n1\n1\n0\n621.1240\n639.6835\n621.1767\n\n\n2\n1\n1\n0\n1\n0\n807.3255\n825.8850\n807.3782\n\n\n3\n1\n0\n0\n1\n0\n810.3586\n828.9181\n810.4113\n\n\n\n\n\nAccording to the AIC and BIC scores, the best model is the (1,1,1)(0,1,1) model, as it has the lowest AIC and BIC scores. Several other models also have similar AIC and BIC scores, but upon comparing the top models, the (1,1,1)(0,1,1) model is indeed the best one. However, all the options here would provide adequate forecasts for this series, as their performance is very similar.\n\n\nModel Diagnostics\nNow, let’s examine the model diagnostics using standardized residuals.\n\nmodel_output <- capture.output(sarima(CO2_Concentration,1,1,1,0,1,1,12))\n\n\n\n\n\nfit1 <- Arima(CO2_Concentration, order=c(1,1,1), seasonal=c(0,1,1))\nsummary(fit1)\n\nSeries: CO2_Concentration \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2164  -0.5793  -0.8629\ns.e.  0.0880   0.0738   0.0184\n\nsigma^2 = 0.09906:  log likelihood = -201.36\nAIC=410.71   AICc=410.77   BIC=429.27\n\nTraining set error measures:\n                     ME      RMSE       MAE         MPE       MAPE      MASE\nTraining set 0.02449999 0.3114867 0.2426831 0.006651617 0.06804522 0.1498467\n                     ACF1\nTraining set -0.003090451\n\n\n\n\nCross-Validation\nTo further validate our SARIMA(1,1,1)(0,1,1) model, we will perform a one-step-ahead cross-validation. In this approach, we iteratively train the model on a portion of the data and then use the model to forecast the next point. We compare this forecast to the actual data and calculate the error. This process is repeated for each data point in the time series, and the overall accuracy of the model is assessed using the mean squared error (MSE) of the one-step-ahead forecasts.\n\n# Cross-validation function\none_step_ahead_cv <- function(data, order, seasonal_order) {\n  n <- length(data)\n  errors <- numeric(n)\n  \n  for (t in (length(seasonal_order) + 2):n) {\n    train_data <- window(data, end = t - 1)\n    test_data <- data[t]\n    if (length(train_data) > 0) {\n      model <- Arima(train_data, order = order, seasonal = seasonal_order)\n      forecasted_value <- forecast(model, h = 1)$mean\n      errors[t] <- (forecasted_value - test_data)^2\n    }\n  }\n  \n  mean(errors, na.rm = TRUE)\n}\n\n\n# Perform cross-validation\n# mse <- one_step_ahead_cv(data = CO2_Concentration, order = c(1, 1, 1), seasonal_order = c(0, 1, 1))\n# mse\n\n\n\nBenchmark Methods and Forecast\nTo evaluate the effectiveness of our model, we will compare it to other benchmark methods. The graphs and accuracy tests below demonstrate that the SARIMA(1,1,1)(0,1,1) model significantly outperforms any of the benchmark methods. This gives us confidence in using this model for our forecasts.\n\nautoplot(CO2_Concentration) +\n  autolayer(meanf(CO2_Concentration, h=120),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(CO2_Concentration, h=120),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(CO2_Concentration, h=120),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(CO2_Concentration, h=120, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit1,120), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\nFinally, we can use the SARIMA model to forecast the CO2 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(1,1,1)(0,1,1) model.\n\nfit1 %>% forecast(h=120) %>% autoplot()\n\n\n\n\nIn conclusion, we have successfully built and validated a SARIMA(1,1,1)(0,1,1) model for forecasting future atmospheric concentrations of carbon dioxide. This model effectively accounts for the seasonality present in the CO2 concentration data and outperforms the benchmark methods. Utilizing this model, we can now generate forecasts of CO2 concentrations over the next 10 years and contribute to the understanding of potential future trends in atmospheric CO2 levels."
  },
  {
    "objectID": "conc.html",
    "href": "conc.html",
    "title": "Conclusions",
    "section": "",
    "text": "In conclusion, my project “Exploring Atmospheric Concentrations of Greenhouse Gases with Time Series Analysis” has revealed many insights concerning greenhouse gas concentrations and their possible consequences on our climate. Throughout this exploration, different time series techniques have been employed to determine relationships between four greenhouse gases, carbon dioxide, methane, nitrous oxide, and sulfur hexafluoride. One of the most important findings was one of the simplest, despite coming from many of the same sources and generally following an increasing trend, the are serious differences between the four greenhouses in terms of seasonality ad short term trends. These differences can have severe environmental impacts beyond the basic understanding of just CO2 increases. Mental lenity could be pointed toward the continuous increase of sulfur hexafluoride (SF6), an extremely potent greenhouse gas, whose levels have risen nearly threefold since 1997.\n\n\n\n\n\nAnother fundamental observation from this study is that higher global mean temperatures correlate to increased greenhouse gas concentrations, particularly carbon dioxide (CO2). Given that regulating CO2 emissions is crucial for mitigating global warming, the accurate forecasting of CO2 concentrations using the SARIMA(1,1,1)(0,1,1) model developed in this project can serve as a valuable contribution in addressing the challenges posed by climate change. Additionally, by employing a Vector Autoregression (VAR) model to explore the multivariate relationships involving multiple greenhouse gases, this project provides an additional source of insights into the potential directions of future greenhouse gas concentrations. Decision-makers can utilize these forecasts to navigate policymaking choices for mitigation and adaptation, although it is crucial to undertake a cautious interpretation of anticipated projections, considering probable uncertainties and changes.\n\nThe study of carbon dioxide atmospheric concentration using the SARIMA model, vector autoregression, and deep learning techniques provided a comprehensive understanding of how these variables influence each other and predict future trends in greenhouse gas emissions. Using deep learning models resulted in more accurate predictions, demonstrating that they might serve as a novel and innovative approach to time series analysis of greenhouse gas data. By utilizing real-world data acquired from sources such as the Global Monitoring Laboratory (GML) and the Environmental Protection Agency (EPA), the results are demonstreated to be accurate and ecologically relevant. Our findings contribute to the efforts directed towards effectively fighting global warming and climate change, empowering the authorities and policymakers in developing sustainable strategies for the future.\n\nI also examined Exxon’s stock return volatility offered an exciting cross-disciplinary investigation into financial markets and time series analysis. While ultimately finding the more complex GARCH model did not offer significant performance advantages over simpler ARIMA, this research reaffirmed the importance of tailoring models to specific applications and iterating to identify the best possible outcomes.\n\nLastly, aside from its quantitative contributions, this project discusses environmental protection, informing academics and concerned citizens, with collective resonance towards crafting innovative proposals that turn these findings into a roadmap preparing humans for an increasingly era of climate catastrophes. Going forward, further work could be done to refine and expand upon some of the areas of time series analysis used in this project. Employing different types, periods, and geographic location sets while incorporating influencing factors such as emissions regulations, geo-political factors or socio-economic indicators in the models would improve the efficacy of the conclusions and their applicability to real-world scenarios.\nThis project has demonstrated the power and versatility of time series analysis in analyzing patterns, relationships of greenhouse gases, and forecasting the environmental impacts, paving the way for actions devised at fighting against climate change. Through evaluation, the evidence demonstrates the project’s role not only in carving a sophisticated and nuanced modeling approach to an environmental need but also to engineer ideas for the future."
  },
  {
    "objectID": "ds.html#data-exploration",
    "href": "ds.html#data-exploration",
    "title": "Data Sources",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nEnvironmental Protection Agency (EPA): Greenhouse Gas Reporting Program (GHGRP)\nThe Environmental Protection Agency’s Greenhouse Gas Reporting Program is important in the battle against climate change. It gives us a complete picture of emissions data from all over the country, helping to guide policy decisions and find areas that need more focus. The data helps the EPA keep track of emissions trends and can be used to come up with ways to reduce greenhouse gas emissions. With this program, the EPA can keep adapting to changing environmental conditions and working towards a sustainable future for our planet.\nThe data can be accessed at the Data Sets section of the GHGRP website.They are stored in a zip file with the data from the last decade. The most interesting data set (to me) is the record of large polluters. I think it could be very interesting to use this to gauge how station proximity to large emitters effects the amplitude of seasonal cycles."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "This project investigates the concentrations of four major greenhouse gases: carbon dioxide (\\(CO_2\\)), methane (CH4), nitrous oxide (N2O), and sulfur hexafluoride (SF6). While there are many greenhouse gases, these four are among the most prevalent and damaging to our environment. As mentioned previously, one of the most commonly overlooked yet critical greenhouse gases is water vapor. Unfortunately, I was unable to source water vapor data on the same time scales as the other variables. Since water vapor is left out of many scientific analyses of greenhouse gases, that is also the case with this project.\nIn this section, the concentrations of the four greenhouse gases are visualized individually and collectively. Data visualizations play a vital role in comprehending the data and informing future modeling decisions, ultimately contributing to a better understanding of our climate and guiding us towards more effective mitigation strategies. This section does not include visualizations of many traditional time-series specific components, like trend and seasonality, since these are included in the EDA and modeling sections."
  },
  {
    "objectID": "dv.html#co_2-carbon-dioxide",
    "href": "dv.html#co_2-carbon-dioxide",
    "title": "Data Visualizations",
    "section": "\\(CO_2\\): Carbon Dioxide",
    "text": "\\(CO_2\\): Carbon Dioxide\nCarbon dioxide (\\(CO_2\\)) is an important and naturally occurring greenhouse gas that plays a vital role in maintaining Earth’s temperature by trapping heat from the sun’s radiation. This is a natural process that has occurred for billions of years and likely played an important role in the establishment of life on Earth. While a certain amount of \\(CO_2\\) is necessary for life, human activities such as burning fossil fuels, deforestation, and industrial processes have led to a rapid increase in atmospheric \\(CO_2\\) levels. This has intensified the greenhouse effect, resulting in global warming and climate change. For many reasons, CO2 is the most commonly known and studied greenhouse gas. It has become the “boogeyman” of greenhouse gases despite being “less powerful” than many of the others mentioned below.\n\n\n\n\n\n\nThe measurement of atmospheric carbon dioxide levels and understanding of its role as a greenhouse gas have evolved over time. In the early 1800s century when Joseph Fourier (who also discovered the Fourier series) discovered the greenhouse effect, which explained how Earth’s atmosphere traps heat. In the 1850s, John Tyndall identified CO2 as one of the key gases responsible for this phenomenon. However, it wasn’t until the 20th century that systematic measurements of atmospheric CO2 levels began. The modern era of CO2 measurement started in the 1950s with Charles David Keeling, an American scientist who pioneered the continuous monitoring of CO2 concentrations. He established the Mauna Loa Observatory in Hawaii, which has been collecting data since 1958. The resulting graph, known as the Keeling Curve, demonstrates the steady increase in atmospheric CO2 levels over time. The graph below plots the data specifically from Mauna Loa, as opposed to the project data from NOAA which is a global average. Despite being a global average, the CO2 levels are based on the Northern Hemisphere season, as that is the source of a majority of greenhouse emissions. The chart below also provides a single year break out to showcase the seasonality."
  },
  {
    "objectID": "dv.html#ch_4-methane",
    "href": "dv.html#ch_4-methane",
    "title": "Data Visualizations",
    "section": "\\(CH_4\\): Methane",
    "text": "\\(CH_4\\): Methane\nMethane (CH4) is a potent greenhouse gas that, like CO2, contributes significantly to the warming of Earth’s atmosphere. Although present in much lower concentrations compared to CO2 (its measured in parts-per-billion instead of parts-per-million), methane is approximately 25 times more effective at trapping heat on a 100-year timescale. It is mainly produced through natural processes, such as wetlands and termites, and human activities, including agriculture, fossil fuel extraction, and waste management.\nDue to its shorter atmospheric lifetime (approximately 12 years, compared to 200 years for CO2), methane’s impact on climate change is often overlooked compared to CO2. However, reducing methane emissions is crucial for mitigating short-term climate impacts and slowing the rate of global warming. Efforts to address methane emissions complement CO2 reduction strategies, as both gases play essential roles in influencing Earth’s climate and require targeted mitigation efforts to ensure a sustainable future for our planet.\n\n\n\n\n\n\nAnalyzing the plot of methane emissions over time, it is evident that there are some distinct differences compared to the CO2 emissions curve. First, the seasonality of methane emissions is less pronounced and less consistent than that of CO2. This is likely due to the fact that methane’s sources, such as wetlands and agriculture, are subject to more variable environmental factors that can influence their rates of emission. Additionally, there is a noticeable flattening of the methane curve between 2000 and 2010, which is not present in the CO2 curve."
  },
  {
    "objectID": "dv.html#n_2o-nitrous-oxide",
    "href": "dv.html#n_2o-nitrous-oxide",
    "title": "Data Visualizations",
    "section": "\\(N_2O\\): Nitrous Oxide",
    "text": "\\(N_2O\\): Nitrous Oxide\nNitrous oxide (N2O), also known as laughing gas, is a lesser-known but potent greenhouse gas that contributes to global warming and climate change. Although present in lower concentrations than carbon dioxide (CO2) and methane (CH4), nitrous oxide is approximately 300 times more effective at trapping heat over a 100-year period. It has an atmospheric lifetime of around 114 years, meaning its impact on climate is long-lasting. N2O is generated through both natural processes, such as microbial activity in soils and water bodies, and human activities, including agriculture (fertilizer application), industrial processes, and fossil fuel combustion. In addition to its role as a greenhouse gas, nitrous oxide also contributes to the depletion of the ozone layer."
  },
  {
    "objectID": "dv.html#sf_6-sulpher-hexaflouride",
    "href": "dv.html#sf_6-sulpher-hexaflouride",
    "title": "Data Visualizations",
    "section": "\\(SF_6\\): Sulpher Hexaflouride",
    "text": "\\(SF_6\\): Sulpher Hexaflouride\nSulfur hexafluoride (SF6) is an exceptionally potent greenhouse gas with an impressive global warming potential (GWP) of 23,500 over a 100-year timescale. This means that it is significantly more effective at trapping heat in the Earth’s atmosphere than carbon dioxide (CO2), despite being present in much lower concentrations. Primarily utilized as an electrical insulator and arc suppressant in high-voltage equipment, SF6 is a synthetic, non-toxic, and inert gas. Its widespread use in the electrical industry, as well as in other applications such as magnesium production, has led to increasing emissions of SF6 into the atmosphere. SF6 stays in the atmosphere for about 3,200 years, its huge GWP means it’s crucial to focus on when coming up with strategies to fight climate change."
  },
  {
    "objectID": "dv.html#comparing-greenhouse-gases",
    "href": "dv.html#comparing-greenhouse-gases",
    "title": "Data Visualizations",
    "section": "Comparing Greenhouse Gases",
    "text": "Comparing Greenhouse Gases\nAs mentioned before, scientists don’t measure these greenhouse gases with the same concentration. Carbon dioxide is measured in parts per million (ppm), methane and nitrous oxide in parts per billion (ppb), and sulfur hexafluoride in parts per trillion (ppt). Also, it’s important to remember that just because there’s more of one gas doesn’t mean it has the same impact on the environment as another. Each gas has its own unique way of interacting with the environment, making their relationship extremely complex and difficutl to model.\nTo show the different scales of these gases, the graph below changes all concentrations to parts per trillion, so we can clearly see the differences between them. This comparison helps us better understand the relative concentration of each gas and what role it plays in the overall greenhouse effect."
  },
  {
    "objectID": "dv.html#scaled-concentrations",
    "href": "dv.html#scaled-concentrations",
    "title": "Data Visualizations",
    "section": "Scaled Concentrations",
    "text": "Scaled Concentrations\nAlthough the previous graph effectively illustrates the differences in scales among the greenhouse gases, it may not be the most suitable for time series analysis. To facilitate this, we can rescale the variables so that all of them are on a 0 to 1 scale. This normalization is a common practice in data science to ensure that each variable has an equal opportunity to influence the model outcomes. In this case, we will also limit the data to include only the years for which we have concentration measurements for all four gases, specifically from 2001 to the present. By doing so, we create a consistent dataset for our time series analysis, enabling a more accurate examination of the trends and interactions among these key greenhouse gases over time."
  },
  {
    "objectID": "dv.html#scaled-seasonal-variation",
    "href": "dv.html#scaled-seasonal-variation",
    "title": "Data Visualizations",
    "section": "Scaled Seasonal Variation",
    "text": "Scaled Seasonal Variation\nLastly, we can calculate the differences of the normalized variables by subtracting each observation from the previous one. This method allows us to examine the changes between subsequent observations and better understand the seasonal variations in greenhouse gas concentrations. Based on the univariate graphs presented earlier, we expect CO2 to exhibit the most significant seasonal variation. However, to ensure a fair comparison, let’s analyze the seasonal patterns using the scaled variables rather than the absolute values previously employed. This approach will provide a clearer insight into the relative variations among the greenhouse gases and help us uncover any underlying trends or patterns in their behavior over time."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For this section of the EDA, we will be focusing specifically on the Sulfur Hexafluoride (\\(SF_6\\)). This process was repeated for three other atmospheric concentrations of greenhouse gasses as well, but those have been omitted to allow for an in-depth look at Sulfur Hexafluoride.\nSulfur Hexafluoride is an odorless, colorless, and non-toxic gas. It is mostly used as an electrical insulator, as it has a very high dielectric strength. \\(SF_6\\) also has other uses, such as in the production of integrated circuits and flat panel displays, as a tracer gas in leak detection, and as a propellant in aerosol cans. \\(SF_6\\) is a greenhouse gas, and its global warming potential is estimated to be 23,900 times higher than that of carbon dioxide. As such, SF6 is regulated by the Montreal Protocol and the Kyoto Protocol.\nSimilar to the other greenhouse gases, Sulfur Hexafluoride is measured continually at many monitoring stations across the globe. Then, NOAA produces a monthly estimate that accounts for additional factors like weather and available monitoring stations. Sulfur Hexafluoride is measured in parts per trillion.\n\n\n\n\n\n\n\n\n\nThe above plot depicts the alarming rise in the mean monthly concentration of \\(SF_6\\) since 1997, with an increase of nearly threefold in just 25 years. This is a cause of great concern, as \\(SF_6\\) is one of the most potent greenhouse gases released into the atmosphere today. Even though it exists in parts per trillion, this accumulation of \\(SF_6\\) will continue to contribute to the disruption of the planet’s climate in conjunction with other greenhouse gases. Undoubtedly, this is a worrisome trend that must be addressed before it is too late.\nBased on the above plot, we can see a strongly increasing trend of \\(SF_6\\) that has continued uninterrupted since records began. The trend is very consistent and appears to be almost linear, with just a slight exponential increase detectable. There appear to be only a few months in the 25 years where the \\(SF_6\\) concentration was lower than it was in a previous month. Unlike many other greenhouse gases, the concentration of \\(SF_6\\) does not exhibit strong season variation. Gases like \\(CO_2\\) or \\(CH_4\\) which were discussed previously, see significant change depending on the season and location of the monitoring station, however \\(SF_6\\) does not. Other than that, there appear to be no periodic fluctuations and the level of noise appears to be minimal.\nBased only on background knowledge and the above plot, the data appears to be additive rather than multiplicative. One way to know the series is additive is linearity and this plot appears to be nearly linear. Another method is to look at variance, based on this plot, the variance appears to be consistent, pointing to an additive series. These assumptions will be more easy to identify when looking at the decomposition of the series. Examining the individual components of the series can help to confirm whether the data is additive or multiplicative."
  },
  {
    "objectID": "eda.html#decomposition",
    "href": "eda.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\nAfter decomposing the series, it becomes easier to identify the trend, seasonal, and remainder components. As expected, there is a strong increasing trend that was readily apparent from the initial plot. Additionally, the decomposition process revealed a much subtler seasonal component to this data, which was not easily detected from the initial plot. The seasonal component is highly regular, and appears very similar to the seasonal components of other greenhouse gas concentrations. However, the scale of this seasonal component is miniscule (just 0.01 part per trillion) compared to the increase of the trend, which accounts for an average of 3.5 parts per trillion per year. Similarly, the decomposition reveals that the randomness in the data is very small when compared to the overall trend. This is why the plot appears to be almost linear in nature. It is important to note however, that while randomness may be difficult to detect in the initial plot, it is still an important factor that should be taken into consideration when analyzing the data. Using the decomposition, the series is clearly additive. This is based on the linearly increasing trend as well as the seasonality which does not increase in variance as the series increases.\n\n\n\n\n\n\nThis plot clearly illustrates the strong correlation between the monthly lags for the first 6 months. As expected, the values are highly correlated with their preceding values. Since the series lacks any noticeable seasonality, these lags are even more evident. The lag plots demonstrate that the series has a high degree of autocorrelation, which can be diminished with the use of detrending. By utilizing this technique, we can effectively reduce the influence of autocorrelation and gain a better insight into the data."
  },
  {
    "objectID": "eda.html#stationarity",
    "href": "eda.html#stationarity",
    "title": "Exploratory Data Analysis",
    "section": "Stationarity",
    "text": "Stationarity\n\nSF6 Concentration (No differencing)\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\nSF6 Concentration (1st order difference)\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\nSF6 Concentration (2nd order difference)\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\n\n\nWe can also consider moving average smoothing to identify underlying patterns in the data. For this example, I will be using the \\(CO_2\\) concentration as it provides a more rich comparison than \\(SF_6\\).\nBelow are four graphs of the \\(CO_2\\) concentration at various levels of smoothing. The first plot is the raw data with no smoothing applied. The seasonal variation is readily apparent in this plot, but that causes the trend to be obscured. One way we can view the trend more easily (other than decomposing) is to use moving average smoothing.\n\n\n\n\n\nThe second graph shows a smoothed moving average of the \\(CO_2\\) concentration with a window that is too small (3 months). Since the concentration follows a 12 month seasonal cycle, the 3 month smoothing only serves to decrease the relative size of the season cycle. While this is a good start to get us to a clear trend line, it is not enough. We will need to apply the smooth moving average with a window that matches the seasonal cycle.\n\n\n\n\n\nThe last two graphs are constructed using smooth moving average windows that are multiples of the seasonal cycle length (12 months). Thus, we can no longer see any seasonal variation because each observation now includes data from all parts of the year equally. We could have achieved the same result by decomposing the series, but this is another option if we know the length of a seasonal cycle. Overall, the 12-month and 24 month windows look very similar, especially since this data has a very steady trend."
  },
  {
    "objectID": "ftsm.html#background",
    "href": "ftsm.html#background",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Background",
    "text": "Background\n(Generalized) Autoregressive Conditional Heteroskedasticity (ARCH) and (GARCH) are statistical models used in econometrics, particularly in financial time series analysis. These models are designed to capture the dynamic nature of volatility, a key concept in financial markets that signifies the degree of variation of returns for a given security or market index. Volatility is a significant aspect of financial time series data, reflecting the degree of risk or uncertainty in a market. High volatility indicates a higher degree of risk, as the asset’s price can change dramatically in a short time. Thus, accurately modeling and forecasting volatility with ARCH and GARCH models can lead to more informed investment strategies and improved risk assessment.\n\n\n\n\n\nARCH models, introduced by Robert Engle in 1982, describe the variance of the current error term or innovation as a function of the actual sizes of the previous time periods’ error terms. In other words, ARCH models capture the phenomenon of “volatility clustering,” where periods of high volatility tend to be followed by high volatility, and low by low.\nGARCH models, a generalized version of ARCH models, incorporate lagged values of the error variance itself into the model. GARCH models are particularly beneficial when dealing with time series data that exhibits volatility clustering and leverage effects. They can effectively model and forecast time-varying volatility, which is a crucial aspect of financial decision making, risk management, and options pricing."
  },
  {
    "objectID": "ftsm.html#exxon-model-fitting-with-arima-and-garch",
    "href": "ftsm.html#exxon-model-fitting-with-arima-and-garch",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Exxon Model Fitting with ARIMA and GARCH",
    "text": "Exxon Model Fitting with ARIMA and GARCH\nSince my project has exactly nothing to do with financial time series data (which was not a requirement when choosing projects) I will instead focus on a completely random topic because that’s what the professor told me to do. As mentioned many times, one of the main greenhouse emitters are energy companies, specifically oil companies. They are terrible and I need not say more. So, let’s get the stock data for our favorite company, Exxon! After getting the data, we plot to see what we are working with. Based on the data, it would be wise to take the logarithm since the values are highly skewed from when the stock was doing well and when it was doing poorly.\n\nExxon Stock Price"
  },
  {
    "objectID": "ftsm.html#stationarity",
    "href": "ftsm.html#stationarity",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stationarity",
    "text": "Stationarity\nAs with most ARCH/GARCH models, we will be modelling the returns rather than the raw data. This is partly why we took the logarithm, so that the returns are on a more reasonable scale from different time periods of the stock. Plotting the raw data, we can see that this series is clearly not stationary. There is extremely high correlation between values as well as string seasonality present in the data. Thus, to address the non-stationarity, we will need to do differencing. In addition to differencing, we will also need to calculate the logarithm of the data to account for large variations in price that occurred over the time frame of interest. Based on the ACF plot, we can see that after taking the log and differenc of the data, this series is now weakly stationary. There is no need for additional differencing as the ACF and PACF plots are looking good already. Any extra differencing would result in over-differencing and would make modeling more difficult."
  },
  {
    "objectID": "ftsm.html#garchpq-model-fitting-with-arima",
    "href": "ftsm.html#garchpq-model-fitting-with-arima",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH(p,q) model fitting with ARIMA",
    "text": "GARCH(p,q) model fitting with ARIMA\n\nArchTest\nFirst, we check for ARCH effects with the ArchTest() function. We will use a standard significance level of \\(\\alpha=0.05\\) for our null hypothesis test. Because the p-value is much smaller than 0.05, so we reject the null hypothesis and conclude the presence of ARCH(1) effects.\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 87.883, df = 1, p-value < 2.2e-16\n\n\n\n\nARIMA model\nLet’s fit the ARIMA model first. We follow the same procedure as previously. For more on ARIMA models, check out the other tabs of the website.\n\n\n\n\n\n   p d q       AIC       BIC      AICc\n1  0 1 0 -11199.74 -11194.06 -11199.74\n2  0 1 1 -11198.76 -11187.41 -11198.76\n3  0 1 2 -11197.29 -11180.25 -11197.28\n4  0 1 3 -11200.80 -11178.09 -11200.78\n5  0 1 4 -11199.37 -11170.98 -11199.35\n6  1 1 0 -11198.80 -11187.44 -11198.79\n7  1 1 1 -11197.95 -11180.92 -11197.94\n8  1 1 2 -11196.69 -11173.97 -11196.67\n9  1 1 3 -11198.96 -11170.57 -11198.94\n10 1 1 4 -11199.73 -11165.65 -11199.69\n11 2 1 0 -11197.44 -11180.40 -11197.43\n12 2 1 1 -11196.56 -11173.85 -11196.55\n13 2 1 2 -11194.03 -11165.63 -11194.00\n14 2 1 3 -11218.17 -11184.09 -11218.13\n15 2 1 4 -11216.00 -11176.24 -11215.95\n16 3 1 0 -11200.19 -11177.47 -11200.17\n17 3 1 1 -11198.19 -11169.80 -11198.17\n18 3 1 2 -11198.57 -11164.50 -11198.54\n19 3 1 3 -11214.04 -11174.28 -11213.99\n20 3 1 4 -11218.23 -11172.80 -11218.17\n21 4 1 0 -11198.22 -11169.82 -11198.19\n22 4 1 1 -11196.23 -11162.15 -11196.19\n23 4 1 2 -11216.14 -11176.38 -11216.09\n24 4 1 3 -11218.15 -11172.72 -11218.09\n\n\n\n\nARIMA(0,1,0)\n\n\nSeries: log(xom) \nARIMA(0,1,0) \n\nsigma^2 = 0.0003299:  log likelihood = 5600.87\nAIC=-11199.74   AICc=-11199.74   BIC=-11194.06\n\n\nUsing the auto.arima function, we can see the the best model is the ARIMA(0,1,0). But the ACF and PACF does not suggest these are good values. Since the auto arima function is sometimes un-trsutworthy, I am still going to go with the (0,1,0) ARIMA as determined by the manual arima model selection.\n\n\n\nUsing the standardized residuals plots, we can see that the ARIMA model is insufficient to accurately model the financial time series data. Thus, we will need to use the GARCH model on top of the residuals from the ARIMA model. This is a common tactic in financial time series which has a much different pattern than other time series like the greenhouse gases for the remainder of the project. Thus, we will continue modeling with the GARCH model. I choice the GARCH values based on the ACF graph, of the ARIMA mode. As we can see from this chart, we should try all p,q values between 0 and 4."
  },
  {
    "objectID": "ftsm.html#garch-model",
    "href": "ftsm.html#garch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH model",
    "text": "GARCH model\nNext, we will fit the ARIMA model and then fit a GARCH model to the residuals of the ARIMA model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 7\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         b1         b2         b3  \n2.054e-06  1.181e-01  4.199e-07  6.692e-01  1.433e-01  7.001e-02  \n\n\nAfter trying all p,q values from 0,4 in combination, the GARCH(1,2) model is the best and has the lowest combination of AIC and BIC models. I tested all of the models, but only included the output from the best one. I attempted to use cross validation but was unsuccessful in making comparisons between the different models.\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x0000029af9aacd70>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n1.9165e-05  2.3768e-06  1.3352e-01  3.1753e-01  5.4845e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     1.917e-05   2.574e-04    0.074  0.94064    \nomega  2.377e-06   8.702e-07    2.731  0.00631 ** \nalpha1 1.335e-01   1.718e-02    7.773 7.77e-15 ***\nbeta1  3.175e-01   1.299e-01    2.445  0.01447 *  \nbeta2  5.484e-01   1.241e-01    4.420 9.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 6038.335    normalized:  2.790358 \n\nDescription:\n Sat May  6 15:56:16 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  204.3455  0           \n Shapiro-Wilk Test  R    W      0.9875822 9.312571e-13\n Ljung-Box Test     R    Q(10)  3.419432  0.9697634   \n Ljung-Box Test     R    Q(15)  5.198717  0.990306    \n Ljung-Box Test     R    Q(20)  13.75328  0.8427815   \n Ljung-Box Test     R^2  Q(10)  13.4471   0.1997365   \n Ljung-Box Test     R^2  Q(15)  18.69137  0.2280616   \n Ljung-Box Test     R^2  Q(20)  19.5402   0.4870023   \n LM Arch Test       R    TR^2   16.69211  0.1615507   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.576095 -5.562972 -5.576105 -5.571295 \n\n\nSince all the models has similar AIC ,BIC values, I would go with GARCH(1,1) which all the coefficients are significant.\n\nFinal Model\nThe final model has a decent but not great fir for the Carvana stock return data. All of the errors except for mu are significant but the Ljung-Box statistics are well over the standard threshold. Since there is a mix of indicators, this tells us that the model is decent but not quantifiable better than the simpler ARIMA model. Thus, in this case I would rely on the ARIMA since it is a simpler specification.\n\n\nSeries: data \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      3e-04\ns.e.  4e-04\n\nsigma^2 = 0.00033:  log likelihood = 5601.07\nAIC=-11198.15   AICc=-11198.14   BIC=-11186.79\n\nTraining set error measures:\n                       ME       RMSE        MAE          MPE      MAPE\nTraining set 1.916534e-06 0.01815787 0.01249646 -0.001488455 0.3103589\n                   MASE        ACF1\nTraining set 0.04607169 -0.02228051\n\n\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x0000029afc55d818>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n1.9165e-05  2.3768e-06  1.3352e-01  3.1753e-01  5.4845e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     1.917e-05   2.574e-04    0.074  0.94064    \nomega  2.377e-06   8.702e-07    2.731  0.00631 ** \nalpha1 1.335e-01   1.718e-02    7.773 7.77e-15 ***\nbeta1  3.175e-01   1.299e-01    2.445  0.01447 *  \nbeta2  5.484e-01   1.241e-01    4.420 9.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 6038.335    normalized:  2.790358 \n\nDescription:\n Sat May  6 15:56:17 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  204.3455  0           \n Shapiro-Wilk Test  R    W      0.9875822 9.312571e-13\n Ljung-Box Test     R    Q(10)  3.419432  0.9697634   \n Ljung-Box Test     R    Q(15)  5.198717  0.990306    \n Ljung-Box Test     R    Q(20)  13.75328  0.8427815   \n Ljung-Box Test     R^2  Q(10)  13.4471   0.1997365   \n Ljung-Box Test     R^2  Q(15)  18.69137  0.2280616   \n Ljung-Box Test     R^2  Q(20)  19.5402   0.4870023   \n LM Arch Test       R    TR^2   16.69211  0.1615507   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.576095 -5.562972 -5.576105 -5.571295 \n\n\n\n\n [1] \"\"                                                               \n [2] \"Title:\"                                                         \n [3] \" GARCH Modelling \"                                              \n [4] \"\"                                                               \n [5] \"Call:\"                                                          \n [6] \" garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \"\n [7] \"\"                                                               \n [8] \"Mean and Variance Equation:\"                                    \n [9] \" data ~ garch(1, 2)\"                                            \n[10] \"<environment: 0x0000029afc55d818>\"                              \n[11] \" [data = arima.res]\"                                            \n[12] \"\"                                                               \n[13] \"Conditional Distribution:\"                                      \n[14] \" norm \"                                                         \n[15] \"\"                                                               \n[16] \"Coefficient(s):\"                                                \n[17] \"        mu       omega      alpha1       beta1       beta2  \"   \n[18] \"1.9165e-05  2.3768e-06  1.3352e-01  3.1753e-01  5.4845e-01  \"   \n[19] \"\"                                                               \n[20] \"Std. Errors:\"                                                   \n[21] \" based on Hessian \"                                             \n[22] \"\"                                                               \n[23] \"Error Analysis:\"                                                \n[24] \"        Estimate  Std. Error  t value Pr(>|t|)    \"             \n[25] \"mu     1.917e-05   2.574e-04    0.074  0.94064    \"             \n[26] \"omega  2.377e-06   8.702e-07    2.731  0.00631 ** \"             \n[27] \"alpha1 1.335e-01   1.718e-02    7.773 7.77e-15 ***\"             \n[28] \"beta1  3.175e-01   1.299e-01    2.445  0.01447 *  \"             \n[29] \"beta2  5.484e-01   1.241e-01    4.420 9.85e-06 ***\"             \n[30] \"---\"                                                            \n[31] \"Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\" \n[32] \"\"                                                               \n[33] \"Log Likelihood:\"                                                \n[34] \" 6038.335    normalized:  2.790358 \"                            \n[35] \"\"                                                               \n[36] \"Description:\"                                                   \n[37] \" Sat May  6 15:56:17 2023 by user: sleblanc \"                   \n[38] \"\"                                                               \n\n\nLet \\(x_t\\) be the time series data and \\(z_t\\) be the residuals of the ARIMA model. Then the GARCH(1, 2) model is defined by:\n\\[z_t = \\sigma_t * \\epsilon_t\\]\nwhere \\(\\epsilon_t\\) follows a standard normal distribution (mean = 0, sd = 1), and the conditional variance \\(\\sigma_t^2\\) is given by:\n\\[\\sigma_t^2 = \\omega + \\alpha_1 * z_(t-1)^2 + \\beta_1 * \\sigma_(t-1)^2 + \\beta_2 * \\sigma_(t-2)^2\\]\nwith the estimated coefficients being:\n\\(\\omega\\) = 0.00001667 \\(\\alpha_1\\) = 0.08827887 \\(\\beta_1\\) = 0.37861131 \\(\\beta_2\\) = 0.53446470 The model is fitted to the residuals of an ARIMA model (arima.res).\n\n\nForecast\n\n\n\n\n\n    meanForecast  meanError standardDeviation lowerInterval upperInterval\n1   1.916534e-05 0.02085313        0.02085313   -0.04085221    0.04089055\n2   1.916534e-05 0.02082005        0.02082005   -0.04078738    0.04082571\n3   1.916534e-05 0.02088996        0.02088996   -0.04092441    0.04096274\n4   1.916534e-05 0.02090335        0.02090335   -0.04095064    0.04098897\n5   1.916534e-05 0.02094759        0.02094759   -0.04103735    0.04107568\n6   1.916534e-05 0.02097483        0.02097483   -0.04109074    0.04112907\n7   1.916534e-05 0.02101128        0.02101128   -0.04116219    0.04120052\n8   1.916534e-05 0.02104259        0.02104259   -0.04122355    0.04126189\n9   1.916534e-05 0.02107662        0.02107662   -0.04129025    0.04132858\n10  1.916534e-05 0.02110906        0.02110906   -0.04135384    0.04139217\n11  1.916534e-05 0.02114228        0.02114228   -0.04141894    0.04145727\n12  1.916534e-05 0.02117498        0.02117498   -0.04148303    0.04152136\n13  1.916534e-05 0.02120786        0.02120786   -0.04154748    0.04158581\n14  1.916534e-05 0.02124055        0.02124055   -0.04161155    0.04164988\n15  1.916534e-05 0.02127325        0.02127325   -0.04167564    0.04171397\n16  1.916534e-05 0.02130585        0.02130585   -0.04173954    0.04177787\n17  1.916534e-05 0.02133841        0.02133841   -0.04180336    0.04184169\n18  1.916534e-05 0.02137090        0.02137090   -0.04186704    0.04190537\n19  1.916534e-05 0.02140334        0.02140334   -0.04193061    0.04196894\n20  1.916534e-05 0.02143571        0.02143571   -0.04199406    0.04203239\n21  1.916534e-05 0.02146803        0.02146803   -0.04205740    0.04209573\n22  1.916534e-05 0.02150029        0.02150029   -0.04212062    0.04215895\n23  1.916534e-05 0.02153248        0.02153248   -0.04218373    0.04222206\n24  1.916534e-05 0.02156462        0.02156462   -0.04224672    0.04228505\n25  1.916534e-05 0.02159670        0.02159670   -0.04230960    0.04234793\n26  1.916534e-05 0.02162873        0.02162873   -0.04237236    0.04241069\n27  1.916534e-05 0.02166069        0.02166069   -0.04243501    0.04247334\n28  1.916534e-05 0.02169260        0.02169260   -0.04249755    0.04253588\n29  1.916534e-05 0.02172445        0.02172445   -0.04255998    0.04259831\n30  1.916534e-05 0.02175625        0.02175625   -0.04262229    0.04266062\n31  1.916534e-05 0.02178798        0.02178798   -0.04268450    0.04272283\n32  1.916534e-05 0.02181966        0.02181966   -0.04274659    0.04278492\n33  1.916534e-05 0.02185129        0.02185129   -0.04280858    0.04284691\n34  1.916534e-05 0.02188286        0.02188286   -0.04287045    0.04290878\n35  1.916534e-05 0.02191437        0.02191437   -0.04293221    0.04297055\n36  1.916534e-05 0.02194583        0.02194583   -0.04299387    0.04303220\n37  1.916534e-05 0.02197723        0.02197723   -0.04305542    0.04309375\n38  1.916534e-05 0.02200858        0.02200858   -0.04311686    0.04315519\n39  1.916534e-05 0.02203987        0.02203987   -0.04317819    0.04321652\n40  1.916534e-05 0.02207111        0.02207111   -0.04323942    0.04327775\n41  1.916534e-05 0.02210230        0.02210230   -0.04330054    0.04333887\n42  1.916534e-05 0.02213343        0.02213343   -0.04336156    0.04339989\n43  1.916534e-05 0.02216450        0.02216450   -0.04342247    0.04346080\n44  1.916534e-05 0.02219553        0.02219553   -0.04348327    0.04352160\n45  1.916534e-05 0.02222650        0.02222650   -0.04354397    0.04358230\n46  1.916534e-05 0.02225742        0.02225742   -0.04360457    0.04364290\n47  1.916534e-05 0.02228828        0.02228828   -0.04366506    0.04370339\n48  1.916534e-05 0.02231909        0.02231909   -0.04372545    0.04376378\n49  1.916534e-05 0.02234985        0.02234985   -0.04378574    0.04382407\n50  1.916534e-05 0.02238056        0.02238056   -0.04384592    0.04388425\n51  1.916534e-05 0.02241121        0.02241121   -0.04390600    0.04394433\n52  1.916534e-05 0.02244181        0.02244181   -0.04396598    0.04400431\n53  1.916534e-05 0.02247237        0.02247237   -0.04402586    0.04406419\n54  1.916534e-05 0.02250287        0.02250287   -0.04408564    0.04412397\n55  1.916534e-05 0.02253332        0.02253332   -0.04414532    0.04418365\n56  1.916534e-05 0.02256371        0.02256371   -0.04420490    0.04424323\n57  1.916534e-05 0.02259406        0.02259406   -0.04426438    0.04430271\n58  1.916534e-05 0.02262436        0.02262436   -0.04432376    0.04436209\n59  1.916534e-05 0.02265461        0.02265461   -0.04438305    0.04442138\n60  1.916534e-05 0.02268480        0.02268480   -0.04444223    0.04448056\n61  1.916534e-05 0.02271495        0.02271495   -0.04450132    0.04453965\n62  1.916534e-05 0.02274505        0.02274505   -0.04456031    0.04459864\n63  1.916534e-05 0.02277509        0.02277509   -0.04461920    0.04465753\n64  1.916534e-05 0.02280509        0.02280509   -0.04467800    0.04471633\n65  1.916534e-05 0.02283504        0.02283504   -0.04473670    0.04477503\n66  1.916534e-05 0.02286494        0.02286494   -0.04479530    0.04483363\n67  1.916534e-05 0.02289480        0.02289480   -0.04485381    0.04489214\n68  1.916534e-05 0.02292460        0.02292460   -0.04491222    0.04495055\n69  1.916534e-05 0.02295435        0.02295435   -0.04497054    0.04500887\n70  1.916534e-05 0.02298406        0.02298406   -0.04502877    0.04506710\n71  1.916534e-05 0.02301372        0.02301372   -0.04508690    0.04512523\n72  1.916534e-05 0.02304333        0.02304333   -0.04514493    0.04518326\n73  1.916534e-05 0.02307289        0.02307289   -0.04520288    0.04524121\n74  1.916534e-05 0.02310241        0.02310241   -0.04526073    0.04529906\n75  1.916534e-05 0.02313188        0.02313188   -0.04531849    0.04535682\n76  1.916534e-05 0.02316130        0.02316130   -0.04537615    0.04541448\n77  1.916534e-05 0.02319068        0.02319068   -0.04543373    0.04547206\n78  1.916534e-05 0.02322001        0.02322001   -0.04549121    0.04552954\n79  1.916534e-05 0.02324929        0.02324929   -0.04554860    0.04558694\n80  1.916534e-05 0.02327853        0.02327853   -0.04560591    0.04564424\n81  1.916534e-05 0.02330772        0.02330772   -0.04566312    0.04570145\n82  1.916534e-05 0.02333686        0.02333686   -0.04572024    0.04575857\n83  1.916534e-05 0.02336596        0.02336596   -0.04577727    0.04581560\n84  1.916534e-05 0.02339501        0.02339501   -0.04583421    0.04587254\n85  1.916534e-05 0.02342402        0.02342402   -0.04589107    0.04592940\n86  1.916534e-05 0.02345298        0.02345298   -0.04594783    0.04598616\n87  1.916534e-05 0.02348190        0.02348190   -0.04600451    0.04604284\n88  1.916534e-05 0.02351077        0.02351077   -0.04606110    0.04609943\n89  1.916534e-05 0.02353960        0.02353960   -0.04611760    0.04615593\n90  1.916534e-05 0.02356838        0.02356838   -0.04617401    0.04621234\n91  1.916534e-05 0.02359712        0.02359712   -0.04623034    0.04626867\n92  1.916534e-05 0.02362581        0.02362581   -0.04628658    0.04632491\n93  1.916534e-05 0.02365446        0.02365446   -0.04634273    0.04638106\n94  1.916534e-05 0.02368307        0.02368307   -0.04639880    0.04643713\n95  1.916534e-05 0.02371163        0.02371163   -0.04645478    0.04649311\n96  1.916534e-05 0.02374015        0.02374015   -0.04651068    0.04654901\n97  1.916534e-05 0.02376863        0.02376863   -0.04656649    0.04660482\n98  1.916534e-05 0.02379706        0.02379706   -0.04662221    0.04666055\n99  1.916534e-05 0.02382545        0.02382545   -0.04667786    0.04671619\n100 1.916534e-05 0.02385380        0.02385380   -0.04673341    0.04677175"
  },
  {
    "objectID": "ftsm.html#volatality-plot",
    "href": "ftsm.html#volatality-plot",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volatality plot",
    "text": "Volatality plot\nFinally, we should also consider the volatility of the data. Volatility is a key feature of financial time series data and will have large effect on the model we end up choosing. Below is the plot of the volatility of the Carvana data. When looking at the volatility plot, we see several large spikes that stick out. The first is in March 2020, which was right at the beginning of COVID, so the volatility is expected. The second large spike occurs at the send of 2022, which is unexpected because the value of the stock is so low at that point. However, it could be thatwith such a low stock price, relative changes have a greater effect. A $10 gain when the stock is at $10 is a 100% increase, but when the stock was at $400 this would be just a 2.5% gain. So it is not crazy that the stock because much more volatile as they value fell off a cliff in 2022."
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html",
    "href": "greenhouse_gases_deep_learning.html",
    "title": "AN560: Sam LeBlanc",
    "section": "",
    "text": "Deep learning, which is a subset of machine learning, uses artificial neural networks to simulate human decision-making. These networks are composed of layers of interconnected nodes or “neurons” that process input data and generate output. Deep learning and all of its components are incredibly complex, and I cannot exaplin them all here. If you are interested in learning more, IBM provides a great explanation for sata-savvy folks that doesn’t get too lost in the weeds.Deep learning, a subset of machine learning, employs artificial neural networks to mimic human decision-making. These networks consist of layers of interconnected nodes or “neurons” that process input data and generate output.\nDeep learning is especially effective with large, unstructured datasets, and although it demands substantial computational power, it’s capable of producing highly accurate models. Thanks to tools like TensorFlow and Keras, deep learning has become more accessible, making it easier to design, train, and deploy models. Deep learning, with its intricate components, is an intricate field, and a comprehensive explanation is beyond this page’s scope. For those interested in diving deeper, IBM offers an excellent resource that provides a solid understanding without getting overly technical.\nHere are some of the critical elements of deep learning leveraged in the following models include neural networks, weights and biases, activation functions, backpropogation and gradient descent. In the models below, I will make specirfic note of when each of these elements is being utilized.\nNeural Networks: These are inspired by the human brain’s structure, with networks transforming input data through layers of interconnected nodes. Each layer refines the input it receives, gradually building up complex patterns and associations much like our brain does when processing information.\nWeights and Biases: These are parameters that are fine-tuned during the learning process. They determine the significance of inputs and play a pivotal role in the accuracy of the model’s predictions. Think of them as the factors that determine how much importance should be given to each input when making a prediction.\nActivation Functions: These functions dictate whether a neuron should be activated based on its inputs. They’re like the gatekeepers of information that decide whether the input they receive is relevant enough to be passed on to the next layer.\nBackpropagation and Gradient Descent: These are techniques used to adjust weights and biases and minimize the error function. They are the backbone of learning in neural networks, allowing the model to learn from its mistakes and improve over time.\nLeveraging the same univariate time-series data that was utilized in the ARMA/ARIMA models, I successfully implemented and trained three distinct neural network models using Keras. These models included a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and a Long Short-Term Memory (LSTM) model. A partition of the data was set aside for training and validation purposes, ensuring an unbiased evaluation of the model performance. Furthermore, to mitigate the risk of overfitting, I incorporated regularization techniques into the model’s architecture. Consequently, these models now provide a robust framework for making future predictions.\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN,LSTM,GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import regularizers\n\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#import-and-prepare-data",
    "href": "greenhouse_gases_deep_learning.html#import-and-prepare-data",
    "title": "AN560: Sam LeBlanc",
    "section": "Import and Prepare Data",
    "text": "Import and Prepare Data\n\ndf = pd.read_csv('co2.csv')[['year','month','average']]\nprint(df)\n\n     year  month  average\n0    1958      3   315.70\n1    1958      4   317.45\n2    1958      5   317.51\n3    1958      6   317.24\n4    1958      7   315.86\n..    ...    ...      ...\n773  2022      8   417.19\n774  2022      9   415.95\n775  2022     10   415.78\n776  2022     11   417.51\n777  2022     12   418.95\n\n[778 rows x 3 columns]\n\n\n\nConvert features to Numpy Array\n\nX = np.array(df[\"average\"].values.astype('float32')).reshape(df.shape[0],1)\nprint(X.shape)\n\n(778, 1)\n\n\n\n\nVisualize Raw Data\n\ndef plotly_line_plot(t,y,title=\"Plot\", x_label=\"Months since March 1958\", y_label=\"CO2 Concentration\"):\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n    for i in range(1,len(y)):\n        if len(t[i])==1: \n            fig.add_scatter(x=t[i],y=y[i])\n        else: \n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n    fig.update_layout(xaxis_title=x_label, yaxis_title=y_label, template=\"plotly_white\", showlegend=False)\n    fig.show()\n\n\nplotly_line_plot([[*range(0,len(X))]],[X[:,0]],title=\"Atmospheric CO2 Concentration since March 1958\")\n\n\n                                                \n\n\n\n\nPreform Train-Test Split\n\ndef get_train_test(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1)) # apply min-max scalar\n    data = scaler.fit_transform(data).flatten()\n    split = int(len(data)*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\ntrain_data, test_data, data = get_train_test(X)\nprint('Train Data:',train_data.shape)\nprint('Test Data:',test_data.shape)\n\nTrain Data: (622,)\nTest Data: (156,)\n\n\n\n\nVisualize Train-Test Split\n\nt1=[*range(0,len(train_data))]\nt2=len(train_data)+np.array([*range(0,len(test_data))])\nplotly_line_plot([t1,t2],[train_data,test_data],title=\"Atmospheric CO2 Concentration since March 1958: Train-Test Split\")\n\n\n                                                \n\n\n\n\nRe-format Data for Deep Learning\n\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind, X, Y_ind, Y\n\n    Y_ind = np.arange(time_steps, len(dat), time_steps);\n    Y = dat[Y_ind]\n\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    \n    del X_ind[::time_steps]\n    X = dat[X_ind]; \n\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n\n    return X1, Y\n\n\np=12 # seasonal lag\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\nprint('Train Data:',testX.shape,testY.shape)\nprint('Test Data:',trainX.shape,trainY.shape)\n\nTrain Data: (12, 11, 1) (12,)\nTest Data: (51, 11, 1) (51,)\n\n\n\n\nVisualize\n\ntmp1=[]; \ntmp2=[]; \ntmp3=[]; \ncount=0\n\nfor i in range(0,trainX.shape[0]):\n    tmp1.append(count+np.array([*range(0,trainX[i,:,0].shape[0])]))\n    tmp1.append([count+trainX[i,:,0].shape[0]]);\n    tmp2.append(trainX[i,:,0])\n    tmp2.append([trainY[i]]);\n    count+=trainX[i,:,0].shape[0]+1\n\nplotly_line_plot(tmp1,tmp2,title=\"Atmospheric CO2 Concentration since March 1958: Training Points\")"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#create-models",
    "href": "greenhouse_gases_deep_learning.html#create-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Create Models",
    "text": "Create Models\n\nModel and Training Parameters\n\nrecurrent_hidden_units=3\nepochs=60\nf_batch=0.2 \noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n3 Models: LSTM, SimpleRNN, and GRU\n\nmod_lstm = Sequential()\nmod_lstm.add(\n    LSTM(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_lstm.add(Dense(units=1, activation='linear'))\nmod_lstm.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_lstm.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 3)                 60        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmod_srnn = Sequential()\nmod_srnn.add(\n    SimpleRNN(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_srnn.add(Dense(units=1, activation='linear'))\nmod_srnn.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_srnn.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n                                                                 \n dense_1 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmod_gru = Sequential()\nmod_gru.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_gru.add(Dense(units=1, activation='linear'))\nmod_gru.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_gru.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 3)                 54        \n                                                                 \n dense_2 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#train-models",
    "href": "greenhouse_gases_deep_learning.html#train-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Train Models",
    "text": "Train Models\n\nmodels = {\n    'LSTM' : mod_lstm,\n    'SRNN' : mod_srnn,\n    'GRU' : mod_gru\n}\n\nhistories = {\n    'LSTM' : None,\n    'SRNN' : None,\n    'GRU' : None\n}\n\nfor mod in models.keys():\n    model = models[mod]\n    history = model.fit(\n        trainX, trainY, \n        epochs=epochs, \n        batch_size=int(f_batch*trainX.shape[0]), \n        validation_split=validation_split,\n        verbose=0\n    )\n    histories[mod] = history"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#compare-models",
    "href": "greenhouse_gases_deep_learning.html#compare-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Compare Models",
    "text": "Compare Models\n\ntrain_predictions = {}\ntest_predictions = {}\n\nfor mod in models.keys():\n    model = models[mod]\n    train_predict = model.predict(trainX).squeeze()\n    test_predict = model.predict(testX).squeeze()\n    train_predictions[mod] = train_predict\n    test_predictions[mod] = test_predict\n\n2/2 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 22ms/step\n2/2 [==============================] - 0s 0s/step\n1/1 [==============================] - 0s 30ms/step\n2/2 [==============================] - 0s 0s/step\n1/1 [==============================] - 0s 24ms/step\n\n\n\nCompute RMSE\n\ndef calculate_rsme(model_name, trainY, testY, train_predict, test_predict):\n    print(f'\\n{model_name}:\\n')\n    train_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\n    test_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n    print('Train MSE = %.5f. RMSE = %.5f' % (train_rmse**2.0,train_rmse))\n    print('Test MSE = %.5f. RMSE = %.5f' % (test_rmse**2.0,test_rmse))\n\n\ncalculate_rsme('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\nLSTM:\n\nTrain MSE = 0.00021. RMSE = 0.01450\nTest MSE = 0.01208. RMSE = 0.10992\n\n\n\ncalculate_rsme('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\nSRNN:\n\nTrain MSE = 0.01732. RMSE = 0.13161\nTest MSE = 0.22555. RMSE = 0.47493\n\n\n\ncalculate_rsme('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])\n\n\nGRU:\n\nTrain MSE = 0.00170. RMSE = 0.04129\nTest MSE = 0.02943. RMSE = 0.17155\n\n\n\n\nLoss by Epoch\n\ndef loss_by_epoch(model_name, epochs_steps, history):\n    plotly_line_plot(\n        [epochs_steps, epochs_steps],\n        [history.history['loss'],\n         history.history['val_loss']],\n        title=f\"{model_name}: Loss by Training Epoch\",\n        x_label=\"Training Epochs\",\n        y_label=\"Loss (MSE)\"\n    )\n\n\nepochs_steps = [*range(0, len(histories['LSTM'].history['loss']))]\nloss_by_epoch('LSTM', epochs_steps, histories['LSTM'])\n\n\n                                                \n\n\n\nepochs_steps = [*range(0, len(histories['SRNN'].history['loss']))]\nloss_by_epoch('SRNN', epochs_steps, histories['SRNN'])\n\n\n                                                \n\n\n\nepochs_steps = [*range(0, len(histories['GRU'].history['loss']))]\nloss_by_epoch('GRU', epochs_steps, histories['GRU'])\n\n\n                                                \n\n\n\n\nParity Plot\n\ndef parity_plot(model_name, trainY, testY, train_predict, test_predict):\n    fig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\n    fig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\n    fig.add_scatter(x=trainY,y=trainY, mode='lines')\n\n    fig.update_layout(\n        title=f\"{model_name}: Predicted vs True Y-Values\",\n        xaxis_title=\"Predicted Y-Value\",\n        yaxis_title=\"True Y-Value\",\n        template=\"plotly_white\",\n        showlegend=False\n    )\n\n    fig.show()\n\n\nparity_plot('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\n                                                \n\n\n\nparity_plot('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\n                                                \n\n\n\nparity_plot('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])\n\n\n                                                \n\n\n\n\nPrediction Result\n\ndef plot_result(model_name, trainY, testY, train_predict, test_predict):\n    train_len = np.arange(len(trainY))\n    test_len = np.arange(len(trainY), len(trainY) + len(testY))\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=train_len, y=trainY,mode='lines+markers',name='Training Data'))\n    fig.add_trace(go.Scatter(x=test_len, y=testY,mode='lines+markers',name='Test Data'))\n    fig.add_trace(go.Scatter(x=train_len, y=train_predict,mode='lines+markers',name='Training Predictions'))\n    fig.add_trace(go.Scatter(x=test_len, y=test_predict,mode='lines+markers',name='Test Predictions'))\n\n    fig.update_layout(title_text=f'{model_name}: Actual and Predicted Values',\n                      xaxis_title='Observation Number',\n                      yaxis_title='CO2 Concentration (Scaled)',\n                      template=\"plotly_white\")\n    fig.show()\n\n\nplot_result('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\n                                                \n\n\n\nplot_result('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\n                                                \n\n\n\nplot_result('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#analysis-questions",
    "href": "greenhouse_gases_deep_learning.html#analysis-questions",
    "title": "AN560: Sam LeBlanc",
    "section": "Analysis Questions",
    "text": "Analysis Questions\n\nHow do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power\nIn this section, we went on the ardous journey of creating three different Artificial Neural Networks (ANNs) - a Simple Recurrent Neural Network (SRNN), a Long Short-Term Memory (LSTM), and a Gated Recurrent Unit (GRU). Our objective was to model and predict the atmospheric \\(CO_2\\) concentration over the next decade. The neural nets harness the auto-correlation of the time-series data, refining the observations to align with the 12-month seasonal lag. Essentially, we used data from the same month (e.g., May) across multiple years to predict what the CO2 concentration would look like in the subsequent Mays. This strategy delivered predictions for the atmospheric CO2 levels for the next ten years (Mays), extending until 2033.\nWhen it comes to comparing the performance of these models, it’s a bit of a mixed bag. There was a notable variation across different runs of the code, with either the LSTM or GRU models showing adequate to good performance. Interestingly, the performance of these two models was typically quite close on any given run, while the SRNN consistently trailed behind in terms of results. In the final run of the code, the LSTM model was the best model, with the GRU model close behind, and the SRNN rounding out the three. This performance hierarchy was evident across various evaluation methods, including Root Mean Squared Error (RMSE), Loss by Epoch, Parity Plots, and Predictive Results. In the end, the LSTM model emerged as the top model in accurately predicting CO2 concentrations. However, it’s important to remember that this was just the final run. In other iterations, the GRU model demonstrated similar prowess to the LSTM model. Thus, while we can glean some insights from this exercise, the dynamic nature of these models means that the ‘best’ model can vary from one run to the next.\n\n\nWhat effect does including regularization have on your results.\nRegularization had a subtle yet noticeable impact on the precision of our modes. As mentioned earlier, the accuracy of different models exhibited considerable variance across different runs, making it a challenge to definitively quantify the influence of regularization. However, generally speaking, the models’ performance seemed to dip slightly when regularization was omitted.\nI surmise that the modest effect of regularization can be attributed to the relatively stable scale of the data throughout the time series. In 1958, the atmospheric CO2 concentration was 320 ppm, which rose to 420 ppm by 2023. While this increase is alarmingly significant from an environmental perspective, it’s not such a drastic shift that would significantly amplify the impact of regularization in our models. Over the span of these years, the increase in CO2 concentration amounts to around 35%.\nRegularization often proves to be particularly beneficial in scenarios where the data exhibits substantial scale disparity either within a single dataset or when combined with another dataset. In our case, given the relatively modest scale difference in the CO2 concentration data, the effect of regularization was understandably muted. Nonetheless, its influence, albeit small, contributed to the overall performance increase of our models when using regularization.\n\n\nHow far into the future can the deep learning model accurately predict the future.\nDetermining the precise extent up to which our deep learning models can accurately predict is a challenging task, due in part to the model’s performance fluctuations across multiple code runs. However, a general observation is that the predictions tend to demonstrate internal consistency - if the model is correctly predicting values up to a certain point, it is likely to continue to do so, but not always. Interestingly, in the case of the LSTM model, the training predictions mirrored the actual values up to a specific point, beyond which it began to underestimate the true values. This underestimation could be attributed to the accelerating rate of carbon dioxide emissions in the atmosphere. The model seems to predict a linear continuation of the trend, but the reality has surpassed this linear progression, indicating an accelerating greenhouse gas phenomenon. This acceleration is a grave concern for climate researchers and has been the focus of numerous studies. Therefore, while the predictions demonstrate a consistency with their own trends, it does not necessarily equate to accuracy with the real-world data. The LSTM model’s performance is indicative of this: minor inaccuracies can accumulate over time, leading to a model that eventually falls short in accurately predicting CO2 concentration towards the end of the test data period. It serves as a reminder that these models must be periodically recalibrated and retrained to adapt to the changing trends in our ever-dynamic climate.\n\n\nHow does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\nThe deep learning models, specifically the LSTM and GRU models, significantly outperformed the traditional single-variable time-series ARMA/ARIMA models in predicting atmospheric CO2 concentrations. Initially, I applied the ARMA/ARIMA models to SF6 concentration data, as it did not exhibit seasonality. However, upon revisiting the analysis with CO2 concentration data, the deep learning models demonstrated superior performance.\nThe best CO2 models using the ARMA/ARIMA methods achieved an RMSE of 0.6, while the LSTM model had an RMSE of 0.1 and the GRU model an RMSE of 0.17 for the test data. It is important to note that training ARIMA and deep learning models is a complex and challenging task. It is highly probable that alternative hyperparameter configurations for both methods could yield more accurate predictions. Nonetheless, based on the models we have, it is evident that the deep learning models are substantially more effective in modeling CO2 data and making accurate predictions.\n\n\nCompare your models (use RMSE) and forecasts from these sections with your Deep Learning Models: ARIMA/SARIMA/VAR\nWhen comparing the deep learning models with the SARIMA model, the outcomes echo those of the ARIMA model. The deep learning models, specifically LSTM and GRU, substantially outperform the traditional time series models in modeling and predicting CO2 concentrations. While the SARIMA model did present improved performance over ARIMA, its best RMSE was 0.4, compared to the LSTM model with an RMSE of 0.1, and the GRU model with an RMSE of 0.17 for the test data. Just like with the ARIMA model, the SARIMA model’s performance may have been hindered by the lack of extensive hyperparameter tuning. It’s challenging to definitively state which model is superior without further optimization. Nonetheless, the preliminary results indicate that the deep learning models significantly outperform the traditional time series models.\nIn the VAR section, I employed data from four greenhouse gases to construct a multivariate model, aiming to predict future concentrations of all four gases concurrently. I did my best to implement a deep learning model for this task but encountered challenges during the data wrangling stage. As Professor James suggested, the most challenging aspect of the deep learning process (and data science in general) is ensuring the data is appropriately prepared and in a format that TensorFlow likes. I am still working on the multivariate deep learning model and aim to have it ready for the final deliverable. Comparing just the prediciting of the CO2 concentration from the VAR model, we find the same thing as the SARIMA model. The VAR model’s best RSME for the CO2 is 0.4, which is not even close to the deep learning model."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to my (Sam LeBlanc, nice to meet you 👋) class portfolio for ANLY 560 - Time Series Analysis at Georgetown University. The course is an in-depth exploration of time series data analysis, and provides a critical approach for using statistics and machine to learn about sequential time data, which holds unique challenges due to the inherent correlation of adjacent observations. If that sounds like a lot, don’t worry, it is 🙃. This project is titled, Exploring Atmospheric Concentrations of Greenhouse Gases with Time Series Analysis, and I will let you (dear reader) figure out what that means…"
  },
  {
    "objectID": "intro.html#what-is-a-time-series",
    "href": "intro.html#what-is-a-time-series",
    "title": "Introduction",
    "section": "What is a Time Series?",
    "text": "What is a Time Series?\n\nAny metric that is consistently measured over regular time intervals. Some examples of time series data include: weather, stocks, industry forecasts, traffic, and energy prices.\nWhat makes a time series special of the auto-correlation in the data. Successive observations are not independent as they strictly rely on data from directly before the observation.\nThis can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "intro.html#what-are-greenhouse-gases",
    "href": "intro.html#what-are-greenhouse-gases",
    "title": "Introduction",
    "section": "What are Greenhouse Gases?",
    "text": "What are Greenhouse Gases?\n\nNature and Composition: Greenhouse gases are atmospheric gases that have the capacity to absorb and emit infrared radiation, contributing to the greenhouse effect. Principal greenhouse gases include carbon dioxide (\\(CO_2\\)), methane (\\(CH_4\\)), nitrous oxide (\\(N2_O\\)), and fluorinated gases such as sulfur hexafluoride (\\(SF_6\\)).\nRole in Climate Change: These gases play a critical role in the warming of our planet. They trap heat in the Earth’s atmosphere by preventing it from escaping back into space, causing global temperatures to rise—a phenomenon known as global warming. This is a major driver of the ongoing changes in our global climate.\nSources: Greenhouse gases are released from both natural and human-made sources. Natural sources include decomposition, respiration, ocean release, and volcanic eruptions. Human activities, such as burning fossil fuels for electricity, heat, and transportation, deforestation, and industrial processes, significantly contribute to the increase in greenhouse gases in the atmosphere.\n\n\n\n\n\nExample of several important greenhouse gases.\n\n\n\n\n\n\nDiagram of the (natural) Greenhouse Effect. This image does not account for additional anthropogenic greenhouses gas emissions."
  },
  {
    "objectID": "intro.html#my-project",
    "href": "intro.html#my-project",
    "title": "Introduction",
    "section": "My Project",
    "text": "My Project\nThis semester, my project revolves around analyzing the atmospheric concentrations of greenhouse gases over time. Using the tools and techniques I’ve learned in ANLY 560, I’ll be investigating why these concentrations are increasing and how this knowledge can be used to inform policy decisions and contribute to the fight against climate change.\nBased on data from NOAA’s Global Monitoring Lab, in 2020, even with decreased emissions due to the COVID-19 pandemic, the global average atmospheric carbon dioxide still managed to reach a new record high of 414.72 ppm (parts per million). This increase, the 5th-highest yearly increase since NOAA’s record began in 1958, shows the unabated rise in greenhouse gases. The main reason for the rise in carbon dioxide is the burning of fossil fuels, which has tripled from an average of 3 billion tons of carbon (11 billion tons of carbon dioxide) per year in the 1960s to 9.5 billion tons of carbon (35 billion tons of carbon dioxide) per year in the 2010s.\n\nWhile carbon dioxide often takes the spotlight in discussions about greenhouse gases, it’s crucial to remember that it’s not the only player. Methane, for example, which is heavily produced by livestock such as cows, has a larger effect on global warming. As part of this project, I will investigate the correlation between methane and carbon dioxide emissions and aim to raise awareness about the environmental impact of methane.\nI am looking into the potential of using greenhouse gas concentrations (by themselves) as predictors for temperature changes. Although I recognize that greenhouse gases alone may not be sufficient to provide accurate temperature predictions, this exploration serves as a starting point for understanding their impact. A good climate model would surely integrate greenhouse gases along with various other factors; however, isolating the influence of these gases will allow us to gauge their predictive power in isolation. By employing advanced statistical methods and time series analysis techniques, I aim to uncover the extent to which greenhouse gas concentrations can be used to forecast temperature fluctuations, shedding light on their significance in the climate change narrative."
  },
  {
    "objectID": "intro.html#guiding-questions",
    "href": "intro.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions",
    "text": "Guiding Questions\nTo navigate through the complex world of greenhouse gases and their impact on our climate, I have outlined several guiding questions for this study:\n\nIs there a greater correlation between carbon-based greenhouse gases (carbon dioxide and methane) versus non-carbon based greenhouse gases (nitrous oxide and sulfur hexafluoride)?\nTo what degree is the trend of the four greenhouse gas concentrations correlated?\nTo what degree is the seasonality of the four greenhouse gas concentrations correlated?\nAre any of the greenhouse gas concentrations strongly or weakly stationary?\nWhat is the daily predictive accuracy of the four greenhouse gases?\nAre there significant differences in seasonal trends between the northern and southern hemispheres, or at the equator?\nIs there a correlation between the proximity of a station to large greenhouse gas emitters and the magnitude of seasonal variation in concentrations?\nHow do the temporal components of the data vary by station or region?\nTo what extent are greenhouse gas concentrations affected by meteorological conditions, specifically temperature?\nDo the data indicate any potential avenues of mitigation of climate change?\n\nThese questions will steer my exploration and analysis, as I endeavor to unravel the secrets that the time series data of atmospheric greenhouse gases hold. Join me on this fascinating journey, where every discovery brings us one step closer to understanding and mitigating the effects of climate change."
  },
  {
    "objectID": "intro.html#project-goals-and-structure",
    "href": "intro.html#project-goals-and-structure",
    "title": "Introduction",
    "section": "Project Goals and Structure",
    "text": "Project Goals and Structure\nThe primary goal of this project is to employ time series analysis techniques to unravel the patterns of greenhouse gas concentrations over time and comprehend their consequences for our climate. Gathering reliable and extensive datasets from reputable sources such as NOAA’s Global Monitoring Lab will lay the groundwork, while data storytelling techniques will be used to create engaging and clear visual representations of the data. Throughout the project, we will focus on understanding the implications of rising greenhouse gas concentrations, the role of human activities in these increases, and exploring potential pathways for climate change mitigation. By using techniques such as ARMA/ARIMA/SARIMA models, ARIMAX/SARIMAX/VAR, and deep learning for time series, and concentrating on these themes, I hope to generate insights that could inform policy decisions and contribute to the worldwide discussion on climate change."
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Sam LeBlanc and I am an aspiring data scientist with a deep passion for public policy, social justice, and the transformative power of data! Currently, I am one week away from graduating from Georgetown University’s McCourt School of Public Policy with a Master’s degree in Data Science for Public Policy. My research is focused on the compelling intersection of data science and public policy, with a particular emphasis on issues related to climate and energy.\nI believe that data, when interpreted correctly, can reveal insights that drive meaningful social change. One area where this is especially crucial is in our understanding of climate change, and I am particularly interested in the role time series analysis can play in this context. The ability to track and analyze weather data over time, for instance, has vast implications for policy decisions, and I’m eager to contribute to this field.\nI am a firm advocate for data literacy. In an era increasingly defined by data, it is imperative that we not only understand how to interpret data, but also how to use it responsibly and effectively to inform discourse and decision-making. This is especially true for data communication inn climate science. This is a field where the top-echelon of researchers are incredibly data fluent, but most people in the green-climate industry are woefully inept at using data to their advantage."
  }
]