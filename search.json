[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "# Load data\nco2 = read.csv('co2.csv')\nsf6 = read.csv('sf6.csv')\nch4 = read.csv('ch4.csv')\nn2o = read.csv('n2o.csv')\n\nBased on the plots and background information, we can expect CO2, CH4, N2O, and SF6 to have interrelationships since they are all greenhouse gases and their concentrations may be influenced by similar factors such as human activity and natural processes. From my domain knowledge, I know that most pollution sources contribute more than one type of greenhouse gas to the atmosphere. Thus, it is reasonable to include all four of these time series in the VAR model.\n\n# Plot time series of the individual variables\ng = ggplot(co2, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Carbon Dioxide Concentration since 1958',x='Year',y='CO2 Concentration (ppm)')\nggplotly(g)\n\n\n\n\ng = ggplot(sf6, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Sulfur Hexafluoride Concentration since 1997',x='Year',y='SF6 Concentration (ppt)')\nggplotly(g)\n\n\n\n\ng = ggplot(ch4, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Methane Concentration since 1983',x='Year',y='CH4 Concentration (ppb)')\nggplotly(g)\n\n\n\n\ng = ggplot(n2o, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Nitrous Oxide Concentration since 2001',x='Year',y='N2O Concentration (ppb)')\nggplotly(g)\n\n\n\n\n\n\n# Convert data into time series objects\nCO2_Concentration <- ts(co2$average, start=c(1958,3), frequency=12)\nCH4_Concentration <- ts(ch4$average, start=c(1983,7), frequency=12)\nN2O_Concentration <- ts(n2o$average, start=c(2001,1), frequency=12)\nSF6_Concentration <- ts(sf6$average, start=c(1997,7), frequency=12)\n\n# Truncate time series to the same date range\nstart_date <- c(2001, 1)\nend_date <- c(2022, 8)\n\nCO2_Concentration <- window(CO2_Concentration, start=start_date, end=end_date)\nCH4_Concentration <- window(CH4_Concentration, start=start_date, end=end_date)\nN2O_Concentration <- window(N2O_Concentration, start=start_date, end=end_date)\nSF6_Concentration <- window(SF6_Concentration, start=start_date, end=end_date)\n\n\n# Plot pairwise relationships between variables\npairs(cbind(\n  CO2=CO2_Concentration, \n  CH4=CH4_Concentration, \n  N2O=N2O_Concentration,\n  SF6=SF6_Concentration\n  ))\n\n\n\n\n\n# Plot all time series together\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nplot.ts(x , main = \"\", xlab = \"\")\n\n\n\n\n\n\nUsing VAR selection methods, we have found several possible values for p.Â These are 6 and 10. This is surprising as I would have thought 12 would be a good choice, as that is the length of the seasonal lag. We will test these new p choices along with the standard p=1 and p=2 choices that will give us something to compare to.\n\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nvar_lag_selection <- VARselect(x, lag.max=12, type=\"both\")\nprint(var_lag_selection)\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10     10      6     10 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -1.477896e+01 -1.664075e+01 -1.780180e+01 -1.809010e+01 -1.853678e+01\nHQ(n)  -1.464208e+01 -1.641262e+01 -1.748243e+01 -1.767947e+01 -1.803490e+01\nSC(n)  -1.443895e+01 -1.607407e+01 -1.700845e+01 -1.707007e+01 -1.729008e+01\nFPE(n)  3.815889e-07  5.930499e-08  1.857741e-08  1.393205e-08  8.920559e-09\n                   6             7             8             9            10\nAIC(n) -1.884106e+01 -1.887424e+01 -1.916644e+01 -1.941668e+01 -1.954365e+01\nHQ(n)  -1.824794e+01 -1.818987e+01 -1.839081e+01 -1.854980e+01 -1.858552e+01\nSC(n)  -1.736769e+01 -1.717420e+01 -1.723972e+01 -1.726329e+01 -1.716358e+01\nFPE(n)  6.588292e-09  6.383928e-09  4.776846e-09  3.729662e-09  3.296301e-09\n                  11            12\nAIC(n) -1.952806e+01 -1.953284e+01\nHQ(n)  -1.847869e+01 -1.839222e+01\nSC(n)  -1.692133e+01 -1.669943e+01\nFPE(n)  3.362150e-09  3.362979e-09\n\n\n\n# Fit different VAR models based on the selected lags\nfitvar1 = VAR(x, p=1, type=\"both\")\nfitvar2 = VAR(x, p=2, type=\"both\")\nfitvar6 = VAR(x, p=6, type=\"both\")\nfitvar10 = VAR(x, p=10, type=\"both\")\n\n\n\n\nUsing cross-validation, we see that the model with the lowest MSE is the p=10 model. Thus, we will be selection this as our best model\n\ncv_var <- function(ts_data, p, k) {\n  n <- length(ts_data[, 1])\n  start_ts <- tsp(ts_data)[1] + (k - 1) / 12\n  rmse <- numeric(n - k)\n  \n  for (i in 1:(n - k)) {\n    train_data <- window(ts_data, end = start_ts + (i - 1) / 12)\n    test_data <- window(ts_data, start = start_ts + (i - 1) / 12 + 1 / 12, end = start_ts + i / 12)\n    var_model <- VAR(train_data, p = p, type = \"const\")\n    forecast <- predict(var_model, n.ahead = 1)$fcst\n    \n    k_step_forecast <- matrix(0, nrow = 1, ncol = ncol(ts_data))\n    for (j in 1:ncol(ts_data)) {\n      k_step_forecast[, j] <- forecast[[j]][1, 1] # Extract the 1-step ahead forecasts\n    }\n    \n    rmse[i] <- sqrt(mean((test_data - k_step_forecast)^2))\n  }\n  \n  return(mean(rmse, na.rm = TRUE))\n}\n\n\n# Calculate RMSE for each model using cross-validation\nk <- 12 # Number of steps ahead for the forecasts\nRMSE_lag1 <- cv_var(x, p = 1, k = k)\nRMSE_lag2 <- cv_var(x, p = 2, k = k)\nRMSE_lag6 <- cv_var(x, p = 6, k = k)\nRMSE_lag10 <- cv_var(x, p = 10, k = k)\n\n\n# Plot the RMSEs\nbarplot(c(RMSE_lag1, RMSE_lag2, RMSE_lag6, RMSE_lag10), names.arg = c(\"Lag 1\", \"Lag 2\",\"Lag 6\", \"Lag 10\"), ylab = \"RMSE\")\n\n\n\n\n\n\n\nThe selected model, VAR(10), was chosen based on its lowest RMSE value obtained through cross-validation, which suggests that this model provides the best fit for the data among the tested models. This model can be utilized to predict future concentrations of CO2, CH4, N2O, and SF6, all of which are greenhouse gases. By analyzing these forecasts, we can gain insights into the potential future trends of these greenhouse gas concentrations. It is important to note, however, that the accuracy of these forecasts depends on the underlying assumptions of the VAR model and the quality of the historical data. As such, the results should be interpreted with caution, considering possible uncertainties and changes in factors influencing greenhouse gas concentrations.\n\nbest_var_model <- fitvar10 # Replace with the best model\nn_forecast <- 120 # Number of months to forecast\nvar_forecast <- forecast(best_var_model, h=n_forecast)\nplot(var_forecast)"
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For this section, we will be using an ARIMA model to forecast future atmospheric concentrations of sulfur hexaflouride.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\n\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\n\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot.\n\n\n\nBased on the PACF and ACF plots, I will be modeling the twice-differenced \\(SF_6\\) concentration because I want to ensure that our series is stationary. Based on the ACF and PACF plots of the twice differenced \\(SF_6\\) concentration shown above, I will select 1 and 3 as possible choices for \\(p\\) and 1,2,3, and 4 as possible choices for \\(q\\). Since this data is twice differenced, I will be using a \\(q\\) value of 2.\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n-1926.281\n-1922.574\n-1926.268\n\n\n0\n2\n1\n-1972.627\n-1965.213\n-1972.587\n\n\n0\n2\n2\n-2017.168\n-2006.047\n-2017.087\n\n\n0\n2\n3\n-2024.154\n-2009.325\n-2024.019\n\n\n2\n2\n0\n-1943.242\n-1932.120\n-1943.161\n\n\n2\n2\n1\n-2021.723\n-2006.894\n-2021.588\n\n\n2\n2\n2\n-2042.100\n-2023.564\n-2041.896\n\n\n2\n2\n3\n-2026.081\n-2003.839\n-2025.796\n\n\n\n\n\nBased on the table, the model with the lowest AIC and BIC scores is the 2,2,2 model. None of the other models are that close to this one in terms of AIC and BIC scores. For the sake of comparison, I will select the worst model of the bunch (0,2,0) to show that even this is very similar to our best model. All of the option here would be adequate for forecasting this series.\n\n\n\nFor a (2,2,2) ARIMA model, the equation is:\n\\[y(t) = c + Ï(1)*y(t-1) + Ï(2)*y(t-2) - Î¸(1)*Îµ(t-1) - Î¸(2)*Îµ(t-2) + Îµ(t)\\]\nwhere:\n\n\\(y(t)\\) is the value of the time series at time t\n\\(c\\) is a constant term (i.e., the mean of the series)\n\\(Ï(1)\\) and \\(Ï(2)\\) are the autoregressive coefficients for lags 1 and 2, respectively\n\\(Î¸(1)\\) and \\(Î¸(2)\\) are the moving average coefficients for lags 1 and 2, respectively\n\\(Îµ(t)\\) is white noise (i.e., a random error term) with mean zero and constant variance\n\nNow, lets consider the model diagnostics using standardized residuals.\n\n\n\n\n\nFor the most part, the residuals plots looks as we would expect. The residuals have a constant mean and variance, and the lagged p-values for the Ljung-Box statistic are under the 0.05 threshold for all except two of the lags. Something that does still appear is a spike in ACF residuals at lags 12 and 24. This would suggest that the series may not have been adequately stationary prior ro modeling. However, we mentioned this earlier, and it wsa determined that twice-differenced was the proper metric to use, as three-times differences would over difference the data and lose valuable insights.\nNext, we can plot the raw data, our chosen model (2,2,2), and the second model (0,2,0) on the same plot to see how they compare on the training data. As expected, both models are nearly identical to the SF6 data. The lines are directly on top of each other and it is impossible to make out any difference between the models and the data.\n\n\n\n\n\n\n\n\nNext, we can compare our ARIMA(2,2,2) model to the auto-arima model. Thankfully, the auto-arima function provides the same model as the one we came to through traditional means. This is validating because there were many choices for p,d, and q, so it is nice to know that the best model was selected in both methods.\n\n\nSeries: SF6_Concentration \nARIMA(2,2,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      1.2422  -0.5007  -1.8154  0.8326\ns.e.  0.0653   0.0550   0.0495  0.0499\n\nsigma^2 = 6.44e-05:  log likelihood = 1026.05\nAIC=-2042.1   AICc=-2041.9   BIC=-2023.56\n\n\n\n\n\nFinally, we can use the ARIMA model to forecast the SF6 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(2,2,2) model.\n\n\n\n\n\n\n\n\nIn order to determine the effectiveness of our model, we need to compare to other benchmark methods. Based on the graphs and accuracy tests below, it is clear that the ARIMA(2,2,2) model far outperforms any of the benchmark methods. Thus, we can be confident when using this model going forward.\n\n\n\n\n\n\npred1=forecast(fit1, h=120);\naccuracy(pred1)\n\n                       ME       RMSE         MAE       MPE       MAPE\nTraining set 0.0007896781 0.00794485 0.006303769 0.0115696 0.09585017\n                   MASE        ACF1\nTraining set 0.02238707 -0.04714439\n\n\n\nf1 <- meanf(SF6_Concentration, h=120)\naccuracy(f1)\n\n                       ME     RMSE     MAE       MPE    MAPE     MASE     ACF1\nTraining set 1.282536e-16 2.071243 1.79491 -9.131477 27.7452 6.374407 0.990158\n\n\n\nf2 <- naive(SF6_Concentration, h=120)\naccuracy(f2)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292\n\n\n\nf3 <- rwf(SF6_Concentration, h=120)\naccuracy(f3)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292"
  },
  {
    "objectID": "arma.html#sarima-co2-concentration",
    "href": "arma.html#sarima-co2-concentration",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA: CO2 Concentration",
    "text": "SARIMA: CO2 Concentration\nIn this section, we will use a Seasonal Autoregressive Integrated Moving Average (SARIMA) model to forecast future atmospheric concentrations of carbon dioxide. As the graph below illustrates, the CO2 concentration, though representing a global mean, exhibits a strong seasonal pattern. Therefore, we will employ a SARIMA model for our analysis.\n\n\n\n\n\n\n\n\n\n\nCO2 Concentration (No differencing)\n\n\n\n\n\nThe above plots display the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for the CO2 concentration time series. As anticipated, the ACF plot reveals a strong correlation between values, signifying that this series is non-stationary. This aligns with our previous assumption that the series is non-stationary due to its pronounced seasonality. To achieve stationarity, we will apply differencing.\n\nautoplot(decompose(CO2_Concentration))\n\n\n\n\n\n\nMonthly Lags\nBefore performing differencing, we examine the monthly lag plots to determine the length of the seasonality. Based on domain knowledge, we suspect a 12-month lag, as the seasonal pattern recurs annually. The lag plots below confirm this hypothesis. The Month 12 lag plots exhibit an almost perfect y=x line, which is superior to all other lags up to 12 months.\n\ngglagplot(CO2_Concentration, do.lines=FALSE, lags=12)+xlab(\"Xi\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Monthly CO2_Concentration (1958-2023)\")\n\n\n\n\n\n\nTwice Differences (Once Seasonal and Once Standard)\nHaving identified the seasonal lag, we can now use differencing to make the series stationary. We apply one seasonal differencing and one standard differencing to achieve stationarity. The resulting ACF and PACF plots exhibit no seasonal pattern or serial autocorrelation, indicating that our data is now differenced and ready for parameter selection and model creation.\n\nCO2_Concentration %>% diff(lag=12) %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\nParameter Selection\nUsing the ACF and PACF plots of the differenced data, we can identify potential values for the parameters p, P, q, and Q. Note that d and D have already been determined based on the differencing performed in the previous step. The ACF and PACF plots suggest the following possible values for each parameter:\n\nq = 0,1\nQ = 0,1\np = 0,1,2,3\nP = 0,1,2,3\nd = 1\nD = 1\n\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*25),nrow=25)\n  \n  for (p in p1:p2){\n    for(q in q1:q2){\n      for(P in P1:P2){\n        for(Q in Q1:Q2){\n          if(p+d+q+P+D+Q<=9){\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=2,P1=1,P2=4,Q1=1,Q2=2,data=CO2_Concentration)\nknitr::kable(output)\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n901.4395\n906.0794\n901.4448\n\n\n0\n1\n0\n0\n1\n1\n502.4804\n511.7602\n502.4962\n\n\n0\n1\n0\n1\n1\n0\n708.5527\n717.8325\n708.5685\n\n\n0\n1\n0\n1\n1\n1\n503.8839\n517.8035\n503.9154\n\n\n0\n1\n0\n2\n1\n0\n638.8982\n652.8178\n638.9297\n\n\n0\n1\n0\n2\n1\n1\n505.8830\n524.4425\n505.9356\n\n\n0\n1\n0\n3\n1\n0\n608.7436\n627.3032\n608.7963\n\n\n0\n1\n1\n0\n1\n0\n808.8603\n818.1401\n808.8761\n\n\n0\n1\n1\n0\n1\n1\n413.9145\n427.8341\n413.9460\n\n\n0\n1\n1\n1\n1\n0\n610.7566\n624.6763\n610.7882\n\n\n0\n1\n1\n1\n1\n1\n415.9110\n434.4705\n415.9636\n\n\n0\n1\n1\n2\n1\n0\n524.8992\n543.4587\n524.9518\n\n\n1\n1\n0\n0\n1\n0\n828.5434\n837.8231\n828.5591\n\n\n1\n1\n0\n0\n1\n1\n433.4853\n447.4049\n433.5168\n\n\n1\n1\n0\n1\n1\n0\n630.4665\n644.3862\n630.4981\n\n\n1\n1\n0\n1\n1\n1\n435.3995\n453.9590\n435.4522\n\n\n1\n1\n0\n2\n1\n0\n551.4616\n570.0211\n551.5143\n\n\n1\n1\n1\n0\n1\n0\n806.1513\n820.0710\n806.1829\n\n\n1\n1\n1\n0\n1\n1\n410.7134\n429.2729\n410.7660\n\n\n1\n1\n1\n1\n1\n0\n609.6100\n628.1695\n609.6627\n\n\n2\n1\n0\n0\n1\n0\n819.1535\n833.0731\n819.1850\n\n\n2\n1\n0\n0\n1\n1\n423.5954\n442.1550\n423.6481\n\n\n2\n1\n0\n1\n1\n0\n621.1240\n639.6835\n621.1767\n\n\n2\n1\n1\n0\n1\n0\n807.3255\n825.8850\n807.3782\n\n\n3\n1\n0\n0\n1\n0\n810.3586\n828.9181\n810.4113\n\n\n\n\n\nAccording to the AIC and BIC scores, the best model is the (1,1,1)(0,1,1) model, as it has the lowest AIC and BIC scores. Several other models also have similar AIC and BIC scores, but upon comparing the top models, the (1,1,1)(0,1,1) model is indeed the best one. However, all the options here would provide adequate forecasts for this series, as their performance is very similar.\n\n\nModel Diagnostics\nNow, letâs examine the model diagnostics using standardized residuals.\n\nmodel_output <- capture.output(sarima(CO2_Concentration,1,1,1,0,1,1,12))\n\n\n\n\n\nfit1 <- Arima(CO2_Concentration, order=c(1,1,1), seasonal=c(0,1,1))\nsummary(fit1)\n\nSeries: CO2_Concentration \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2164  -0.5793  -0.8629\ns.e.  0.0880   0.0738   0.0184\n\nsigma^2 = 0.09906:  log likelihood = -201.36\nAIC=410.71   AICc=410.77   BIC=429.27\n\nTraining set error measures:\n                     ME      RMSE       MAE         MPE       MAPE      MASE\nTraining set 0.02449999 0.3114867 0.2426831 0.006651617 0.06804522 0.1498467\n                     ACF1\nTraining set -0.003090451\n\n\n\n\nCross-Validation\nTo further validate our SARIMA(1,1,1)(0,1,1) model, we will perform a one-step-ahead cross-validation. In this approach, we iteratively train the model on a portion of the data and then use the model to forecast the next point. We compare this forecast to the actual data and calculate the error. This process is repeated for each data point in the time series, and the overall accuracy of the model is assessed using the mean squared error (MSE) of the one-step-ahead forecasts.\n\n# Cross-validation function\none_step_ahead_cv <- function(data, order, seasonal_order) {\n  n <- length(data)\n  errors <- numeric(n)\n  \n  for (t in (length(seasonal_order) + 2):n) {\n    train_data <- window(data, end = t - 1)\n    test_data <- data[t]\n    if (length(train_data) > 0) {\n      model <- Arima(train_data, order = order, seasonal = seasonal_order)\n      forecasted_value <- forecast(model, h = 1)$mean\n      errors[t] <- (forecasted_value - test_data)^2\n    }\n  }\n  \n  mean(errors, na.rm = TRUE)\n}\n\n\n# Perform cross-validation\n# mse <- one_step_ahead_cv(data = CO2_Concentration, order = c(1, 1, 1), seasonal_order = c(0, 1, 1))\n# mse\n\n\n\nBenchmark Methods and Forecast\nTo evaluate the effectiveness of our model, we will compare it to other benchmark methods. The graphs and accuracy tests below demonstrate that the SARIMA(1,1,1)(0,1,1) model significantly outperforms any of the benchmark methods. This gives us confidence in using this model for our forecasts.\n\nautoplot(CO2_Concentration) +\n  autolayer(meanf(CO2_Concentration, h=120),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(CO2_Concentration, h=120),\n            series=\"NaÃ¯ve\", PI=FALSE) +\n  autolayer(snaive(CO2_Concentration, h=120),\n            series=\"SNaÃ¯ve\", PI=FALSE)+\n  autolayer(rwf(CO2_Concentration, h=120, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit1,120), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\nFinally, we can use the SARIMA model to forecast the CO2 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(1,1,1)(0,1,1) model.\n\nfit1 %>% forecast(h=120) %>% autoplot()\n\n\n\n\nIn conclusion, we have successfully built and validated a SARIMA(1,1,1)(0,1,1) model for forecasting future atmospheric concentrations of carbon dioxide. This model effectively accounts for the seasonality present in the CO2 concentration data and outperforms the benchmark methods. Utilizing this model, we can now generate forecasts of CO2 concentrations over the next 10 years and contribute to the understanding of potential future trends in atmospheric CO2 levels."
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "Header\n  \n    Primary card title\n    Some quick example text to build on the card title and make up the bulk of the card's content."
  },
  {
    "objectID": "ds.html#data-exploration",
    "href": "ds.html#data-exploration",
    "title": "Data Sources",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nEnvironmental Protection Agency (EPA): Greenhouse Gas Reporting Program (GHGRP)\nThe Environmental Protection Agencyâs Greenhouse Gas Reporting Program is a critical tool in the fight against climate change. The program provides a comprehensive overview of emissions data from across the country, helping to inform policy decisions and identify areas that need additional attention. The data collected helps the EPA track trends in emissions levels and can be used to inform strategies for reducing greenhouse gas emissions. With the help of this program, the EPA is able to create an effective response to the ever-changing environmental conditions and ensure a sustainable future for the planet.\nThe data can be accessed at the Data Sets section of the GHGRP website.They are stored in a zip file containing data over the last decade. The most interesting data set from the EPA is the record of large polluters. I think it could be very interesting to use this to gauge how station proximity to large emitters effects the amplitude of seasonal cycles."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "Data Visualization with Stock Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCarvanna Stock Value using plotly\n\n\n\n\n\n\n\n\n\nClimate Data"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For this section of the EDA, we will be focusing specifically on the Sulfur Hexafluoride (\\(SF_6\\)). This process was repeated for three other atmospheric concentrations of greenhouse gasses as well, but those have been omitted to allow for an in-depth look at Sulfur Hexafluoride.\nSulfur Hexafluoride is an odorless, colorless, and non-toxic gas. It is mostly used as an electrical insulator, as it has a very high dielectric strength. \\(SF_6\\) also has other uses, such as in the production of integrated circuits and flat panel displays, as a tracer gas in leak detection, and as a propellant in aerosol cans. \\(SF_6\\) is a greenhouse gas, and its global warming potential is estimated to be 23,900 times higher than that of carbon dioxide. As such, SF6 is regulated by the Montreal Protocol and the Kyoto Protocol.\nSimilar to the other greenhouse gases, Sulfur Hexafluoride is measured continually at many monitoring stations across the globe. Then, NOAA produces a monthly estimate that accounts for additional factors like weather and available monitoring stations. Sulfur Hexafluoride is measured in parts per trillion.\n\n\n\n\n\n\n\n\n\nThe above plot depicts the alarming rise in the mean monthly concentration of \\(SF_6\\) since 1997, with an increase of nearly threefold in just 25 years. This is a cause of great concern, as \\(SF_6\\) is one of the most potent greenhouse gases released into the atmosphere today. Even though it exists in parts per trillion, this accumulation of \\(SF_6\\) will continue to contribute to the disruption of the planetâs climate in conjunction with other greenhouse gases. Undoubtedly, this is a worrisome trend that must be addressed before it is too late.\nBased on the above plot, we can see a strongly increasing trend of \\(SF_6\\) that has continued uninterrupted since records began. The trend is very consistent and appears to be almost linear, with just a slight exponential increase detectable. There appear to be only a few months in the 25 years where the \\(SF_6\\) concentration was lower than it was in a previous month. Unlike many other greenhouse gases, the concentration of \\(SF_6\\) does not exhibit strong season variation. Gases like \\(CO_2\\) or \\(CH_4\\) which were discussed previously, see significant change depending on the season and location of the monitoring station, however \\(SF_6\\) does not. Other than that, there appear to be no periodic fluctuations and the level of noise appears to be minimal.\nBased only on background knowledge and the above plot, the data appears to be additive rather than multiplicative. One way to know the series is additive is linearity and this plot appears to be nearly linear. Another method is to look at variance, based on this plot, the variance appears to be consistent, pointing to an additive series. These assumptions will be more easy to identify when looking at the decomposition of the series. Examining the individual components of the series can help to confirm whether the data is additive or multiplicative."
  },
  {
    "objectID": "eda.html#decomposition",
    "href": "eda.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\nAfter decomposing the series, it becomes easier to identify the trend, seasonal, and remainder components. As expected, there is a strong increasing trend that was readily apparent from the initial plot. Additionally, the decomposition process revealed a much subtler seasonal component to this data, which was not easily detected from the initial plot. The seasonal component is highly regular, and appears very similar to the seasonal components of other greenhouse gas concentrations. However, the scale of this seasonal component is miniscule (just 0.01 part per trillion) compared to the increase of the trend, which accounts for an average of 3.5 parts per trillion per year. Similarly, the decomposition reveals that the randomness in the data is very small when compared to the overall trend. This is why the plot appears to be almost linear in nature. It is important to note however, that while randomness may be difficult to detect in the initial plot, it is still an important factor that should be taken into consideration when analyzing the data. Using the decomposition, the series is clearly additive. This is based on the linearly increasing trend as well as the seasonality which does not increase in variance as the series increases.\n\n\n\n\n\n\nThis plot clearly illustrates the strong correlation between the monthly lags for the first 6 months. As expected, the values are highly correlated with their preceding values. Since the series lacks any noticeable seasonality, these lags are even more evident. The lag plots demonstrate that the series has a high degree of autocorrelation, which can be diminished with the use of detrending. By utilizing this technique, we can effectively reduce the influence of autocorrelation and gain a better insight into the data."
  },
  {
    "objectID": "eda.html#stationarity",
    "href": "eda.html#stationarity",
    "title": "Exploratory Data Analysis",
    "section": "Stationarity",
    "text": "Stationarity\n\nSF6 Concentration (No differencing)\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\nSF6 Concentration (1st order difference)\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\nSF6 Concentration (2nd order difference)\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\n\n\nWe can also consider moving average smoothing to identify underlying patterns in the data. For this example, I will be using the \\(CO_2\\) concentration as it provides a more rich comparison than \\(SF_6\\).\nBelow are four graphs of the \\(CO_2\\) concentration at various levels of smoothing. The first plot is the raw data with no smoothing applied. The seasonal variation is readily apparent in this plot, but that causes the trend to be obscured. One way we can view the trend more easily (other than decomposing) is to use moving average smoothing.\n\n\n\n\n\nThe second graph shows a smoothed moving average of the \\(CO_2\\) concentration with a window that is too small (3 months). Since the concentration follows a 12 month seasonal cycle, the 3 month smoothing only serves to decrease the relative size of the season cycle. While this is a good start to get us to a clear trend line, it is not enough. We will need to apply the smooth moving average with a window that matches the seasonal cycle.\n\n\n\n\n\nThe last two graphs are constructed using smooth moving average windows that are multiples of the seasonal cycle length (12 months). Thus, we can no longer see any seasonal variation because each observation now includes data from all parts of the year equally. We could have achieved the same result by decomposing the series, but this is another option if we know the length of a seasonal cycle. Overall, the 12-month and 24 month windows look very similar, especially since this data has a very steady trend."
  },
  {
    "objectID": "ftsm.html#carvana-model-fitting-with-arima-and-garch",
    "href": "ftsm.html#carvana-model-fitting-with-arima-and-garch",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Carvana Model Fitting with ARIMA and GARCH",
    "text": "Carvana Model Fitting with ARIMA and GARCH\nSince my project has exactly nothing to do with financial time series data (which was not a requirement when choosing projects) I will instead focus on a completely random topic because thatâs what the professor told me to do. So fun and worthwhile!\nFirst, letâs get the stock data for our favorite company Carvana! After getting the data, we plot to see what we are working with.\n\nCVNA = getSymbols(\"CVNA\",auto.assign = FALSE, from = \"2014-10-01\",src=\"yahoo\")\nchartSeries(CVNA, theme = chartTheme(\"white\"), \n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\")\n\n\n\nlog(CVNA$`CVNA.Adjusted`) %>% diff() %>% chartSeries()\n\n\n\n\nBased on the data, it would be wise to take the logarithm since the values are highly skewed from when the stock was doing well and when it was doing poorly."
  },
  {
    "objectID": "ftsm.html#stationarity",
    "href": "ftsm.html#stationarity",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stationarity",
    "text": "Stationarity\nAs with most ARCH/GARCH models, we will be modelling the returns rather than the raw data. This is partly why we took the logarithm, so that the returns are on a more reasonable scale from different time periods of the stock.\n\ncvna = ts(CVNA$`CVNA.Adjusted`, start=decimal_date(as.Date(\"2014-10-01\")), frequency = 365.25)\n\n\nautoplot(cvna) +ggtitle(\"Carvana Price\")\n\n\n\n\n\ncvna %>% ggtsdisplay()\n\n\n\n\nPlotting the raw data, we can see that this series is clearly not stationary. There is extremely high correlation between values as well as string seasonality present in the data. Thus, to address the non-stationarity, we will need to do differencing. In addition to differencing, we will also need to calculate the logarithm of the data to account for large variations in price that occurred over the time frame of interest.\n\nreturns = log(cvna) %>% diff()\nreturns %>% ggtsdisplay()\n\n\n\n\nBased on the ACF plot, we can see that after taking the log and differenc of the data, this series is now weakly stationary. There is no need for additional differencing as the ACF and PACF plots are looking good already. Any extra differencing would result in over-differencing and would make modeling more difficult."
  },
  {
    "objectID": "ftsm.html#garchpq-model-fitting-with-arima",
    "href": "ftsm.html#garchpq-model-fitting-with-arima",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH(p,q) model fitting with ARIMA",
    "text": "GARCH(p,q) model fitting with ARIMA\n\nArchTest\nFirst, we check for ARCH effects with the ArchTest() function. We will use a standard significance level of \\(\\alpha=0.05\\) for our null hypothesis test. Because the p-value is much smaller than 0.05, so we reject the null hypothesis and conclude the presence of ARCH(1) effects.\n\nArchTest(returns, lags=1, demean=TRUE)\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 50.596, df = 1, p-value = 1.135e-12\n\n\n\n\nARIMA model\nLetâs fit the ARIMA model first. We follow the same procedure as previously. For more on ARIMA models, check out the other tabs of the website.\n\nARIMA.f=function(p1,p2,q1,q2,data){\n  temp=c()\n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,6*24),nrow=24)\n  \n  for (p in p1:p2){\n    for(q in q1:q2){\n      if(p+1+q<=8){\n        model= Arima(data,order=c(p,1,q))\n        ls[i,]= c(p,1,q,model$aic,model$bic,model$aicc)\n        i=i+1\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n}\n\n\noutput = ARIMA.f(0,4,0,4,data=log(cvna))\noutput\n\n   p d q       AIC       BIC      AICc\n1  0 1 0 -4029.563 -4024.244 -4029.561\n2  0 1 1 -4029.215 -4018.576 -4029.207\n3  0 1 2 -4027.552 -4011.592 -4027.536\n4  0 1 3 -4026.393 -4005.113 -4026.366\n5  0 1 4 -4027.981 -4001.382 -4027.941\n6  1 1 0 -4029.258 -4018.618 -4029.250\n7  1 1 1 -4027.303 -4011.343 -4027.287\n8  1 1 2 -4025.635 -4004.355 -4025.608\n9  1 1 3 -4025.688 -3999.089 -4025.648\n10 1 1 4 -4026.002 -3994.083 -4025.946\n11 2 1 0 -4027.454 -4011.494 -4027.438\n12 2 1 1 -4025.262 -4003.983 -4025.236\n13 2 1 2 -4034.277 -4007.678 -4034.237\n14 2 1 3 -4026.394 -3994.475 -4026.338\n15 2 1 4 -4024.224 -3986.985 -4024.149\n16 3 1 0 -4026.635 -4005.355 -4026.608\n17 3 1 1 -4026.035 -3999.436 -4025.995\n18 3 1 2 -4024.603 -3992.684 -4024.547\n19 3 1 3 -4022.766 -3985.527 -4022.691\n20 3 1 4 -4022.240 -3979.681 -4022.144\n21 4 1 0 -4028.141 -4001.541 -4028.101\n22 4 1 1 -4026.153 -3994.233 -4026.097\n23 4 1 2 -4024.293 -3987.054 -4024.219\n24 4 1 3 -4022.402 -3979.843 -4022.306\n\n\nARIMA(2,1,2)\n\nauto.arima(log(cvna))\n\nSeries: log(cvna) \nARIMA(5,2,0) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5\n      -0.7921  -0.6080  -0.4742  -0.3629  -0.1890\ns.e.   0.0253   0.0311   0.0327   0.0312   0.0254\n\nsigma^2 = 0.004752:  log likelihood = 1896.75\nAIC=-3781.5   AICc=-3781.44   BIC=-3749.58\n\n\nUsing the auto.arima function, we can see the the best model is the ARIMA(5,2,0). But the ACF and PACF does not suggest these are good values. Since the auto arima function is sometimes un-trsutworthy, I am still going to go with the (2,1,2) ARIMA as determined by the manual arima model selection.\n\nmodel_output <- capture.output(Arima(cvna, order=c(2,1,2),include.drift = TRUE))\n\nUsing the standardized residuals plots, we can see that the ARIMA model is insufficient to accurately model the financial time series data. Thus, we will need to use the GARCH model on top of the residuals from the ARIMA model. This is a common tactic in financial time series which has a much different pattern than other time series like the greenhouse gases for the remainder of the project. Thus, we will continue modeling with the GARCH model. I choice the GARCH values based on the ACF graph, of the ARIMA mode. As we can see from this chart, we should try all p,q values between 0 and 4."
  },
  {
    "objectID": "ftsm.html#garch-model",
    "href": "ftsm.html#garch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH model",
    "text": "GARCH model\nNext, we will fit the ARIMA model and then fit a GARCH model to the residuals of the ARIMA model.\n\ndata=log(cvna)\n\narima.fit=Arima(data,order=c(2,1,2),include.drift = TRUE)\narima.res=arima.fit$residuals\n\nacf(arima.res)\n\n\n\nacf(arima.res^2) # clear correlation 1,2,4\n\n\n\npacf(arima.res^2) # clear correlation 1,4\n\n\n\n\n\nmodel = list()\ncc = 1\nfor (p in 1:4) {\n  for (q in 1:4) {\n    model[[cc]] = garch(arima.res,order=c(q,p),trace=F)\n    cc = cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nGARCH_AIC = sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n[1] 2\n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2  \n1.774e-05  8.952e-02  3.755e-01  5.358e-01  \n\n\nAfter trying all p,q values from 0,4 in combination, the GARCH(1,2) model is the best and has the lowest combination of AIC and BIC models. I tested all of the models, but only included the output from the best one. I attempted to use cross validation but was unsuccessful in making comparisons between the different models.\n\nsummary(garchFit(~garch(1,2), arima.res,trace = F))\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x000001971d76a758>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n3.2701e-04  1.7583e-05  8.9457e-02  3.7710e-01  5.3440e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.270e-04   1.106e-03    0.296  0.76749    \nomega  1.758e-05   8.715e-06    2.018  0.04363 *  \nalpha1 8.946e-02   1.522e-02    5.876 4.19e-09 ***\nbeta1  3.771e-01   1.453e-01    2.595  0.00946 ** \nbeta2  5.344e-01   1.372e-01    3.896 9.79e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2310.996    normalized:  1.529448 \n\nDescription:\n Mon May  1 12:44:34 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1408.668  0        \n Shapiro-Wilk Test  R    W      0.9548206 0        \n Ljung-Box Test     R    Q(10)  12.31858  0.2643006\n Ljung-Box Test     R    Q(15)  19.50652  0.1916912\n Ljung-Box Test     R    Q(20)  20.63407  0.4189453\n Ljung-Box Test     R^2  Q(10)  6.022325  0.8133841\n Ljung-Box Test     R^2  Q(15)  9.940587  0.8234604\n Ljung-Box Test     R^2  Q(20)  11.10935  0.9433373\n LM Arch Test       R    TR^2   8.351932  0.757059 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.052277 -3.034671 -3.052299 -3.045721 \n\n\nSince all the models has similar AIC ,BIC values, I would go with GARCH(1,1) which all the coefficients are significant.\n\nFinal Model\nThe final model has a decent but not great fir for the Carvana stock return data. All of the errors except for mu are significant but the Ljung-Box statistics are well over the standard threshold. Since there is a mix of indicators, this tells us that the model is decent but not quantifiable better than the simpler ARIMA model. Thus, in this case I would rely on the ARIMA since it is a simpler specification.\n\narima.fit=Arima(data,order=c(2,1,2),include.drift = TRUE)\nsummary(arima.fit)\n\nSeries: data \nARIMA(2,1,2) with drift \n\nCoefficients:\n         ar1      ar2      ma1    ma2    drift\n      0.8796  -0.9659  -0.8770  0.987  -0.0003\ns.e.  0.0116   0.0193   0.0078  0.014   0.0017\n\nsigma^2 = 0.004033:  log likelihood = 2022.16\nAIC=-4032.33   AICc=-4032.27   BIC=-4000.41\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE     MAPE       MASE\nTraining set 3.270138e-05 0.06337709 0.04147109 -0.03123005 1.238118 0.02851845\n                   ACF1\nTraining set 0.02665098\n\n\n\nfinal.fit = garchFit(~garch(1,2), arima.res,trace = F)\nsummary(final.fit)\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x000001971f6ebfb0>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n3.2701e-04  1.7583e-05  8.9457e-02  3.7710e-01  5.3440e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.270e-04   1.106e-03    0.296  0.76749    \nomega  1.758e-05   8.715e-06    2.018  0.04363 *  \nalpha1 8.946e-02   1.522e-02    5.876 4.19e-09 ***\nbeta1  3.771e-01   1.453e-01    2.595  0.00946 ** \nbeta2  5.344e-01   1.372e-01    3.896 9.79e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2310.996    normalized:  1.529448 \n\nDescription:\n Mon May  1 12:44:35 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1408.668  0        \n Shapiro-Wilk Test  R    W      0.9548206 0        \n Ljung-Box Test     R    Q(10)  12.31858  0.2643006\n Ljung-Box Test     R    Q(15)  19.50652  0.1916912\n Ljung-Box Test     R    Q(20)  20.63407  0.4189453\n Ljung-Box Test     R^2  Q(10)  6.022325  0.8133841\n Ljung-Box Test     R^2  Q(15)  9.940587  0.8234604\n Ljung-Box Test     R^2  Q(20)  11.10935  0.9433373\n LM Arch Test       R    TR^2   8.351932  0.757059 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.052277 -3.034671 -3.052299 -3.045721 \n\n\n\ncapture.output(final.fit)\n\n [1] \"\"                                                               \n [2] \"Title:\"                                                         \n [3] \" GARCH Modelling \"                                              \n [4] \"\"                                                               \n [5] \"Call:\"                                                          \n [6] \" garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \"\n [7] \"\"                                                               \n [8] \"Mean and Variance Equation:\"                                    \n [9] \" data ~ garch(1, 2)\"                                            \n[10] \"<environment: 0x000001971f6ebfb0>\"                              \n[11] \" [data = arima.res]\"                                            \n[12] \"\"                                                               \n[13] \"Conditional Distribution:\"                                      \n[14] \" norm \"                                                         \n[15] \"\"                                                               \n[16] \"Coefficient(s):\"                                                \n[17] \"        mu       omega      alpha1       beta1       beta2  \"   \n[18] \"3.2701e-04  1.7583e-05  8.9457e-02  3.7710e-01  5.3440e-01  \"   \n[19] \"\"                                                               \n[20] \"Std. Errors:\"                                                   \n[21] \" based on Hessian \"                                             \n[22] \"\"                                                               \n[23] \"Error Analysis:\"                                                \n[24] \"        Estimate  Std. Error  t value Pr(>|t|)    \"             \n[25] \"mu     3.270e-04   1.106e-03    0.296  0.76749    \"             \n[26] \"omega  1.758e-05   8.715e-06    2.018  0.04363 *  \"             \n[27] \"alpha1 8.946e-02   1.522e-02    5.876 4.19e-09 ***\"             \n[28] \"beta1  3.771e-01   1.453e-01    2.595  0.00946 ** \"             \n[29] \"beta2  5.344e-01   1.372e-01    3.896 9.79e-05 ***\"             \n[30] \"---\"                                                            \n[31] \"Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\" \n[32] \"\"                                                               \n[33] \"Log Likelihood:\"                                                \n[34] \" 2310.996    normalized:  1.529448 \"                            \n[35] \"\"                                                               \n[36] \"Description:\"                                                   \n[37] \" Mon May  1 12:44:35 2023 by user: sleblanc \"                   \n[38] \"\"                                                               \n\n\nLet \\(x_t\\) be the time series data and \\(z_t\\) be the residuals of the ARIMA model. Then the GARCH(1, 2) model is defined by:\n\\[z_t = \\sigma_t * \\epsilon_t\\]\nwhere \\(\\epsilon_t\\) follows a standard normal distribution (mean = 0, sd = 1), and the conditional variance \\(\\sigma_t^2\\) is given by:\n\\[\\sigma_t^2 = \\omega + \\alpha_1 * z_(t-1)^2 + \\beta_1 * \\sigma_(t-1)^2 + \\beta_2 * \\sigma_(t-2)^2\\]\nwith the estimated coefficients being:\n\\(\\omega\\) = 0.00001667 \\(\\alpha_1\\) = 0.08827887 \\(\\beta_1\\) = 0.37861131 \\(\\beta_2\\) = 0.53446470 The model is fitted to the residuals of an ARIMA model (arima.res).\n\n\nForecast\n\npredict(final.fit, n.ahead = 100, plot=TRUE)\n\n\n\n\n    meanForecast  meanError standardDeviation lowerInterval upperInterval\n1   0.0003270138 0.06720245        0.06720245    -0.1313874     0.1320414\n2   0.0003270138 0.06778052        0.06778052    -0.1325204     0.1331744\n3   0.0003270138 0.06763475        0.06763475    -0.1322347     0.1328887\n4   0.0003270138 0.06787450        0.06787450    -0.1327046     0.1333586\n5   0.0003270138 0.06790845        0.06790845    -0.1327711     0.1334251\n6   0.0003270138 0.06805196        0.06805196    -0.1330524     0.1337064\n7   0.0003270138 0.06813690        0.06813690    -0.1332188     0.1338729\n8   0.0003270138 0.06825292        0.06825292    -0.1334462     0.1341003\n9   0.0003270138 0.06835221        0.06835221    -0.1336409     0.1342949\n10  0.0003270138 0.06846027        0.06846027    -0.1338527     0.1345067\n11  0.0003270138 0.06856352        0.06856352    -0.1340550     0.1347090\n12  0.0003270138 0.06866918        0.06866918    -0.1342621     0.1349161\n13  0.0003270138 0.06877341        0.06877341    -0.1344664     0.1351204\n14  0.0003270138 0.06887826        0.06887826    -0.1346719     0.1353259\n15  0.0003270138 0.06898263        0.06898263    -0.1348765     0.1355305\n16  0.0003270138 0.06908712        0.06908712    -0.1350813     0.1357353\n17  0.0003270138 0.06919140        0.06919140    -0.1352856     0.1359397\n18  0.0003270138 0.06929565        0.06929565    -0.1354900     0.1361440\n19  0.0003270138 0.06939978        0.06939978    -0.1356940     0.1363481\n20  0.0003270138 0.06950383        0.06950383    -0.1358980     0.1365520\n21  0.0003270138 0.06960778        0.06960778    -0.1361017     0.1367558\n22  0.0003270138 0.06971164        0.06971164    -0.1363053     0.1369593\n23  0.0003270138 0.06981542        0.06981542    -0.1365087     0.1371627\n24  0.0003270138 0.06991910        0.06991910    -0.1367119     0.1373659\n25  0.0003270138 0.07002269        0.07002269    -0.1369149     0.1375690\n26  0.0003270138 0.07012620        0.07012620    -0.1371178     0.1377718\n27  0.0003270138 0.07022962        0.07022962    -0.1373205     0.1379745\n28  0.0003270138 0.07033295        0.07033295    -0.1375230     0.1381771\n29  0.0003270138 0.07043619        0.07043619    -0.1377254     0.1383794\n30  0.0003270138 0.07053934        0.07053934    -0.1379276     0.1385816\n31  0.0003270138 0.07064241        0.07064241    -0.1381296     0.1387836\n32  0.0003270138 0.07074539        0.07074539    -0.1383314     0.1389854\n33  0.0003270138 0.07084828        0.07084828    -0.1385331     0.1391871\n34  0.0003270138 0.07095109        0.07095109    -0.1387346     0.1393886\n35  0.0003270138 0.07105382        0.07105382    -0.1389359     0.1395899\n36  0.0003270138 0.07115646        0.07115646    -0.1391371     0.1397911\n37  0.0003270138 0.07125902        0.07125902    -0.1393381     0.1399921\n38  0.0003270138 0.07136149        0.07136149    -0.1395389     0.1401930\n39  0.0003270138 0.07146388        0.07146388    -0.1397396     0.1403936\n40  0.0003270138 0.07156618        0.07156618    -0.1399401     0.1405942\n41  0.0003270138 0.07166840        0.07166840    -0.1401405     0.1407945\n42  0.0003270138 0.07177054        0.07177054    -0.1403407     0.1409947\n43  0.0003270138 0.07187260        0.07187260    -0.1405407     0.1411947\n44  0.0003270138 0.07197458        0.07197458    -0.1407406     0.1413946\n45  0.0003270138 0.07207648        0.07207648    -0.1409403     0.1415943\n46  0.0003270138 0.07217829        0.07217829    -0.1411398     0.1417939\n47  0.0003270138 0.07228003        0.07228003    -0.1413392     0.1419933\n48  0.0003270138 0.07238168        0.07238168    -0.1415385     0.1421925\n49  0.0003270138 0.07248326        0.07248326    -0.1417376     0.1423916\n50  0.0003270138 0.07258475        0.07258475    -0.1419365     0.1425905\n51  0.0003270138 0.07268617        0.07268617    -0.1421353     0.1427893\n52  0.0003270138 0.07278751        0.07278751    -0.1423339     0.1429879\n53  0.0003270138 0.07288877        0.07288877    -0.1425324     0.1431864\n54  0.0003270138 0.07298995        0.07298995    -0.1427307     0.1433847\n55  0.0003270138 0.07309106        0.07309106    -0.1429288     0.1435829\n56  0.0003270138 0.07319209        0.07319209    -0.1431268     0.1437809\n57  0.0003270138 0.07329304        0.07329304    -0.1433247     0.1439787\n58  0.0003270138 0.07339392        0.07339392    -0.1435224     0.1441764\n59  0.0003270138 0.07349472        0.07349472    -0.1437200     0.1443740\n60  0.0003270138 0.07359544        0.07359544    -0.1439174     0.1445714\n61  0.0003270138 0.07369609        0.07369609    -0.1441147     0.1447687\n62  0.0003270138 0.07379666        0.07379666    -0.1443118     0.1449658\n63  0.0003270138 0.07389716        0.07389716    -0.1445088     0.1451628\n64  0.0003270138 0.07399759        0.07399759    -0.1447056     0.1453596\n65  0.0003270138 0.07409794        0.07409794    -0.1449023     0.1455563\n66  0.0003270138 0.07419822        0.07419822    -0.1450988     0.1457528\n67  0.0003270138 0.07429842        0.07429842    -0.1452952     0.1459492\n68  0.0003270138 0.07439855        0.07439855    -0.1454915     0.1461455\n69  0.0003270138 0.07449861        0.07449861    -0.1456876     0.1463416\n70  0.0003270138 0.07459860        0.07459860    -0.1458836     0.1465376\n71  0.0003270138 0.07469851        0.07469851    -0.1460794     0.1467334\n72  0.0003270138 0.07479836        0.07479836    -0.1462751     0.1469291\n73  0.0003270138 0.07489813        0.07489813    -0.1464706     0.1471247\n74  0.0003270138 0.07499783        0.07499783    -0.1466660     0.1473201\n75  0.0003270138 0.07509746        0.07509746    -0.1468613     0.1475153\n76  0.0003270138 0.07519702        0.07519702    -0.1470564     0.1477105\n77  0.0003270138 0.07529651        0.07529651    -0.1472514     0.1479055\n78  0.0003270138 0.07539593        0.07539593    -0.1474463     0.1481003\n79  0.0003270138 0.07549529        0.07549529    -0.1476410     0.1482951\n80  0.0003270138 0.07559457        0.07559457    -0.1478356     0.1484896\n81  0.0003270138 0.07569378        0.07569378    -0.1480301     0.1486841\n82  0.0003270138 0.07579293        0.07579293    -0.1482244     0.1488784\n83  0.0003270138 0.07589201        0.07589201    -0.1484186     0.1490726\n84  0.0003270138 0.07599102        0.07599102    -0.1486126     0.1492667\n85  0.0003270138 0.07608996        0.07608996    -0.1488066     0.1494606\n86  0.0003270138 0.07618883        0.07618883    -0.1490004     0.1496544\n87  0.0003270138 0.07628764        0.07628764    -0.1491940     0.1498480\n88  0.0003270138 0.07638638        0.07638638    -0.1493875     0.1500416\n89  0.0003270138 0.07648506        0.07648506    -0.1495809     0.1502350\n90  0.0003270138 0.07658367        0.07658367    -0.1497742     0.1504282\n91  0.0003270138 0.07668221        0.07668221    -0.1499674     0.1506214\n92  0.0003270138 0.07678069        0.07678069    -0.1501604     0.1508144\n93  0.0003270138 0.07687910        0.07687910    -0.1503533     0.1510073\n94  0.0003270138 0.07697745        0.07697745    -0.1505460     0.1512000\n95  0.0003270138 0.07707573        0.07707573    -0.1507386     0.1513927\n96  0.0003270138 0.07717395        0.07717395    -0.1509312     0.1515852\n97  0.0003270138 0.07727211        0.07727211    -0.1511235     0.1517776\n98  0.0003270138 0.07737020        0.07737020    -0.1513158     0.1519698\n99  0.0003270138 0.07746823        0.07746823    -0.1515079     0.1521619\n100 0.0003270138 0.07756619        0.07756619    -0.1516999     0.1523540"
  },
  {
    "objectID": "ftsm.html#volatality-plot",
    "href": "ftsm.html#volatality-plot",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volatality plot",
    "text": "Volatality plot\nFinally, we should also consider the volatility of the data. Volatility is a key feature of financial time series data and will have large effect on the model we end up choosing. Below is the plot of the volatility of the Carvana data. When looking at the volatility plot, we see several large spikes that stick out. The first is in March 2020, which was right at the beginning of COVID, so the volatility is expected. The second large spike occurs at the send of 2022, which is unexpected because the value of the stock is so low at that point. However, it could be thatwith such a low stock price, relative changes have a greater effect. A $10 gain when the stock is at $10 is a 100% increase, but when the stock was at $400 this would be just a 2.5% gain. So it is not crazy that the stock because much more volatile as they value fell off a cliff in 2022.\n\nht = final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\nCVNA=data.frame(CVNA)\nCVNA = data.frame(CVNA,rownames(CVNA))\ncolnames(CVNA)[7] = \"date\"\nCVNA$date=as.Date(CVNA$date,\"%Y-%m-%d\")\nstr(CVNA)\n\n'data.frame':   1511 obs. of  7 variables:\n $ CVNA.Open    : num  13.5 11.6 10.95 10.18 8.59 ...\n $ CVNA.High    : num  13.9 11.7 10.9 10.2 10 ...\n $ CVNA.Low     : num  10.7 10.7 10 8.18 8.14 9.75 10.5 10.4 9.61 10.4 ...\n $ CVNA.Close   : num  11.1 10.77 10.1 8.72 9.98 ...\n $ CVNA.Volume  : num  11297800 1291300 991500 3356500 1840200 ...\n $ CVNA.Adjusted: num  11.1 10.77 10.1 8.72 9.98 ...\n $ date         : Date, format: \"2017-04-28\" \"2017-05-01\" ...\n\ndata= data.frame(ht,CVNA$date)\nggplot(data, aes(y = ht, x = CVNA.date)) + geom_line(col = '#009933') + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatality plot of Carvana Stock\")"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This semester, I am focusing on the topic of weather and climate data. In particular, I am interested in exploring the atmospheric concentrations of greenhouse gases over time. Through time series analysis, I aim to gain a better understanding of why these concentrations are increasing and how this information can be used to inform policy decisions.\nBased on analysis from NOAAâs Global Monitoring Lab, in 2020, despite the decrease emissions as a result of the COVID-19 pandemic, the global average atmospheric carbon dioxide reached a new record high of 414.72 ppm (parts per million) This represents an increase of 2.58 ppm over the previous year, and is the 5th-highest yearly increase since NOAAâs record began in 1958. The main reason for the rise in carbon dioxide is burning fossil fuels, which has tripled from an average of 3 billion tons of carbon (11 billion tons of carbon dioxide) per year in the 1960s to 9.5 billion tons of carbon (35 billion tons of carbon dioxide) per year in the 2010s.\nCarbon dioxide has been garnering the most attention in regards to greenhouse gases, however, it is not the most damaging of them. Methane, which is heavily produced by livestock such as cows, has a larger effect on global warming. With this project, I intend to investigate the correlation between methane and carbon dioxide and spread awareness of methaneâs environmental impact.\n\nWhat is a Time Series?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\n\n\nQuestions\n\nIs there a greater correlation between carbon-based greenhouse gases (carbon dioxide and methane) versus non-carbon based greenhouse gases (nitrous oxide and sulfur hexafluoride)?\nTo what degree is the trend of the four greenhouse gas concentrations correlated?\nTo what degree is the seasonality of the four greenhouse gas concentrations correlated?\nAre any of the greenhouse gas concentrations strongly or weakly stationary?\nWhat is the daily predictive accuracy of the four greenhouse gases?\nAre there significant differences in seasonal trends between the northern and southern hemispheres, or at the equator?\nIs there a correlation between the proximity of a station to large greenhouse gas emitters and the magnitude of seasonal variation in concentrations?\nHow do the temporal components of the data vary by station or region?\nTo what extent are greenhouse gas concentrations affected by meteorological conditions, specifically temperature?\nDo the data indicate any potential avenues of mitigation of climate change?"
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "About Me",
    "section": "",
    "text": "Hello, my name is Sam LeBlanc!\nI am an ambitious and dedicated data scientist with a passion for using data to drive positive social change. As a second-year graduate student at Georgetown Universityâs McCourt School of Public Policy, I am working towards my Masterâs degree in Data Science for Public Policy. My research focuses on exploring the intersection between data science, public policy, and social justice. In my free time, I enjoy playing basketball, exploring the outdoors, and listening to podcasts. I am also an active volunteer and am dedicated to helping my local community. I am particularly interested in time series analysis and its implications for public policy. I am eager to explore the potential of using time series to analyze weather data and its impact on policy decisions."
  },
  {
    "objectID": "sunspot.html",
    "href": "sunspot.html",
    "title": "AN560: Sam LeBlanc",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN,LSTM,GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\nimport plotly.io as pio\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "sunspot.html#download-and-explore-data",
    "href": "sunspot.html#download-and-explore-data",
    "title": "AN560: Sam LeBlanc",
    "section": "Download and explore data",
    "text": "Download and explore data\n\n\ndf = pd.read_csv('co2.csv')\ndf = df[['year','month','average']]\nprint(df)\n\n     year  month  average\n0    1958      3   315.70\n1    1958      4   317.45\n2    1958      5   317.51\n3    1958      6   317.24\n4    1958      7   315.86\n..    ...    ...      ...\n773  2022      8   417.19\n774  2022      9   415.95\n775  2022     10   415.78\n776  2022     11   417.51\n777  2022     12   418.95\n\n[778 rows x 3 columns]\n\n\n\nX = np.array(df[\"average\"].values.astype('float32')).reshape(df.shape[0],1)\nprint(X.shape)\n\n(778, 1)\n\n\n\nprint(X[0:10])\n\n[[315.7 ]\n [317.45]\n [317.51]\n [317.24]\n [315.86]\n [314.93]\n [313.2 ]\n [312.43]\n [313.33]\n [314.67]]"
  },
  {
    "objectID": "sunspot.html#visualization-plotting-function",
    "href": "sunspot.html#visualization-plotting-function",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualization plotting function",
    "text": "Visualization plotting function\n\ndef plotly_line_plot(t,y,title=\"Plot\", x_label=\"Months since March 1958\", y_label=\"CO2 Concentration\"):\n\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n\n    for i in range(1,len(y)):\n        if len(t[i])==1:\n            #print(t[i],y[i])\n            fig.add_scatter(x=t[i],y=y[i])\n        else:\n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n\n    fig.update_layout(\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"plotly_white\",\n        showlegend=False\n    )\n    \n    fig.show()"
  },
  {
    "objectID": "sunspot.html#visualize",
    "href": "sunspot.html#visualize",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize",
    "text": "Visualize\n\n# SINGLE SERIES \nt=[*range(0,len(X))]\nplotly_line_plot([t],[X[:,0]],title=\"Atmospheric CO2 Concentration since March 1958\")"
  },
  {
    "objectID": "sunspot.html#utility-function-test-train-split-data",
    "href": "sunspot.html#utility-function-test-train-split-data",
    "title": "AN560: Sam LeBlanc",
    "section": "Utility function: test-train split data",
    "text": "Utility function: test-train split data\n\n# Parameter split_percent defines the ratio of training examples\ndef get_train_test(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\ntrain_data, test_data, data = get_train_test(X)\n\nprint(train_data.shape)\nprint(test_data.shape)\n\n(622,)\n(156,)"
  },
  {
    "objectID": "sunspot.html#visualize-training-test-split",
    "href": "sunspot.html#visualize-training-test-split",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize: training-test split",
    "text": "Visualize: training-test split\n\n# SINGLE SERIES \nt1=[*range(0,len(train_data))]\nt2=len(train_data)+np.array([*range(0,len(test_data))])\nplotly_line_plot([t1,t2],[train_data,test_data],title=\"Sunspots per month since 1749-01\")"
  },
  {
    "objectID": "sunspot.html#re-format-data-into-required-shape",
    "href": "sunspot.html#re-format-data-into-required-shape",
    "title": "AN560: Sam LeBlanc",
    "section": "Re-format data into required shape",
    "text": "Re-format data into required shape\n\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] #if time_steps=10 remove every 10th entry\n    X = dat[X_ind]; \n\n    #PLOT\n    if(plot_data_partition):\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    #RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n#PARTITION DATA\np=12 # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\n\nprint(testX.shape,testY.shape)\nprint(trainX.shape,trainY.shape)\nprint(type(trainX))\n\n(12, 11, 1) (12,)\n(51, 11, 1) (51,)\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "sunspot.html#visualize-1",
    "href": "sunspot.html#visualize-1",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize",
    "text": "Visualize\n\n## Build list \ntmp1=[]; tmp2=[]; tmp3=[]; count=0\nfor i in range(0,trainX.shape[0]):\n    # tmp1.append()\n    tmp1.append(count+np.array([*range(0,trainX[i,:,0].shape[0])]))\n    tmp1.append([count+trainX[i,:,0].shape[0]]); #print(([count+trainX[i,:,0].shape[0]]))\n    # tmp1.append([count+trainX[i,:,0].shape[0]+1])\n    tmp2.append(trainX[i,:,0])\n    tmp2.append([trainY[i]]); #print([trainY[i]])\n    # tmp2.append([trainY[i]])\n\n    count+=trainX[i,:,0].shape[0]+1\n\n    # print(i,trainX[i,:,0].shape)\n# print(tmp1)\n# print(tmp2)\nplotly_line_plot(tmp1,tmp2,title=\"Sunspots per month since 1749-01\")"
  },
  {
    "objectID": "sunspot.html#model-and-training-parameters",
    "href": "sunspot.html#model-and-training-parameters",
    "title": "AN560: Sam LeBlanc",
    "section": "Model and training parameters",
    "text": "Model and training parameters\n\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=60\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\nprint(trainX.shape,p,trainY.shape)\n\n# trainY=trainY.reshape(trainY.shape[0],1)\n# testY=testY.reshape(testY.shape[0],1)\nprint(p,trainX.shape,testX.shape,trainY.shape,testY.shape)\n\n(51, 11, 1) 12 (51,)\n12 (51, 11, 1) (12, 11, 1) (51,) (12,)"
  },
  {
    "objectID": "sunspot.html#create-model",
    "href": "sunspot.html#create-model",
    "title": "AN560: Sam LeBlanc",
    "section": "Create model",
    "text": "Create model\n\nfrom tensorflow.keras import regularizers\n\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\n# model.add(SimpleRNN(\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \nrecurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 3)                 54        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "sunspot.html#train-model",
    "href": "sunspot.html#train-model",
    "title": "AN560: Sam LeBlanc",
    "section": "Train model",
    "text": "Train model\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\nEpoch 1/60\n4/4 - 4s - loss: 0.5047 - val_loss: 1.2623 - 4s/epoch - 1s/step\nEpoch 2/60\n4/4 - 0s - loss: 0.4825 - val_loss: 1.1819 - 95ms/epoch - 24ms/step\nEpoch 3/60\n4/4 - 0s - loss: 0.4461 - val_loss: 1.1225 - 97ms/epoch - 24ms/step\nEpoch 4/60\n4/4 - 0s - loss: 0.4158 - val_loss: 1.0692 - 84ms/epoch - 21ms/step\nEpoch 5/60\n4/4 - 0s - loss: 0.3982 - val_loss: 1.0208 - 79ms/epoch - 20ms/step\nEpoch 6/60\n4/4 - 0s - loss: 0.3843 - val_loss: 0.9750 - 94ms/epoch - 23ms/step\nEpoch 7/60\n4/4 - 0s - loss: 0.3654 - val_loss: 0.9313 - 94ms/epoch - 24ms/step\nEpoch 8/60\n4/4 - 0s - loss: 0.3603 - val_loss: 0.8886 - 100ms/epoch - 25ms/step\nEpoch 9/60\n4/4 - 0s - loss: 0.3351 - val_loss: 0.8505 - 95ms/epoch - 24ms/step\nEpoch 10/60\n4/4 - 0s - loss: 0.3291 - val_loss: 0.8133 - 96ms/epoch - 24ms/step\nEpoch 11/60\n4/4 - 0s - loss: 0.3130 - val_loss: 0.7783 - 94ms/epoch - 24ms/step\nEpoch 12/60\n4/4 - 0s - loss: 0.3018 - val_loss: 0.7457 - 85ms/epoch - 21ms/step\nEpoch 13/60\n4/4 - 0s - loss: 0.2984 - val_loss: 0.7136 - 99ms/epoch - 25ms/step\nEpoch 14/60\n4/4 - 0s - loss: 0.2819 - val_loss: 0.6836 - 104ms/epoch - 26ms/step\nEpoch 15/60\n4/4 - 0s - loss: 0.2760 - val_loss: 0.6537 - 87ms/epoch - 22ms/step\nEpoch 16/60\n4/4 - 0s - loss: 0.2732 - val_loss: 0.6259 - 91ms/epoch - 23ms/step\nEpoch 17/60\n4/4 - 0s - loss: 0.2661 - val_loss: 0.5993 - 98ms/epoch - 25ms/step\nEpoch 18/60\n4/4 - 0s - loss: 0.2513 - val_loss: 0.5782 - 101ms/epoch - 25ms/step\nEpoch 19/60\n4/4 - 0s - loss: 0.2470 - val_loss: 0.5559 - 100ms/epoch - 25ms/step\nEpoch 20/60\n4/4 - 0s - loss: 0.2414 - val_loss: 0.5353 - 97ms/epoch - 24ms/step\nEpoch 21/60\n4/4 - 0s - loss: 0.2380 - val_loss: 0.5162 - 96ms/epoch - 24ms/step\nEpoch 22/60\n4/4 - 0s - loss: 0.2305 - val_loss: 0.4983 - 100ms/epoch - 25ms/step\nEpoch 23/60\n4/4 - 0s - loss: 0.2229 - val_loss: 0.4847 - 98ms/epoch - 24ms/step\nEpoch 24/60\n4/4 - 0s - loss: 0.2151 - val_loss: 0.4713 - 97ms/epoch - 24ms/step\nEpoch 25/60\n4/4 - 0s - loss: 0.2140 - val_loss: 0.4584 - 96ms/epoch - 24ms/step\nEpoch 26/60\n4/4 - 0s - loss: 0.2103 - val_loss: 0.4500 - 99ms/epoch - 25ms/step\nEpoch 27/60\n4/4 - 0s - loss: 0.2061 - val_loss: 0.4402 - 99ms/epoch - 25ms/step\nEpoch 28/60\n4/4 - 0s - loss: 0.2035 - val_loss: 0.4307 - 97ms/epoch - 24ms/step\nEpoch 29/60\n4/4 - 0s - loss: 0.1986 - val_loss: 0.4182 - 97ms/epoch - 24ms/step\nEpoch 30/60\n4/4 - 0s - loss: 0.1930 - val_loss: 0.4086 - 92ms/epoch - 23ms/step\nEpoch 31/60\n4/4 - 0s - loss: 0.1928 - val_loss: 0.4001 - 106ms/epoch - 26ms/step\nEpoch 32/60\n4/4 - 0s - loss: 0.1847 - val_loss: 0.3921 - 103ms/epoch - 26ms/step\nEpoch 33/60\n4/4 - 0s - loss: 0.1775 - val_loss: 0.3840 - 96ms/epoch - 24ms/step\nEpoch 34/60\n4/4 - 0s - loss: 0.1789 - val_loss: 0.3762 - 113ms/epoch - 28ms/step\nEpoch 35/60\n4/4 - 0s - loss: 0.1729 - val_loss: 0.3666 - 117ms/epoch - 29ms/step\nEpoch 36/60\n4/4 - 0s - loss: 0.1683 - val_loss: 0.3605 - 122ms/epoch - 31ms/step\nEpoch 37/60\n4/4 - 0s - loss: 0.1647 - val_loss: 0.3523 - 113ms/epoch - 28ms/step\nEpoch 38/60\n4/4 - 0s - loss: 0.1594 - val_loss: 0.3455 - 116ms/epoch - 29ms/step\nEpoch 39/60\n4/4 - 0s - loss: 0.1567 - val_loss: 0.3391 - 112ms/epoch - 28ms/step\nEpoch 40/60\n4/4 - 0s - loss: 0.1506 - val_loss: 0.3303 - 118ms/epoch - 29ms/step\nEpoch 41/60\n4/4 - 0s - loss: 0.1505 - val_loss: 0.3221 - 114ms/epoch - 29ms/step\nEpoch 42/60\n4/4 - 0s - loss: 0.1459 - val_loss: 0.3199 - 124ms/epoch - 31ms/step\nEpoch 43/60\n4/4 - 0s - loss: 0.1422 - val_loss: 0.3127 - 124ms/epoch - 31ms/step\nEpoch 44/60\n4/4 - 0s - loss: 0.1383 - val_loss: 0.3087 - 130ms/epoch - 32ms/step\nEpoch 45/60\n4/4 - 0s - loss: 0.1365 - val_loss: 0.2987 - 116ms/epoch - 29ms/step\nEpoch 46/60\n4/4 - 0s - loss: 0.1293 - val_loss: 0.2907 - 122ms/epoch - 30ms/step\nEpoch 47/60\n4/4 - 0s - loss: 0.1271 - val_loss: 0.2862 - 113ms/epoch - 28ms/step\nEpoch 48/60\n4/4 - 0s - loss: 0.1242 - val_loss: 0.2769 - 70ms/epoch - 18ms/step\nEpoch 49/60\n4/4 - 0s - loss: 0.1221 - val_loss: 0.2699 - 78ms/epoch - 19ms/step\nEpoch 50/60\n4/4 - 0s - loss: 0.1168 - val_loss: 0.2644 - 104ms/epoch - 26ms/step\nEpoch 51/60\n4/4 - 0s - loss: 0.1154 - val_loss: 0.2574 - 103ms/epoch - 26ms/step\nEpoch 52/60\n4/4 - 0s - loss: 0.1121 - val_loss: 0.2504 - 95ms/epoch - 24ms/step\nEpoch 53/60\n4/4 - 0s - loss: 0.1083 - val_loss: 0.2475 - 99ms/epoch - 25ms/step\nEpoch 54/60\n4/4 - 0s - loss: 0.1076 - val_loss: 0.2411 - 101ms/epoch - 25ms/step\nEpoch 55/60\n4/4 - 0s - loss: 0.1044 - val_loss: 0.2359 - 81ms/epoch - 20ms/step\nEpoch 56/60\n4/4 - 0s - loss: 0.1006 - val_loss: 0.2303 - 105ms/epoch - 26ms/step\nEpoch 57/60\n4/4 - 0s - loss: 0.0969 - val_loss: 0.2249 - 95ms/epoch - 24ms/step\nEpoch 58/60\n4/4 - 0s - loss: 0.0950 - val_loss: 0.2187 - 83ms/epoch - 21ms/step\nEpoch 59/60\n4/4 - 0s - loss: 0.0937 - val_loss: 0.2138 - 90ms/epoch - 23ms/step\nEpoch 60/60\n4/4 - 0s - loss: 0.0895 - val_loss: 0.2081 - 113ms/epoch - 28ms/step"
  },
  {
    "objectID": "sunspot.html#visualize-fitting-history",
    "href": "sunspot.html#visualize-fitting-history",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize fitting history",
    "text": "Visualize fitting history\n\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n2/2 [==============================] - 1s 1ms/step\n1/1 [==============================] - 0s 54ms/step\n(51, 11, 1) (51,) (51,) (12, 11, 1) (12,) (12,)\n\n\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n(51,) (51,)\n0.046757948\n0.3968842\nTrain MSE = 0.04676 RMSE = 0.21624\nTest MSE = 0.39688 RMSE = 0.62999\n\n\n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\nprint(history.history['loss'])\n\n\n                                                \n\n\n[0.5047488212585449, 0.48250818252563477, 0.44607725739479065, 0.4157511591911316, 0.3982289433479309, 0.38434669375419617, 0.36540862917900085, 0.3603326678276062, 0.3350667655467987, 0.3290526568889618, 0.3129689395427704, 0.3018222451210022, 0.2984009385108948, 0.2819467782974243, 0.27599412202835083, 0.2731618881225586, 0.2660587430000305, 0.2512662708759308, 0.24698388576507568, 0.24139201641082764, 0.23800456523895264, 0.23049971461296082, 0.22285766899585724, 0.21511372923851013, 0.21396541595458984, 0.21032771468162537, 0.20612254738807678, 0.20345744490623474, 0.1986091583967209, 0.19301706552505493, 0.19275657832622528, 0.18468406796455383, 0.17752352356910706, 0.1789451390504837, 0.17290137708187103, 0.1683419942855835, 0.1647375077009201, 0.15942975878715515, 0.15667292475700378, 0.15064620971679688, 0.1504872590303421, 0.1459072381258011, 0.1421777307987213, 0.13825400173664093, 0.13654674589633942, 0.12925390899181366, 0.1270645707845688, 0.12423168122768402, 0.12211966514587402, 0.11678657680749893, 0.1153644472360611, 0.11207455396652222, 0.10825835168361664, 0.10758113861083984, 0.10436172783374786, 0.10062526166439056, 0.09688971191644669, 0.09498926997184753, 0.0936746671795845, 0.08947247266769409]\n\n\n\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1e15259d960>"
  },
  {
    "objectID": "sunspot.html#visualize-parity-plot",
    "href": "sunspot.html#visualize-parity-plot",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize parity plot",
    "text": "Visualize parity plot\n\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "sunspot.html#visualize-predictions-2",
    "href": "sunspot.html#visualize-predictions-2",
    "title": "AN560: Sam LeBlanc",
    "section": "Visualize predictions-2",
    "text": "Visualize predictions-2\n\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=80)\n    #ORIGINAL DATA\n    print(X.shape,Y.shape)\n    plt.plot(Y_ind, Y,'o', label='target')\n    plt.plot(X_ind, X,'.', label='training points');     \n    plt.plot(Y_ind, train_predict,'r.', label='prediction');    \n    plt.plot(Y_ind, train_predict,'-');    \n    plt.legend()\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Sunspots scaled')\n    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n    plt.show()\nplot_result(trainY, testY, train_predict, test_predict)\n\n(561,) (51,)"
  }
]