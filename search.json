[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In this section, we are presented with the opportunity to leverage more intricate models, such as ARIMAX, SARIMAX, or VAR, to better understand the time series of greenhouse gas concentrations. We will persist in utilizing the variables of carbon dioxide, methane, nitrous oxide, and sulphur hexafluoride, as provided by the Global Monitoring Laboratory at NOAA. As previously indicated, these time series are not isolated phenomena. Numerous greenhouse gas emitters discharge more than one of the aforementioned gases, and all are generally associated with increased industrialization. Therefore, to simultaneously predict alterations in these interconnected variables over time, we will employ a VAR model in this segment.\n\n\nVector Autoregression (VAR) is a multivariate forecasting algorithm widely used in time series data analysis. It enables predictions of changes in several interdependent variables over time. Unlike univariate autoregressive models, which analyze one series independently, VAR explores multiple series in unison, encapsulating the linear interdependencies among them.\n\n\n\n\n\nIn a VAR model, each variable is represented by an equation that illustrates its evolution over time. This equation is derived from the variable’s own prior values (lags), as well as the lags of all other variables under consideration. The model also incorporates an error term to account for variations not explained by the time-lagged values. This integration allows the VAR model to comprehend and encapsulate the dynamic interplay between different variables.\nVAR models are particularly advantageous in fields where numerous variables interrelate over time, such as economics and natural sciences. They offer simplicity and adaptability, necessitating minimal prior knowledge about the variables’ influencing factors. The sole prerequisite is a set of variables hypothesized to affect one another over time.\nWhile the primary aspects of VAR are underlined above, it’s crucial to note that VAR models possess certain limitations. As linear models, they may struggle to capture intricate non-linear relationships. They also presume that the relationships between variables remain constant over time, which might not always hold true. Lastly, VAR models, when handling a multitude of variables and lags, can involve numerous parameters, leading to potential overfitting issues. Despite these constraints, VAR models remain an instrumental tool in multivariate time series analysis.\n\n\n\n\n\n\n\n# Load data\nco2 = read.csv('co2.csv')\nsf6 = read.csv('sf6.csv')\nch4 = read.csv('ch4.csv')\nn2o = read.csv('n2o.csv')\n\nBased on the plots and background information, we can expect CO2, CH4, N2O, and SF6 to have interrelationships since they are all greenhouse gases and their concentrations may be influenced by similar factors such as human activity and natural processes. From my domain knowledge, I know that most pollution sources contribute more than one type of greenhouse gas to the atmosphere. Thus, it is reasonable to include all four of these time series in the VAR model.\n\n# Plot time series of the individual variables\ng = ggplot(co2, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Carbon Dioxide Concentration since 1958',x='Year',y='CO2 Concentration (ppm)')\nggplotly(g)\n\n\n\n\ng = ggplot(sf6, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Sulfur Hexafluoride Concentration since 1997',x='Year',y='SF6 Concentration (ppt)')\nggplotly(g)\n\n\n\n\ng = ggplot(ch4, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Methane Concentration since 1983',x='Year',y='CH4 Concentration (ppb)')\nggplotly(g)\n\n\n\n\ng = ggplot(n2o, aes(x=decimal, y=average)) +\n  geom_line(color='darkgreen', size=1) +\n  theme_minimal() + \n  labs(title='Mean Monthly Nitrous Oxide Concentration since 2001',x='Year',y='N2O Concentration (ppb)')\nggplotly(g)\n\n\n\n\n\n\n# Convert data into time series objects\nCO2_Concentration <- ts(co2$average, start=c(1958,3), frequency=12)\nCH4_Concentration <- ts(ch4$average, start=c(1983,7), frequency=12)\nN2O_Concentration <- ts(n2o$average, start=c(2001,1), frequency=12)\nSF6_Concentration <- ts(sf6$average, start=c(1997,7), frequency=12)\n\n# Truncate time series to the same date range\nstart_date <- c(2001, 1)\nend_date <- c(2022, 8)\n\nCO2_Concentration <- window(CO2_Concentration, start=start_date, end=end_date)\nCH4_Concentration <- window(CH4_Concentration, start=start_date, end=end_date)\nN2O_Concentration <- window(N2O_Concentration, start=start_date, end=end_date)\nSF6_Concentration <- window(SF6_Concentration, start=start_date, end=end_date)\n\n\n# Plot pairwise relationships between variables\npairs(cbind(\n  CO2=CO2_Concentration, \n  CH4=CH4_Concentration, \n  N2O=N2O_Concentration,\n  SF6=SF6_Concentration\n  ))\n\n\n\n\n\n# Plot all time series together\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nplot.ts(x , main = \"\", xlab = \"\")\n\n\n\n\n\n\nUsing VAR selection methods, we have found several possible values for p. These are 6 and 10. This is surprising as I would have thought 12 would be a good choice, as that is the length of the seasonal lag. We will test these new p choices along with the standard p=1 and p=2 choices that will give us something to compare to.\n\nx = cbind(CO2_Concentration, CH4_Concentration, N2O_Concentration, SF6_Concentration)\nvar_lag_selection <- VARselect(x, lag.max=12, type=\"both\")\nprint(var_lag_selection)\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10     10      6     10 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -1.477896e+01 -1.664075e+01 -1.780180e+01 -1.809010e+01 -1.853678e+01\nHQ(n)  -1.464208e+01 -1.641262e+01 -1.748243e+01 -1.767947e+01 -1.803490e+01\nSC(n)  -1.443895e+01 -1.607407e+01 -1.700845e+01 -1.707007e+01 -1.729008e+01\nFPE(n)  3.815889e-07  5.930499e-08  1.857741e-08  1.393205e-08  8.920559e-09\n                   6             7             8             9            10\nAIC(n) -1.884106e+01 -1.887424e+01 -1.916644e+01 -1.941668e+01 -1.954365e+01\nHQ(n)  -1.824794e+01 -1.818987e+01 -1.839081e+01 -1.854980e+01 -1.858552e+01\nSC(n)  -1.736769e+01 -1.717420e+01 -1.723972e+01 -1.726329e+01 -1.716358e+01\nFPE(n)  6.588292e-09  6.383928e-09  4.776846e-09  3.729662e-09  3.296301e-09\n                  11            12\nAIC(n) -1.952806e+01 -1.953284e+01\nHQ(n)  -1.847869e+01 -1.839222e+01\nSC(n)  -1.692133e+01 -1.669943e+01\nFPE(n)  3.362150e-09  3.362979e-09\n\n\n\n# Fit different VAR models based on the selected lags\nfitvar1 = VAR(x, p=1, type=\"both\")\nfitvar2 = VAR(x, p=2, type=\"both\")\nfitvar6 = VAR(x, p=6, type=\"both\")\nfitvar10 = VAR(x, p=10, type=\"both\")\n\n\n\n\nUsing cross-validation, we see that the model with the lowest MSE is the p=10 model. Thus, we will be selection this as our best model\n\ncv_var <- function(ts_data, p, k) {\n  n <- length(ts_data[, 1])\n  start_ts <- tsp(ts_data)[1] + (k - 1) / 12\n  rmse <- numeric(n - k)\n  \n  for (i in 1:(n - k)) {\n    train_data <- window(ts_data, end = start_ts + (i - 1) / 12)\n    test_data <- window(ts_data, start = start_ts + (i - 1) / 12 + 1 / 12, end = start_ts + i / 12)\n    var_model <- VAR(train_data, p = p, type = \"const\")\n    forecast <- predict(var_model, n.ahead = 1)$fcst\n    \n    k_step_forecast <- matrix(0, nrow = 1, ncol = ncol(ts_data))\n    for (j in 1:ncol(ts_data)) {\n      k_step_forecast[, j] <- forecast[[j]][1, 1] # Extract the 1-step ahead forecasts\n    }\n    \n    rmse[i] <- sqrt(mean((test_data - k_step_forecast)^2))\n  }\n  \n  return(mean(rmse, na.rm = TRUE))\n}\n\n\n# Calculate RMSE for each model using cross-validation\nk <- 12 # Number of steps ahead for the forecasts\nRMSE_lag1 <- cv_var(x, p = 1, k = k)\nRMSE_lag2 <- cv_var(x, p = 2, k = k)\nRMSE_lag6 <- cv_var(x, p = 6, k = k)\nRMSE_lag10 <- cv_var(x, p = 10, k = k)\n\n\n# Plot the RMSEs\nbarplot(c(RMSE_lag1, RMSE_lag2, RMSE_lag6, RMSE_lag10), names.arg = c(\"Lag 1\", \"Lag 2\",\"Lag 6\", \"Lag 10\"), ylab = \"RMSE\")\n\n\n\n\n\n\n\nThe selected model, VAR(10), was chosen based on its lowest RMSE value obtained through cross-validation, which suggests that this model provides the best fit for the data among the tested models. This model can be utilized to predict future concentrations of CO2, CH4, N2O, and SF6, all of which are greenhouse gases. By analyzing these forecasts, we can gain insights into the potential future trends of these greenhouse gas concentrations. It is important to note, however, that the accuracy of these forecasts depends on the underlying assumptions of the VAR model and the quality of the historical data. As such, the results should be interpreted with caution, considering possible uncertainties and changes in factors influencing greenhouse gas concentrations.\n\nbest_var_model <- fitvar10 # Replace with the best model\nn_forecast <- 120 # Number of months to forecast\nvar_forecast <- forecast(best_var_model, h=n_forecast)\nplot(var_forecast)"
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For this section, we will be using an ARIMA model to forecast future atmospheric concentrations of sulfur hexaflouride.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\n\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\n\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot.\n\n\n\nBased on the PACF and ACF plots, I will be modeling the twice-differenced \\(SF_6\\) concentration because I want to ensure that our series is stationary. Based on the ACF and PACF plots of the twice differenced \\(SF_6\\) concentration shown above, I will select 1 and 3 as possible choices for \\(p\\) and 1,2,3, and 4 as possible choices for \\(q\\). Since this data is twice differenced, I will be using a \\(q\\) value of 2.\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n-1926.281\n-1922.574\n-1926.268\n\n\n0\n2\n1\n-1972.627\n-1965.213\n-1972.587\n\n\n0\n2\n2\n-2017.168\n-2006.047\n-2017.087\n\n\n0\n2\n3\n-2024.154\n-2009.325\n-2024.019\n\n\n2\n2\n0\n-1943.242\n-1932.120\n-1943.161\n\n\n2\n2\n1\n-2021.723\n-2006.894\n-2021.588\n\n\n2\n2\n2\n-2042.100\n-2023.564\n-2041.896\n\n\n2\n2\n3\n-2026.081\n-2003.839\n-2025.796\n\n\n\n\n\nBased on the table, the model with the lowest AIC and BIC scores is the 2,2,2 model. None of the other models are that close to this one in terms of AIC and BIC scores. For the sake of comparison, I will select the worst model of the bunch (0,2,0) to show that even this is very similar to our best model. All of the option here would be adequate for forecasting this series.\n\n\n\nFor a (2,2,2) ARIMA model, the equation is:\n\\[y(t) = c + ϕ(1)*y(t-1) + ϕ(2)*y(t-2) - θ(1)*ε(t-1) - θ(2)*ε(t-2) + ε(t)\\]\nwhere:\n\n\\(y(t)\\) is the value of the time series at time t\n\\(c\\) is a constant term (i.e., the mean of the series)\n\\(ϕ(1)\\) and \\(ϕ(2)\\) are the autoregressive coefficients for lags 1 and 2, respectively\n\\(θ(1)\\) and \\(θ(2)\\) are the moving average coefficients for lags 1 and 2, respectively\n\\(ε(t)\\) is white noise (i.e., a random error term) with mean zero and constant variance\n\nNow, lets consider the model diagnostics using standardized residuals.\n\n\n\n\n\nFor the most part, the residuals plots looks as we would expect. The residuals have a constant mean and variance, and the lagged p-values for the Ljung-Box statistic are under the 0.05 threshold for all except two of the lags. Something that does still appear is a spike in ACF residuals at lags 12 and 24. This would suggest that the series may not have been adequately stationary prior ro modeling. However, we mentioned this earlier, and it wsa determined that twice-differenced was the proper metric to use, as three-times differences would over difference the data and lose valuable insights.\nNext, we can plot the raw data, our chosen model (2,2,2), and the second model (0,2,0) on the same plot to see how they compare on the training data. As expected, both models are nearly identical to the SF6 data. The lines are directly on top of each other and it is impossible to make out any difference between the models and the data.\n\n\n\n\n\n\n\n\nNext, we can compare our ARIMA(2,2,2) model to the auto-arima model. Thankfully, the auto-arima function provides the same model as the one we came to through traditional means. This is validating because there were many choices for p,d, and q, so it is nice to know that the best model was selected in both methods.\n\n\nSeries: SF6_Concentration \nARIMA(2,2,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      1.2422  -0.5007  -1.8154  0.8326\ns.e.  0.0653   0.0550   0.0495  0.0499\n\nsigma^2 = 6.44e-05:  log likelihood = 1026.05\nAIC=-2042.1   AICc=-2041.9   BIC=-2023.56\n\n\n\n\n\nFinally, we can use the ARIMA model to forecast the SF6 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(2,2,2) model.\n\n\n\n\n\n\n\n\nIn order to determine the effectiveness of our model, we need to compare to other benchmark methods. Based on the graphs and accuracy tests below, it is clear that the ARIMA(2,2,2) model far outperforms any of the benchmark methods. Thus, we can be confident when using this model going forward.\n\n\n\n\n\n\npred1=forecast(fit1, h=120);\naccuracy(pred1)\n\n                       ME       RMSE         MAE       MPE       MAPE\nTraining set 0.0007896781 0.00794485 0.006303769 0.0115696 0.09585017\n                   MASE        ACF1\nTraining set 0.02238707 -0.04714439\n\n\n\nf1 <- meanf(SF6_Concentration, h=120)\naccuracy(f1)\n\n                       ME     RMSE     MAE       MPE    MAPE     MASE     ACF1\nTraining set 1.282536e-16 2.071243 1.79491 -9.131477 27.7452 6.374407 0.990158\n\n\n\nf2 <- naive(SF6_Concentration, h=120)\naccuracy(f2)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292\n\n\n\nf3 <- rwf(SF6_Concentration, h=120)\naccuracy(f3)\n\n                     ME       RMSE        MAE       MPE     MAPE       MASE\nTraining set 0.02344371 0.02547561 0.02350993 0.3366936 0.338216 0.08349269\n                  ACF1\nTraining set 0.5054292"
  },
  {
    "objectID": "arma.html#sarima-co2-concentration",
    "href": "arma.html#sarima-co2-concentration",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA: CO2 Concentration",
    "text": "SARIMA: CO2 Concentration\nIn this section, we will use a Seasonal Autoregressive Integrated Moving Average (SARIMA) model to forecast future atmospheric concentrations of carbon dioxide. As the graph below illustrates, the CO2 concentration, though representing a global mean, exhibits a strong seasonal pattern. Therefore, we will employ a SARIMA model for our analysis.\n\n\n\n\n\n\n\n\n\n\nCO2 Concentration (No differencing)\n\n\n\n\n\nThe above plots display the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for the CO2 concentration time series. As anticipated, the ACF plot reveals a strong correlation between values, signifying that this series is non-stationary. This aligns with our previous assumption that the series is non-stationary due to its pronounced seasonality. To achieve stationarity, we will apply differencing.\n\nautoplot(decompose(CO2_Concentration))\n\n\n\n\n\n\nMonthly Lags\nBefore performing differencing, we examine the monthly lag plots to determine the length of the seasonality. Based on domain knowledge, we suspect a 12-month lag, as the seasonal pattern recurs annually. The lag plots below confirm this hypothesis. The Month 12 lag plots exhibit an almost perfect y=x line, which is superior to all other lags up to 12 months.\n\ngglagplot(CO2_Concentration, do.lines=FALSE, lags=12)+xlab(\"Xi\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Monthly CO2_Concentration (1958-2023)\")\n\n\n\n\n\n\nTwice Differences (Once Seasonal and Once Standard)\nHaving identified the seasonal lag, we can now use differencing to make the series stationary. We apply one seasonal differencing and one standard differencing to achieve stationarity. The resulting ACF and PACF plots exhibit no seasonal pattern or serial autocorrelation, indicating that our data is now differenced and ready for parameter selection and model creation.\n\nCO2_Concentration %>% diff(lag=12) %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\nParameter Selection\nUsing the ACF and PACF plots of the differenced data, we can identify potential values for the parameters p, P, q, and Q. Note that d and D have already been determined based on the differencing performed in the previous step. The ACF and PACF plots suggest the following possible values for each parameter:\n\nq = 0,1\nQ = 0,1\np = 0,1,2,3\nP = 0,1,2,3\nd = 1\nD = 1\n\nAfter running the model, we get the following table of AIC and BIC values based on the model parameters.\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*25),nrow=25)\n  \n  for (p in p1:p2){\n    for(q in q1:q2){\n      for(P in P1:P2){\n        for(Q in Q1:Q2){\n          if(p+d+q+P+D+Q<=9){\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=2,P1=1,P2=4,Q1=1,Q2=2,data=CO2_Concentration)\nknitr::kable(output)\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n901.4395\n906.0794\n901.4448\n\n\n0\n1\n0\n0\n1\n1\n502.4804\n511.7602\n502.4962\n\n\n0\n1\n0\n1\n1\n0\n708.5527\n717.8325\n708.5685\n\n\n0\n1\n0\n1\n1\n1\n503.8839\n517.8035\n503.9154\n\n\n0\n1\n0\n2\n1\n0\n638.8982\n652.8178\n638.9297\n\n\n0\n1\n0\n2\n1\n1\n505.8830\n524.4425\n505.9356\n\n\n0\n1\n0\n3\n1\n0\n608.7436\n627.3032\n608.7963\n\n\n0\n1\n1\n0\n1\n0\n808.8603\n818.1401\n808.8761\n\n\n0\n1\n1\n0\n1\n1\n413.9145\n427.8341\n413.9460\n\n\n0\n1\n1\n1\n1\n0\n610.7566\n624.6763\n610.7882\n\n\n0\n1\n1\n1\n1\n1\n415.9110\n434.4705\n415.9636\n\n\n0\n1\n1\n2\n1\n0\n524.8992\n543.4587\n524.9518\n\n\n1\n1\n0\n0\n1\n0\n828.5434\n837.8231\n828.5591\n\n\n1\n1\n0\n0\n1\n1\n433.4853\n447.4049\n433.5168\n\n\n1\n1\n0\n1\n1\n0\n630.4665\n644.3862\n630.4981\n\n\n1\n1\n0\n1\n1\n1\n435.3995\n453.9590\n435.4522\n\n\n1\n1\n0\n2\n1\n0\n551.4616\n570.0211\n551.5143\n\n\n1\n1\n1\n0\n1\n0\n806.1513\n820.0710\n806.1829\n\n\n1\n1\n1\n0\n1\n1\n410.7134\n429.2729\n410.7660\n\n\n1\n1\n1\n1\n1\n0\n609.6100\n628.1695\n609.6627\n\n\n2\n1\n0\n0\n1\n0\n819.1535\n833.0731\n819.1850\n\n\n2\n1\n0\n0\n1\n1\n423.5954\n442.1550\n423.6481\n\n\n2\n1\n0\n1\n1\n0\n621.1240\n639.6835\n621.1767\n\n\n2\n1\n1\n0\n1\n0\n807.3255\n825.8850\n807.3782\n\n\n3\n1\n0\n0\n1\n0\n810.3586\n828.9181\n810.4113\n\n\n\n\n\nAccording to the AIC and BIC scores, the best model is the (1,1,1)(0,1,1) model, as it has the lowest AIC and BIC scores. Several other models also have similar AIC and BIC scores, but upon comparing the top models, the (1,1,1)(0,1,1) model is indeed the best one. However, all the options here would provide adequate forecasts for this series, as their performance is very similar.\n\n\nModel Diagnostics\nNow, let’s examine the model diagnostics using standardized residuals.\n\nmodel_output <- capture.output(sarima(CO2_Concentration,1,1,1,0,1,1,12))\n\n\n\n\n\nfit1 <- Arima(CO2_Concentration, order=c(1,1,1), seasonal=c(0,1,1))\nsummary(fit1)\n\nSeries: CO2_Concentration \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2164  -0.5793  -0.8629\ns.e.  0.0880   0.0738   0.0184\n\nsigma^2 = 0.09906:  log likelihood = -201.36\nAIC=410.71   AICc=410.77   BIC=429.27\n\nTraining set error measures:\n                     ME      RMSE       MAE         MPE       MAPE      MASE\nTraining set 0.02449999 0.3114867 0.2426831 0.006651617 0.06804522 0.1498467\n                     ACF1\nTraining set -0.003090451\n\n\n\n\nCross-Validation\nTo further validate our SARIMA(1,1,1)(0,1,1) model, we will perform a one-step-ahead cross-validation. In this approach, we iteratively train the model on a portion of the data and then use the model to forecast the next point. We compare this forecast to the actual data and calculate the error. This process is repeated for each data point in the time series, and the overall accuracy of the model is assessed using the mean squared error (MSE) of the one-step-ahead forecasts.\n\n# Cross-validation function\none_step_ahead_cv <- function(data, order, seasonal_order) {\n  n <- length(data)\n  errors <- numeric(n)\n  \n  for (t in (length(seasonal_order) + 2):n) {\n    train_data <- window(data, end = t - 1)\n    test_data <- data[t]\n    if (length(train_data) > 0) {\n      model <- Arima(train_data, order = order, seasonal = seasonal_order)\n      forecasted_value <- forecast(model, h = 1)$mean\n      errors[t] <- (forecasted_value - test_data)^2\n    }\n  }\n  \n  mean(errors, na.rm = TRUE)\n}\n\n\n# Perform cross-validation\n# mse <- one_step_ahead_cv(data = CO2_Concentration, order = c(1, 1, 1), seasonal_order = c(0, 1, 1))\n# mse\n\n\n\nBenchmark Methods and Forecast\nTo evaluate the effectiveness of our model, we will compare it to other benchmark methods. The graphs and accuracy tests below demonstrate that the SARIMA(1,1,1)(0,1,1) model significantly outperforms any of the benchmark methods. This gives us confidence in using this model for our forecasts.\n\nautoplot(CO2_Concentration) +\n  autolayer(meanf(CO2_Concentration, h=120),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(CO2_Concentration, h=120),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(CO2_Concentration, h=120),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(CO2_Concentration, h=120, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit1,120), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\nFinally, we can use the SARIMA model to forecast the CO2 concentration over the next 10 years (120 months). The plot below shows the forecast of the ARIMA(1,1,1)(0,1,1) model.\n\nfit1 %>% forecast(h=120) %>% autoplot()\n\n\n\n\nIn conclusion, we have successfully built and validated a SARIMA(1,1,1)(0,1,1) model for forecasting future atmospheric concentrations of carbon dioxide. This model effectively accounts for the seasonality present in the CO2 concentration data and outperforms the benchmark methods. Utilizing this model, we can now generate forecasts of CO2 concentrations over the next 10 years and contribute to the understanding of potential future trends in atmospheric CO2 levels."
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "Header\n  \n    Primary card title\n    Some quick example text to build on the card title and make up the bulk of the card's content."
  },
  {
    "objectID": "ds.html#data-exploration",
    "href": "ds.html#data-exploration",
    "title": "Data Sources",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nEnvironmental Protection Agency (EPA): Greenhouse Gas Reporting Program (GHGRP)\nThe Environmental Protection Agency’s Greenhouse Gas Reporting Program is a critical tool in the fight against climate change. The program provides a comprehensive overview of emissions data from across the country, helping to inform policy decisions and identify areas that need additional attention. The data collected helps the EPA track trends in emissions levels and can be used to inform strategies for reducing greenhouse gas emissions. With the help of this program, the EPA is able to create an effective response to the ever-changing environmental conditions and ensure a sustainable future for the planet.\nThe data can be accessed at the Data Sets section of the GHGRP website.They are stored in a zip file containing data over the last decade. The most interesting data set from the EPA is the record of large polluters. I think it could be very interesting to use this to gauge how station proximity to large emitters effects the amplitude of seasonal cycles."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "Data Visualization with Stock Data"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For this section of the EDA, we will be focusing specifically on the Sulfur Hexafluoride (\\(SF_6\\)). This process was repeated for three other atmospheric concentrations of greenhouse gasses as well, but those have been omitted to allow for an in-depth look at Sulfur Hexafluoride.\nSulfur Hexafluoride is an odorless, colorless, and non-toxic gas. It is mostly used as an electrical insulator, as it has a very high dielectric strength. \\(SF_6\\) also has other uses, such as in the production of integrated circuits and flat panel displays, as a tracer gas in leak detection, and as a propellant in aerosol cans. \\(SF_6\\) is a greenhouse gas, and its global warming potential is estimated to be 23,900 times higher than that of carbon dioxide. As such, SF6 is regulated by the Montreal Protocol and the Kyoto Protocol.\nSimilar to the other greenhouse gases, Sulfur Hexafluoride is measured continually at many monitoring stations across the globe. Then, NOAA produces a monthly estimate that accounts for additional factors like weather and available monitoring stations. Sulfur Hexafluoride is measured in parts per trillion.\n\n\n\n\n\n\n\n\n\nThe above plot depicts the alarming rise in the mean monthly concentration of \\(SF_6\\) since 1997, with an increase of nearly threefold in just 25 years. This is a cause of great concern, as \\(SF_6\\) is one of the most potent greenhouse gases released into the atmosphere today. Even though it exists in parts per trillion, this accumulation of \\(SF_6\\) will continue to contribute to the disruption of the planet’s climate in conjunction with other greenhouse gases. Undoubtedly, this is a worrisome trend that must be addressed before it is too late.\nBased on the above plot, we can see a strongly increasing trend of \\(SF_6\\) that has continued uninterrupted since records began. The trend is very consistent and appears to be almost linear, with just a slight exponential increase detectable. There appear to be only a few months in the 25 years where the \\(SF_6\\) concentration was lower than it was in a previous month. Unlike many other greenhouse gases, the concentration of \\(SF_6\\) does not exhibit strong season variation. Gases like \\(CO_2\\) or \\(CH_4\\) which were discussed previously, see significant change depending on the season and location of the monitoring station, however \\(SF_6\\) does not. Other than that, there appear to be no periodic fluctuations and the level of noise appears to be minimal.\nBased only on background knowledge and the above plot, the data appears to be additive rather than multiplicative. One way to know the series is additive is linearity and this plot appears to be nearly linear. Another method is to look at variance, based on this plot, the variance appears to be consistent, pointing to an additive series. These assumptions will be more easy to identify when looking at the decomposition of the series. Examining the individual components of the series can help to confirm whether the data is additive or multiplicative."
  },
  {
    "objectID": "eda.html#decomposition",
    "href": "eda.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\nAfter decomposing the series, it becomes easier to identify the trend, seasonal, and remainder components. As expected, there is a strong increasing trend that was readily apparent from the initial plot. Additionally, the decomposition process revealed a much subtler seasonal component to this data, which was not easily detected from the initial plot. The seasonal component is highly regular, and appears very similar to the seasonal components of other greenhouse gas concentrations. However, the scale of this seasonal component is miniscule (just 0.01 part per trillion) compared to the increase of the trend, which accounts for an average of 3.5 parts per trillion per year. Similarly, the decomposition reveals that the randomness in the data is very small when compared to the overall trend. This is why the plot appears to be almost linear in nature. It is important to note however, that while randomness may be difficult to detect in the initial plot, it is still an important factor that should be taken into consideration when analyzing the data. Using the decomposition, the series is clearly additive. This is based on the linearly increasing trend as well as the seasonality which does not increase in variance as the series increases.\n\n\n\n\n\n\nThis plot clearly illustrates the strong correlation between the monthly lags for the first 6 months. As expected, the values are highly correlated with their preceding values. Since the series lacks any noticeable seasonality, these lags are even more evident. The lag plots demonstrate that the series has a high degree of autocorrelation, which can be diminished with the use of detrending. By utilizing this technique, we can effectively reduce the influence of autocorrelation and gain a better insight into the data."
  },
  {
    "objectID": "eda.html#stationarity",
    "href": "eda.html#stationarity",
    "title": "Exploratory Data Analysis",
    "section": "Stationarity",
    "text": "Stationarity\n\nSF6 Concentration (No differencing)\n\n\n\n\n\nThe above plots show the ACF and PACF for the Sulfur Hexafluoride concentration time series. As expected, the ACF plot shows a very high degree of correlation between values, indicating that this series is not stationary. This follows the previous hypothesis that the series was not stationary due to the strong trend and weak seasonality. Since the series is not stationary, there are several options available to make it so. One of these options is differencing, where the values are recaluclated by subtracting each from the previous, providing the difference between each pair of values. Other methods include smoothing the data with a moving average or using a Box-Cox transformation to bring the data into a more stationary state. Ultimately, the goal of these transformations is to make the data stationary so that it can be better modeled and better predictions can be made.\n\n\nSF6 Concentration (1st order difference)\n\n\n\n\n\nThese plots demonstrate the first-order differencing of the time series. After eliminating most of the trend component, the seasonality is much more visible in the ACF plot. It is also worth noting that the magnitude of the bars in the differenced graph is much smaller than that of the initial ACF plot. A closer analysis of the ACF plot reveals that the seasonality remains relatively strong even after one differencing.The seasonal spikes appear every 12, 24 and 36 months, which is the expected result for a monthly series. Even though there is still an obvious pattern in the ACF plot, it is necessary to differentiate the series once again to make it stationary.\n\n\nSF6 Concentration (2nd order difference)\n\n\n\n\n\nAfter taking the difference for the second time, the plots are showing much stronger indications that the series is now stationary. Through it is not perfect, the ACF plot has much less correlation across the range of lag parameters. Although there are still some spikes at 12 and 24 months, taking another difference would not be unreasonable. However, it is important to note that if we over difference the data, it will be much more difficult to detect the signal that we are aiming for. This is evident when we look at the 3rd order difference, where the plot is almost indistinguishable from white noise. As a result, it is critical to be mindful of the order of difference when analyzing time series data.\nThe Augmented Dickey-Fuller Test (ADF) is a powerful tool for testing for stationarity in a time series. To ensure a comprehensive analysis, the ADF test should be run on the undifferenced, 1st order, and 2nd order difference of the time series. By doing so, we can compare the results and gain a clearer understanding of stationarity. The results of the ADF test are displayed below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration\nDickey-Fuller = -1.0021, Lag order = 6, p-value = 0.9371\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff()\nDickey-Fuller = -9.9963, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SF6_Concentration %>% diff() %>% diff()\nDickey-Fuller = -10.064, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe results of the ADF test indicate that both the 1st and 2nd order differences are stationary, while the undifferenced series is not. This matches my hypothesis, with the exception of the first order difference. Inspection of this ACF plot reveals a clear pattern of correlation at 12 and 24 months. I find it quite puzzling that the ADF test believes that this series is stationary despite the spikes in the ACF plot."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\n\n\nWe can also consider moving average smoothing to identify underlying patterns in the data. For this example, I will be using the \\(CO_2\\) concentration as it provides a more rich comparison than \\(SF_6\\).\nBelow are four graphs of the \\(CO_2\\) concentration at various levels of smoothing. The first plot is the raw data with no smoothing applied. The seasonal variation is readily apparent in this plot, but that causes the trend to be obscured. One way we can view the trend more easily (other than decomposing) is to use moving average smoothing.\n\n\n\n\n\nThe second graph shows a smoothed moving average of the \\(CO_2\\) concentration with a window that is too small (3 months). Since the concentration follows a 12 month seasonal cycle, the 3 month smoothing only serves to decrease the relative size of the season cycle. While this is a good start to get us to a clear trend line, it is not enough. We will need to apply the smooth moving average with a window that matches the seasonal cycle.\n\n\n\n\n\nThe last two graphs are constructed using smooth moving average windows that are multiples of the seasonal cycle length (12 months). Thus, we can no longer see any seasonal variation because each observation now includes data from all parts of the year equally. We could have achieved the same result by decomposing the series, but this is another option if we know the length of a seasonal cycle. Overall, the 12-month and 24 month windows look very similar, especially since this data has a very steady trend."
  },
  {
    "objectID": "ftsm.html#background",
    "href": "ftsm.html#background",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Background",
    "text": "Background\nAutoregressive Conditional Heteroskedasticity (ARCH) and Generalized Autoregressive Conditional Heteroskedasticity (GARCH) are statistical models extensively used in econometrics, particularly in financial time series analysis. These models are designed to capture the dynamic nature of volatility, a key concept in financial markets that signifies the degree of variation or dispersion of returns for a given security or market index.\nVolatility is a significant aspect of financial time series data, reflecting the degree of risk or uncertainty in a market. High volatility indicates a higher degree of risk, as the asset’s price can change dramatically in a short time. Thus, accurately modeling and forecasting volatility with ARCH and GARCH models can lead to more informed investment strategies and improved risk assessment.\n\n\n\n\n\nARCH models, introduced by Robert Engle in 1982, describe the variance of the current error term or innovation as a function of the actual sizes of the previous time periods’ error terms. In other words, ARCH models capture the phenomenon of “volatility clustering,” where periods of high volatility tend to be followed by high volatility, and low by low.\nGARCH models, a generalized version of ARCH models, incorporate lagged values of the error variance itself into the model. GARCH models are particularly beneficial when dealing with time series data that exhibits volatility clustering and leverage effects. They can effectively model and forecast time-varying volatility, which is a crucial aspect of financial decision making, risk management, and options pricing."
  },
  {
    "objectID": "ftsm.html#carvana-model-fitting-with-arima-and-garch",
    "href": "ftsm.html#carvana-model-fitting-with-arima-and-garch",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Carvana Model Fitting with ARIMA and GARCH",
    "text": "Carvana Model Fitting with ARIMA and GARCH\nSince my project has exactly nothing to do with financial time series data (which was not a requirement when choosing projects) I will instead focus on a completely random topic because that’s what the professor told me to do. So fun and worthwhile!\nFirst, let’s get the stock data for our favorite company Carvana! After getting the data, we plot to see what we are working with.\n\nCVNA = getSymbols(\"CVNA\",auto.assign = FALSE, from = \"2014-10-01\",src=\"yahoo\")\nchartSeries(CVNA, theme = chartTheme(\"white\"), \n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\")\n\n\n\nlog(CVNA$`CVNA.Adjusted`) %>% diff() %>% chartSeries()\n\n\n\n\nBased on the data, it would be wise to take the logarithm since the values are highly skewed from when the stock was doing well and when it was doing poorly."
  },
  {
    "objectID": "ftsm.html#stationarity",
    "href": "ftsm.html#stationarity",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stationarity",
    "text": "Stationarity\nAs with most ARCH/GARCH models, we will be modelling the returns rather than the raw data. This is partly why we took the logarithm, so that the returns are on a more reasonable scale from different time periods of the stock.\n\ncvna = ts(CVNA$`CVNA.Adjusted`, start=decimal_date(as.Date(\"2014-10-01\")), frequency = 365.25)\n\n\nautoplot(cvna) +ggtitle(\"Carvana Price\")\n\n\n\n\n\ncvna %>% ggtsdisplay()\n\n\n\n\nPlotting the raw data, we can see that this series is clearly not stationary. There is extremely high correlation between values as well as string seasonality present in the data. Thus, to address the non-stationarity, we will need to do differencing. In addition to differencing, we will also need to calculate the logarithm of the data to account for large variations in price that occurred over the time frame of interest.\n\nreturns = log(cvna) %>% diff()\nreturns %>% ggtsdisplay()\n\n\n\n\nBased on the ACF plot, we can see that after taking the log and differenc of the data, this series is now weakly stationary. There is no need for additional differencing as the ACF and PACF plots are looking good already. Any extra differencing would result in over-differencing and would make modeling more difficult."
  },
  {
    "objectID": "ftsm.html#garchpq-model-fitting-with-arima",
    "href": "ftsm.html#garchpq-model-fitting-with-arima",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH(p,q) model fitting with ARIMA",
    "text": "GARCH(p,q) model fitting with ARIMA\n\nArchTest\nFirst, we check for ARCH effects with the ArchTest() function. We will use a standard significance level of \\(\\alpha=0.05\\) for our null hypothesis test. Because the p-value is much smaller than 0.05, so we reject the null hypothesis and conclude the presence of ARCH(1) effects.\n\nArchTest(returns, lags=1, demean=TRUE)\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 50.634, df = 1, p-value = 1.113e-12\n\n\n\n\nARIMA model\nLet’s fit the ARIMA model first. We follow the same procedure as previously. For more on ARIMA models, check out the other tabs of the website.\n\nARIMA.f=function(p1,p2,q1,q2,data){\n  temp=c()\n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,6*24),nrow=24)\n  \n  for (p in p1:p2){\n    for(q in q1:q2){\n      if(p+1+q<=8){\n        model= Arima(data,order=c(p,1,q))\n        ls[i,]= c(p,1,q,model$aic,model$bic,model$aicc)\n        i=i+1\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n}\n\n\noutput = ARIMA.f(0,4,0,4,data=log(cvna))\noutput\n\n   p d q       AIC       BIC      AICc\n1  0 1 0 -4033.233 -4027.913 -4033.231\n2  0 1 1 -4032.887 -4022.246 -4032.879\n3  0 1 2 -4031.223 -4015.261 -4031.207\n4  0 1 3 -4030.065 -4008.783 -4030.038\n5  0 1 4 -4031.651 -4005.048 -4031.611\n6  1 1 0 -4032.929 -4022.288 -4032.921\n7  1 1 1 -4030.975 -4015.013 -4030.959\n8  1 1 2 -4029.306 -4008.024 -4029.280\n9  1 1 3 -4029.361 -4002.759 -4029.321\n10 1 1 4 -4029.671 -3997.748 -4029.615\n11 2 1 0 -4031.125 -4015.164 -4031.109\n12 2 1 1 -4028.934 -4007.652 -4028.907\n13 2 1 2 -4037.951 -4011.348 -4037.911\n14 2 1 3 -4033.206 -4001.283 -4033.150\n15 2 1 4 -4035.170 -3997.926 -4035.095\n16 3 1 0 -4030.307 -4009.025 -4030.280\n17 3 1 1 -4029.708 -4003.105 -4029.668\n18 3 1 2 -4028.275 -3996.352 -4028.219\n19 3 1 3 -4026.437 -3989.193 -4026.362\n20 3 1 4 -4025.909 -3983.345 -4025.813\n21 4 1 0 -4031.810 -4005.208 -4031.771\n22 4 1 1 -4029.822 -3997.899 -4029.766\n23 4 1 2 -4027.977 -3990.733 -4027.902\n24 4 1 3 -4026.071 -3983.507 -4025.975\n\n\nARIMA(2,1,2)\n\nauto.arima(log(cvna))\n\nSeries: log(cvna) \nARIMA(5,2,0) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5\n      -0.7922  -0.6080  -0.4742  -0.3632  -0.1892\ns.e.   0.0253   0.0311   0.0327   0.0312   0.0254\n\nsigma^2 = 0.004749:  log likelihood = 1898.47\nAIC=-3784.94   AICc=-3784.88   BIC=-3753.02\n\n\nUsing the auto.arima function, we can see the the best model is the ARIMA(5,2,0). But the ACF and PACF does not suggest these are good values. Since the auto arima function is sometimes un-trsutworthy, I am still going to go with the (2,1,2) ARIMA as determined by the manual arima model selection.\n\nmodel_output <- capture.output(Arima(cvna, order=c(2,1,2),include.drift = TRUE))\n\nUsing the standardized residuals plots, we can see that the ARIMA model is insufficient to accurately model the financial time series data. Thus, we will need to use the GARCH model on top of the residuals from the ARIMA model. This is a common tactic in financial time series which has a much different pattern than other time series like the greenhouse gases for the remainder of the project. Thus, we will continue modeling with the GARCH model. I choice the GARCH values based on the ACF graph, of the ARIMA mode. As we can see from this chart, we should try all p,q values between 0 and 4."
  },
  {
    "objectID": "ftsm.html#garch-model",
    "href": "ftsm.html#garch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "GARCH model",
    "text": "GARCH model\nNext, we will fit the ARIMA model and then fit a GARCH model to the residuals of the ARIMA model.\n\ndata=log(cvna)\n\narima.fit=Arima(data,order=c(2,1,2),include.drift = TRUE)\narima.res=arima.fit$residuals\n\nacf(arima.res)\n\n\n\nacf(arima.res^2) # clear correlation 1,2,4\n\n\n\npacf(arima.res^2) # clear correlation 1,4\n\n\n\n\n\nmodel = list()\ncc = 1\nfor (p in 1:4) {\n  for (q in 1:4) {\n    model[[cc]] = garch(arima.res,order=c(q,p),trace=F)\n    cc = cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nGARCH_AIC = sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n[1] 2\n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2  \n0.0000181  0.0900395  0.3752570  0.5354546  \n\n\nAfter trying all p,q values from 0,4 in combination, the GARCH(1,2) model is the best and has the lowest combination of AIC and BIC models. I tested all of the models, but only included the output from the best one. I attempted to use cross validation but was unsuccessful in making comparisons between the different models.\n\nsummary(garchFit(~garch(1,2), arima.res,trace = F))\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x0000020ff8179eb8>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n6.7174e-04  1.7858e-05  8.9833e-02  3.7835e-01  5.3266e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.717e-04   1.104e-03    0.608 0.543012    \nomega  1.786e-05   8.781e-06    2.034 0.041985 *  \nalpha1 8.983e-02   1.531e-02    5.868 4.42e-09 ***\nbeta1  3.783e-01   1.455e-01    2.600 0.009318 ** \nbeta2  5.327e-01   1.373e-01    3.879 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2313.141    normalized:  1.529855 \n\nDescription:\n Tue May  2 11:16:57 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1421.464  0        \n Shapiro-Wilk Test  R    W      0.9546574 0        \n Ljung-Box Test     R    Q(10)  12.43853  0.2567786\n Ljung-Box Test     R    Q(15)  19.67952  0.1845745\n Ljung-Box Test     R    Q(20)  20.74534  0.4122557\n Ljung-Box Test     R^2  Q(10)  5.90633   0.8230706\n Ljung-Box Test     R^2  Q(15)  9.833252  0.8300956\n Ljung-Box Test     R^2  Q(20)  11.01653  0.9457927\n LM Arch Test       R    TR^2   8.229875  0.7669183\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.053096 -3.035500 -3.053118 -3.046544 \n\n\nSince all the models has similar AIC ,BIC values, I would go with GARCH(1,1) which all the coefficients are significant.\n\nFinal Model\nThe final model has a decent but not great fir for the Carvana stock return data. All of the errors except for mu are significant but the Ljung-Box statistics are well over the standard threshold. Since there is a mix of indicators, this tells us that the model is decent but not quantifiable better than the simpler ARIMA model. Thus, in this case I would rely on the ARIMA since it is a simpler specification.\n\narima.fit=Arima(data,order=c(2,1,2),include.drift = TRUE)\nsummary(arima.fit)\n\nSeries: data \nARIMA(2,1,2) with drift \n\nCoefficients:\n         ar1      ar2      ma1     ma2    drift\n      0.8798  -0.9638  -0.8766  0.9855  -0.0004\ns.e.  0.0123   0.0178   0.0078  0.0134   0.0017\n\nsigma^2 = 0.00403:  log likelihood = 2023.99\nAIC=-4035.98   AICc=-4035.92   BIC=-4004.05\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE     MAPE       MASE\nTraining set 6.717395e-05 0.06335763 0.04145073 -0.03016094 1.237607 0.02846507\n                   ACF1\nTraining set 0.02601668\n\n\n\nfinal.fit = garchFit(~garch(1,2), arima.res,trace = F)\nsummary(final.fit)\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x0000020ffa2b1c00>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n6.7174e-04  1.7858e-05  8.9833e-02  3.7835e-01  5.3266e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.717e-04   1.104e-03    0.608 0.543012    \nomega  1.786e-05   8.781e-06    2.034 0.041985 *  \nalpha1 8.983e-02   1.531e-02    5.868 4.42e-09 ***\nbeta1  3.783e-01   1.455e-01    2.600 0.009318 ** \nbeta2  5.327e-01   1.373e-01    3.879 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2313.141    normalized:  1.529855 \n\nDescription:\n Tue May  2 11:16:58 2023 by user: sleblanc \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1421.464  0        \n Shapiro-Wilk Test  R    W      0.9546574 0        \n Ljung-Box Test     R    Q(10)  12.43853  0.2567786\n Ljung-Box Test     R    Q(15)  19.67952  0.1845745\n Ljung-Box Test     R    Q(20)  20.74534  0.4122557\n Ljung-Box Test     R^2  Q(10)  5.90633   0.8230706\n Ljung-Box Test     R^2  Q(15)  9.833252  0.8300956\n Ljung-Box Test     R^2  Q(20)  11.01653  0.9457927\n LM Arch Test       R    TR^2   8.229875  0.7669183\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.053096 -3.035500 -3.053118 -3.046544 \n\n\n\ncapture.output(final.fit)\n\n [1] \"\"                                                               \n [2] \"Title:\"                                                         \n [3] \" GARCH Modelling \"                                              \n [4] \"\"                                                               \n [5] \"Call:\"                                                          \n [6] \" garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \"\n [7] \"\"                                                               \n [8] \"Mean and Variance Equation:\"                                    \n [9] \" data ~ garch(1, 2)\"                                            \n[10] \"<environment: 0x0000020ffa2b1c00>\"                              \n[11] \" [data = arima.res]\"                                            \n[12] \"\"                                                               \n[13] \"Conditional Distribution:\"                                      \n[14] \" norm \"                                                         \n[15] \"\"                                                               \n[16] \"Coefficient(s):\"                                                \n[17] \"        mu       omega      alpha1       beta1       beta2  \"   \n[18] \"6.7174e-04  1.7858e-05  8.9833e-02  3.7835e-01  5.3266e-01  \"   \n[19] \"\"                                                               \n[20] \"Std. Errors:\"                                                   \n[21] \" based on Hessian \"                                             \n[22] \"\"                                                               \n[23] \"Error Analysis:\"                                                \n[24] \"        Estimate  Std. Error  t value Pr(>|t|)    \"             \n[25] \"mu     6.717e-04   1.104e-03    0.608 0.543012    \"             \n[26] \"omega  1.786e-05   8.781e-06    2.034 0.041985 *  \"             \n[27] \"alpha1 8.983e-02   1.531e-02    5.868 4.42e-09 ***\"             \n[28] \"beta1  3.783e-01   1.455e-01    2.600 0.009318 ** \"             \n[29] \"beta2  5.327e-01   1.373e-01    3.879 0.000105 ***\"             \n[30] \"---\"                                                            \n[31] \"Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\" \n[32] \"\"                                                               \n[33] \"Log Likelihood:\"                                                \n[34] \" 2313.141    normalized:  1.529855 \"                            \n[35] \"\"                                                               \n[36] \"Description:\"                                                   \n[37] \" Tue May  2 11:16:58 2023 by user: sleblanc \"                   \n[38] \"\"                                                               \n\n\nLet \\(x_t\\) be the time series data and \\(z_t\\) be the residuals of the ARIMA model. Then the GARCH(1, 2) model is defined by:\n\\[z_t = \\sigma_t * \\epsilon_t\\]\nwhere \\(\\epsilon_t\\) follows a standard normal distribution (mean = 0, sd = 1), and the conditional variance \\(\\sigma_t^2\\) is given by:\n\\[\\sigma_t^2 = \\omega + \\alpha_1 * z_(t-1)^2 + \\beta_1 * \\sigma_(t-1)^2 + \\beta_2 * \\sigma_(t-2)^2\\]\nwith the estimated coefficients being:\n\\(\\omega\\) = 0.00001667 \\(\\alpha_1\\) = 0.08827887 \\(\\beta_1\\) = 0.37861131 \\(\\beta_2\\) = 0.53446470 The model is fitted to the residuals of an ARIMA model (arima.res).\n\n\nForecast\n\npredict(final.fit, n.ahead = 100, plot=TRUE)\n\n\n\n\n    meanForecast  meanError standardDeviation lowerInterval upperInterval\n1   0.0006717395 0.06459287        0.06459287    -0.1259280     0.1272714\n2   0.0006717395 0.06608815        0.06608815    -0.1288587     0.1302021\n3   0.0006717395 0.06546048        0.06546048    -0.1276284     0.1289719\n4   0.0006717395 0.06595833        0.06595833    -0.1286042     0.1299477\n5   0.0006717395 0.06585706        0.06585706    -0.1284057     0.1297492\n6   0.0006717395 0.06607383        0.06607383    -0.1288306     0.1301741\n7   0.0006717395 0.06612133        0.06612133    -0.1289237     0.1302672\n8   0.0006717395 0.06625862        0.06625862    -0.1291928     0.1305362\n9   0.0006717395 0.06634801        0.06634801    -0.1293680     0.1307114\n10  0.0006717395 0.06646268        0.06646268    -0.1295927     0.1309362\n11  0.0006717395 0.06656374        0.06656374    -0.1297908     0.1311343\n12  0.0006717395 0.06667187        0.06667187    -0.1300027     0.1313462\n13  0.0006717395 0.06677607        0.06677607    -0.1302070     0.1315504\n14  0.0006717395 0.06688220        0.06688220    -0.1304150     0.1317584\n15  0.0006717395 0.06698713        0.06698713    -0.1306206     0.1319641\n16  0.0006717395 0.06709253        0.06709253    -0.1308272     0.1321707\n17  0.0006717395 0.06719752        0.06719752    -0.1310330     0.1323765\n18  0.0006717395 0.06730257        0.06730257    -0.1312389     0.1325824\n19  0.0006717395 0.06740742        0.06740742    -0.1314444     0.1327879\n20  0.0006717395 0.06751221        0.06751221    -0.1316498     0.1329932\n21  0.0006717395 0.06761688        0.06761688    -0.1318549     0.1331984\n22  0.0006717395 0.06772145        0.06772145    -0.1320599     0.1334033\n23  0.0006717395 0.06782591        0.06782591    -0.1322646     0.1336081\n24  0.0006717395 0.06793027        0.06793027    -0.1324691     0.1338126\n25  0.0006717395 0.06803452        0.06803452    -0.1326735     0.1340170\n26  0.0006717395 0.06813868        0.06813868    -0.1328776     0.1342211\n27  0.0006717395 0.06824273        0.06824273    -0.1330815     0.1344250\n28  0.0006717395 0.06834668        0.06834668    -0.1332853     0.1346288\n29  0.0006717395 0.06845052        0.06845052    -0.1334888     0.1348323\n30  0.0006717395 0.06855427        0.06855427    -0.1336922     0.1350356\n31  0.0006717395 0.06865792        0.06865792    -0.1338953     0.1352388\n32  0.0006717395 0.06876146        0.06876146    -0.1340983     0.1354417\n33  0.0006717395 0.06886491        0.06886491    -0.1343010     0.1356445\n34  0.0006717395 0.06896826        0.06896826    -0.1345036     0.1358470\n35  0.0006717395 0.06907151        0.06907151    -0.1347059     0.1360494\n36  0.0006717395 0.06917466        0.06917466    -0.1349081     0.1362516\n37  0.0006717395 0.06927772        0.06927772    -0.1351101     0.1364536\n38  0.0006717395 0.06938068        0.06938068    -0.1353119     0.1366554\n39  0.0006717395 0.06948354        0.06948354    -0.1355135     0.1368570\n40  0.0006717395 0.06958631        0.06958631    -0.1357149     0.1370584\n41  0.0006717395 0.06968898        0.06968898    -0.1359161     0.1372596\n42  0.0006717395 0.06979155        0.06979155    -0.1361172     0.1374607\n43  0.0006717395 0.06989403        0.06989403    -0.1363181     0.1376615\n44  0.0006717395 0.06999642        0.06999642    -0.1365187     0.1378622\n45  0.0006717395 0.07009871        0.07009871    -0.1367192     0.1380627\n46  0.0006717395 0.07020091        0.07020091    -0.1369195     0.1382630\n47  0.0006717395 0.07030302        0.07030302    -0.1371197     0.1384631\n48  0.0006717395 0.07040504        0.07040504    -0.1373196     0.1386631\n49  0.0006717395 0.07050696        0.07050696    -0.1375194     0.1388628\n50  0.0006717395 0.07060879        0.07060879    -0.1377189     0.1390624\n51  0.0006717395 0.07071053        0.07071053    -0.1379184     0.1392618\n52  0.0006717395 0.07081218        0.07081218    -0.1381176     0.1394611\n53  0.0006717395 0.07091374        0.07091374    -0.1383166     0.1396601\n54  0.0006717395 0.07101521        0.07101521    -0.1385155     0.1398590\n55  0.0006717395 0.07111659        0.07111659    -0.1387142     0.1400577\n56  0.0006717395 0.07121788        0.07121788    -0.1389127     0.1402562\n57  0.0006717395 0.07131908        0.07131908    -0.1391111     0.1404546\n58  0.0006717395 0.07142019        0.07142019    -0.1393093     0.1406527\n59  0.0006717395 0.07152122        0.07152122    -0.1395073     0.1408508\n60  0.0006717395 0.07162216        0.07162216    -0.1397051     0.1410486\n61  0.0006717395 0.07172301        0.07172301    -0.1399028     0.1412463\n62  0.0006717395 0.07182377        0.07182377    -0.1401003     0.1414437\n63  0.0006717395 0.07192445        0.07192445    -0.1402976     0.1416411\n64  0.0006717395 0.07202504        0.07202504    -0.1404948     0.1418382\n65  0.0006717395 0.07212555        0.07212555    -0.1406917     0.1420352\n66  0.0006717395 0.07222597        0.07222597    -0.1408886     0.1422320\n67  0.0006717395 0.07232631        0.07232631    -0.1410852     0.1424287\n68  0.0006717395 0.07242656        0.07242656    -0.1412817     0.1426252\n69  0.0006717395 0.07252673        0.07252673    -0.1414780     0.1428215\n70  0.0006717395 0.07262682        0.07262682    -0.1416742     0.1430177\n71  0.0006717395 0.07272682        0.07272682    -0.1418702     0.1432137\n72  0.0006717395 0.07282674        0.07282674    -0.1420660     0.1434095\n73  0.0006717395 0.07292658        0.07292658    -0.1422617     0.1436052\n74  0.0006717395 0.07302633        0.07302633    -0.1424572     0.1438007\n75  0.0006717395 0.07312601        0.07312601    -0.1426526     0.1439961\n76  0.0006717395 0.07322560        0.07322560    -0.1428478     0.1441913\n77  0.0006717395 0.07332511        0.07332511    -0.1430428     0.1443863\n78  0.0006717395 0.07342454        0.07342454    -0.1432377     0.1445812\n79  0.0006717395 0.07352389        0.07352389    -0.1434324     0.1447759\n80  0.0006717395 0.07362316        0.07362316    -0.1436270     0.1449705\n81  0.0006717395 0.07372235        0.07372235    -0.1438214     0.1451649\n82  0.0006717395 0.07382146        0.07382146    -0.1440157     0.1453592\n83  0.0006717395 0.07392050        0.07392050    -0.1442098     0.1455533\n84  0.0006717395 0.07401945        0.07401945    -0.1444037     0.1457472\n85  0.0006717395 0.07411833        0.07411833    -0.1445975     0.1459410\n86  0.0006717395 0.07421713        0.07421713    -0.1447912     0.1461346\n87  0.0006717395 0.07431585        0.07431585    -0.1449846     0.1463281\n88  0.0006717395 0.07441449        0.07441449    -0.1451780     0.1465215\n89  0.0006717395 0.07451306        0.07451306    -0.1453712     0.1467146\n90  0.0006717395 0.07461155        0.07461155    -0.1455642     0.1469077\n91  0.0006717395 0.07470996        0.07470996    -0.1457571     0.1471006\n92  0.0006717395 0.07480830        0.07480830    -0.1459498     0.1472933\n93  0.0006717395 0.07490656        0.07490656    -0.1461424     0.1474859\n94  0.0006717395 0.07500475        0.07500475    -0.1463349     0.1476784\n95  0.0006717395 0.07510287        0.07510287    -0.1465272     0.1478707\n96  0.0006717395 0.07520090        0.07520090    -0.1467193     0.1480628\n97  0.0006717395 0.07529887        0.07529887    -0.1469113     0.1482548\n98  0.0006717395 0.07539676        0.07539676    -0.1471032     0.1484467\n99  0.0006717395 0.07549458        0.07549458    -0.1472949     0.1486384\n100 0.0006717395 0.07559232        0.07559232    -0.1474865     0.1488300"
  },
  {
    "objectID": "ftsm.html#volatality-plot",
    "href": "ftsm.html#volatality-plot",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volatality plot",
    "text": "Volatality plot\nFinally, we should also consider the volatility of the data. Volatility is a key feature of financial time series data and will have large effect on the model we end up choosing. Below is the plot of the volatility of the Carvana data. When looking at the volatility plot, we see several large spikes that stick out. The first is in March 2020, which was right at the beginning of COVID, so the volatility is expected. The second large spike occurs at the send of 2022, which is unexpected because the value of the stock is so low at that point. However, it could be thatwith such a low stock price, relative changes have a greater effect. A $10 gain when the stock is at $10 is a 100% increase, but when the stock was at $400 this would be just a 2.5% gain. So it is not crazy that the stock because much more volatile as they value fell off a cliff in 2022.\n\nht = final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\nCVNA=data.frame(CVNA)\nCVNA = data.frame(CVNA,rownames(CVNA))\ncolnames(CVNA)[7] = \"date\"\nCVNA$date=as.Date(CVNA$date,\"%Y-%m-%d\")\nstr(CVNA)\n\n'data.frame':   1512 obs. of  7 variables:\n $ CVNA.Open    : num  13.5 11.6 10.95 10.18 8.59 ...\n $ CVNA.High    : num  13.9 11.7 10.9 10.2 10 ...\n $ CVNA.Low     : num  10.7 10.7 10 8.18 8.14 9.75 10.5 10.4 9.61 10.4 ...\n $ CVNA.Close   : num  11.1 10.77 10.1 8.72 9.98 ...\n $ CVNA.Volume  : num  11297800 1291300 991500 3356500 1840200 ...\n $ CVNA.Adjusted: num  11.1 10.77 10.1 8.72 9.98 ...\n $ date         : Date, format: \"2017-04-28\" \"2017-05-01\" ...\n\ndata= data.frame(ht,CVNA$date)\nggplot(data, aes(y = ht, x = CVNA.date)) + geom_line(col = '#009933') + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatality plot of Carvana Stock\")"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html",
    "href": "greenhouse_gases_deep_learning.html",
    "title": "AN560: Sam LeBlanc",
    "section": "",
    "text": "Deep learning, which is a subset of machine learning, uses artificial neural networks to simulate human decision-making. These networks are composed of layers of interconnected nodes or “neurons” that process input data and generate output. Deep learning and all of its components are incredibly complex, and I cannot exaplin them all here. If you are interested in learning more, IBM provides a great explanation for sata-savvy folks that doesn’t get too lost in the weeds.Deep learning, a subset of machine learning, employs artificial neural networks to mimic human decision-making. These networks consist of layers of interconnected nodes or “neurons” that process input data and generate output.\nDeep learning is especially effective with large, unstructured datasets, and although it demands substantial computational power, it’s capable of producing highly accurate models. Thanks to tools like TensorFlow and Keras, deep learning has become more accessible, making it easier to design, train, and deploy models. Deep learning, with its intricate components, is an intricate field, and a comprehensive explanation is beyond this page’s scope. For those interested in diving deeper, IBM offers an excellent resource that provides a solid understanding without getting overly technical.\nHere are some of the critical elements of deep learning leveraged in the following models include neural networks, weights and biases, activation functions, backpropogation and gradient descent. In the models below, I will make specirfic note of when each of these elements is being utilized.\nNeural Networks: These are inspired by the human brain’s structure, with networks transforming input data through layers of interconnected nodes. Each layer refines the input it receives, gradually building up complex patterns and associations much like our brain does when processing information.\nWeights and Biases: These are parameters that are fine-tuned during the learning process. They determine the significance of inputs and play a pivotal role in the accuracy of the model’s predictions. Think of them as the factors that determine how much importance should be given to each input when making a prediction.\nActivation Functions: These functions dictate whether a neuron should be activated based on its inputs. They’re like the gatekeepers of information that decide whether the input they receive is relevant enough to be passed on to the next layer.\nBackpropagation and Gradient Descent: These are techniques used to adjust weights and biases and minimize the error function. They are the backbone of learning in neural networks, allowing the model to learn from its mistakes and improve over time.\nLeveraging the same univariate time-series data that was utilized in the ARMA/ARIMA models, I successfully implemented and trained three distinct neural network models using Keras. These models included a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and a Long Short-Term Memory (LSTM) model. A partition of the data was set aside for training and validation purposes, ensuring an unbiased evaluation of the model performance. Furthermore, to mitigate the risk of overfitting, I incorporated regularization techniques into the model’s architecture. Consequently, these models now provide a robust framework for making future predictions.\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN,LSTM,GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import regularizers\n\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#import-and-prepare-data",
    "href": "greenhouse_gases_deep_learning.html#import-and-prepare-data",
    "title": "AN560: Sam LeBlanc",
    "section": "Import and Prepare Data",
    "text": "Import and Prepare Data\n\ndf = pd.read_csv('co2.csv')[['year','month','average']]\nprint(df)\n\n     year  month  average\n0    1958      3   315.70\n1    1958      4   317.45\n2    1958      5   317.51\n3    1958      6   317.24\n4    1958      7   315.86\n..    ...    ...      ...\n773  2022      8   417.19\n774  2022      9   415.95\n775  2022     10   415.78\n776  2022     11   417.51\n777  2022     12   418.95\n\n[778 rows x 3 columns]\n\n\n\nConvert features to Numpy Array\n\nX = np.array(df[\"average\"].values.astype('float32')).reshape(df.shape[0],1)\nprint(X.shape)\n\n(778, 1)\n\n\n\n\nVisualize Raw Data\n\ndef plotly_line_plot(t,y,title=\"Plot\", x_label=\"Months since March 1958\", y_label=\"CO2 Concentration\"):\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n    for i in range(1,len(y)):\n        if len(t[i])==1: \n            fig.add_scatter(x=t[i],y=y[i])\n        else: \n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n    fig.update_layout(xaxis_title=x_label, yaxis_title=y_label, template=\"plotly_white\", showlegend=False)\n    fig.show()\n\n\nplotly_line_plot([[*range(0,len(X))]],[X[:,0]],title=\"Atmospheric CO2 Concentration since March 1958\")\n\n\n                                                \n\n\n\n\nPreform Train-Test Split\n\ndef get_train_test(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1)) # apply min-max scalar\n    data = scaler.fit_transform(data).flatten()\n    split = int(len(data)*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\ntrain_data, test_data, data = get_train_test(X)\nprint('Train Data:',train_data.shape)\nprint('Test Data:',test_data.shape)\n\nTrain Data: (622,)\nTest Data: (156,)\n\n\n\n\nVisualize Train-Test Split\n\nt1=[*range(0,len(train_data))]\nt2=len(train_data)+np.array([*range(0,len(test_data))])\nplotly_line_plot([t1,t2],[train_data,test_data],title=\"Atmospheric CO2 Concentration since March 1958: Train-Test Split\")\n\n\n                                                \n\n\n\n\nRe-format Data for Deep Learning\n\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind, X, Y_ind, Y\n\n    Y_ind = np.arange(time_steps, len(dat), time_steps);\n    Y = dat[Y_ind]\n\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    \n    del X_ind[::time_steps]\n    X = dat[X_ind]; \n\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n\n    return X1, Y\n\n\np=12 # seasonal lag\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\nprint('Train Data:',testX.shape,testY.shape)\nprint('Test Data:',trainX.shape,trainY.shape)\n\nTrain Data: (12, 11, 1) (12,)\nTest Data: (51, 11, 1) (51,)\n\n\n\n\nVisualize\n\ntmp1=[]; \ntmp2=[]; \ntmp3=[]; \ncount=0\n\nfor i in range(0,trainX.shape[0]):\n    tmp1.append(count+np.array([*range(0,trainX[i,:,0].shape[0])]))\n    tmp1.append([count+trainX[i,:,0].shape[0]]);\n    tmp2.append(trainX[i,:,0])\n    tmp2.append([trainY[i]]);\n    count+=trainX[i,:,0].shape[0]+1\n\nplotly_line_plot(tmp1,tmp2,title=\"Atmospheric CO2 Concentration since March 1958: Training Points\")"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#create-models",
    "href": "greenhouse_gases_deep_learning.html#create-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Create Models",
    "text": "Create Models\n\nModel and Training Parameters\n\nrecurrent_hidden_units=3\nepochs=60\nf_batch=0.2 \noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n3 Models: LSTM, SimpleRNN, and GRU\n\nmod_lstm = Sequential()\nmod_lstm.add(\n    LSTM(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_lstm.add(Dense(units=1, activation='linear'))\nmod_lstm.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_lstm.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 3)                 60        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmod_srnn = Sequential()\nmod_srnn.add(\n    SimpleRNN(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_srnn.add(Dense(units=1, activation='linear'))\nmod_srnn.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_srnn.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n                                                                 \n dense_1 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmod_gru = Sequential()\nmod_gru.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1],trainX.shape[2]), \n        recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-1),\n        activation='tanh'\n    )\n) \n     \nmod_gru.add(Dense(units=1, activation='linear'))\nmod_gru.compile(loss='MeanSquaredError', optimizer=optimizer)\nmod_gru.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 3)                 54        \n                                                                 \n dense_2 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#train-models",
    "href": "greenhouse_gases_deep_learning.html#train-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Train Models",
    "text": "Train Models\n\nmodels = {\n    'LSTM' : mod_lstm,\n    'SRNN' : mod_srnn,\n    'GRU' : mod_gru\n}\n\nhistories = {\n    'LSTM' : None,\n    'SRNN' : None,\n    'GRU' : None\n}\n\nfor mod in models.keys():\n    model = models[mod]\n    history = model.fit(\n        trainX, trainY, \n        epochs=epochs, \n        batch_size=int(f_batch*trainX.shape[0]), \n        validation_split=validation_split,\n        verbose=0\n    )\n    histories[mod] = history"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#compare-models",
    "href": "greenhouse_gases_deep_learning.html#compare-models",
    "title": "AN560: Sam LeBlanc",
    "section": "Compare Models",
    "text": "Compare Models\n\ntrain_predictions = {}\ntest_predictions = {}\n\nfor mod in models.keys():\n    model = models[mod]\n    train_predict = model.predict(trainX).squeeze()\n    test_predict = model.predict(testX).squeeze()\n    train_predictions[mod] = train_predict\n    test_predictions[mod] = test_predict\n\n2/2 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 22ms/step\n2/2 [==============================] - 0s 0s/step\n1/1 [==============================] - 0s 30ms/step\n2/2 [==============================] - 0s 0s/step\n1/1 [==============================] - 0s 24ms/step\n\n\n\nCompute RMSE\n\ndef calculate_rsme(model_name, trainY, testY, train_predict, test_predict):\n    print(f'\\n{model_name}:\\n')\n    train_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\n    test_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n    print('Train MSE = %.5f. RMSE = %.5f' % (train_rmse**2.0,train_rmse))\n    print('Test MSE = %.5f. RMSE = %.5f' % (test_rmse**2.0,test_rmse))\n\n\ncalculate_rsme('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\nLSTM:\n\nTrain MSE = 0.00021. RMSE = 0.01450\nTest MSE = 0.01208. RMSE = 0.10992\n\n\n\ncalculate_rsme('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\nSRNN:\n\nTrain MSE = 0.01732. RMSE = 0.13161\nTest MSE = 0.22555. RMSE = 0.47493\n\n\n\ncalculate_rsme('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])\n\n\nGRU:\n\nTrain MSE = 0.00170. RMSE = 0.04129\nTest MSE = 0.02943. RMSE = 0.17155\n\n\n\n\nLoss by Epoch\n\ndef loss_by_epoch(model_name, epochs_steps, history):\n    plotly_line_plot(\n        [epochs_steps, epochs_steps],\n        [history.history['loss'],\n         history.history['val_loss']],\n        title=f\"{model_name}: Loss by Training Epoch\",\n        x_label=\"Training Epochs\",\n        y_label=\"Loss (MSE)\"\n    )\n\n\nepochs_steps = [*range(0, len(histories['LSTM'].history['loss']))]\nloss_by_epoch('LSTM', epochs_steps, histories['LSTM'])\n\n\n                                                \n\n\n\nepochs_steps = [*range(0, len(histories['SRNN'].history['loss']))]\nloss_by_epoch('SRNN', epochs_steps, histories['SRNN'])\n\n\n                                                \n\n\n\nepochs_steps = [*range(0, len(histories['GRU'].history['loss']))]\nloss_by_epoch('GRU', epochs_steps, histories['GRU'])\n\n\n                                                \n\n\n\n\nParity Plot\n\ndef parity_plot(model_name, trainY, testY, train_predict, test_predict):\n    fig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\n    fig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\n    fig.add_scatter(x=trainY,y=trainY, mode='lines')\n\n    fig.update_layout(\n        title=f\"{model_name}: Predicted vs True Y-Values\",\n        xaxis_title=\"Predicted Y-Value\",\n        yaxis_title=\"True Y-Value\",\n        template=\"plotly_white\",\n        showlegend=False\n    )\n\n    fig.show()\n\n\nparity_plot('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\n                                                \n\n\n\nparity_plot('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\n                                                \n\n\n\nparity_plot('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])\n\n\n                                                \n\n\n\n\nPrediction Result\n\ndef plot_result(model_name, trainY, testY, train_predict, test_predict):\n    train_len = np.arange(len(trainY))\n    test_len = np.arange(len(trainY), len(trainY) + len(testY))\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=train_len, y=trainY,mode='lines+markers',name='Training Data'))\n    fig.add_trace(go.Scatter(x=test_len, y=testY,mode='lines+markers',name='Test Data'))\n    fig.add_trace(go.Scatter(x=train_len, y=train_predict,mode='lines+markers',name='Training Predictions'))\n    fig.add_trace(go.Scatter(x=test_len, y=test_predict,mode='lines+markers',name='Test Predictions'))\n\n    fig.update_layout(title_text=f'{model_name}: Actual and Predicted Values',\n                      xaxis_title='Observation Number',\n                      yaxis_title='CO2 Concentration (Scaled)',\n                      template=\"plotly_white\")\n    fig.show()\n\n\nplot_result('LSTM', trainY, testY, train_predictions['LSTM'], test_predictions['LSTM'])\n\n\n                                                \n\n\n\nplot_result('SRNN', trainY, testY, train_predictions['SRNN'], test_predictions['SRNN'])\n\n\n                                                \n\n\n\nplot_result('GRU', trainY, testY, train_predictions['GRU'], test_predictions['GRU'])"
  },
  {
    "objectID": "greenhouse_gases_deep_learning.html#analysis-questions",
    "href": "greenhouse_gases_deep_learning.html#analysis-questions",
    "title": "AN560: Sam LeBlanc",
    "section": "Analysis Questions",
    "text": "Analysis Questions\n\nHow do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power\nIn this section, we went on the ardous journey of creating three different Artificial Neural Networks (ANNs) - a Simple Recurrent Neural Network (SRNN), a Long Short-Term Memory (LSTM), and a Gated Recurrent Unit (GRU). Our objective was to model and predict the atmospheric \\(CO_2\\) concentration over the next decade. The neural nets harness the auto-correlation of the time-series data, refining the observations to align with the 12-month seasonal lag. Essentially, we used data from the same month (e.g., May) across multiple years to predict what the CO2 concentration would look like in the subsequent Mays. This strategy delivered predictions for the atmospheric CO2 levels for the next ten years (Mays), extending until 2033.\nWhen it comes to comparing the performance of these models, it’s a bit of a mixed bag. There was a notable variation across different runs of the code, with either the LSTM or GRU models showing adequate to good performance. Interestingly, the performance of these two models was typically quite close on any given run, while the SRNN consistently trailed behind in terms of results. In the final run of the code, the LSTM model was the best model, with the GRU model close behind, and the SRNN rounding out the three. This performance hierarchy was evident across various evaluation methods, including Root Mean Squared Error (RMSE), Loss by Epoch, Parity Plots, and Predictive Results. In the end, the LSTM model emerged as the top model in accurately predicting CO2 concentrations. However, it’s important to remember that this was just the final run. In other iterations, the GRU model demonstrated similar prowess to the LSTM model. Thus, while we can glean some insights from this exercise, the dynamic nature of these models means that the ‘best’ model can vary from one run to the next.\n\n\nWhat effect does including regularization have on your results.\nRegularization had a subtle yet noticeable impact on the precision of our modes. As mentioned earlier, the accuracy of different models exhibited considerable variance across different runs, making it a challenge to definitively quantify the influence of regularization. However, generally speaking, the models’ performance seemed to dip slightly when regularization was omitted.\nI surmise that the modest effect of regularization can be attributed to the relatively stable scale of the data throughout the time series. In 1958, the atmospheric CO2 concentration was 320 ppm, which rose to 420 ppm by 2023. While this increase is alarmingly significant from an environmental perspective, it’s not such a drastic shift that would significantly amplify the impact of regularization in our models. Over the span of these years, the increase in CO2 concentration amounts to around 35%.\nRegularization often proves to be particularly beneficial in scenarios where the data exhibits substantial scale disparity either within a single dataset or when combined with another dataset. In our case, given the relatively modest scale difference in the CO2 concentration data, the effect of regularization was understandably muted. Nonetheless, its influence, albeit small, contributed to the overall performance increase of our models when using regularization.\n\n\nHow far into the future can the deep learning model accurately predict the future.\nDetermining the precise extent up to which our deep learning models can accurately predict is a challenging task, due in part to the model’s performance fluctuations across multiple code runs. However, a general observation is that the predictions tend to demonstrate internal consistency - if the model is correctly predicting values up to a certain point, it is likely to continue to do so, but not always. Interestingly, in the case of the LSTM model, the training predictions mirrored the actual values up to a specific point, beyond which it began to underestimate the true values. This underestimation could be attributed to the accelerating rate of carbon dioxide emissions in the atmosphere. The model seems to predict a linear continuation of the trend, but the reality has surpassed this linear progression, indicating an accelerating greenhouse gas phenomenon. This acceleration is a grave concern for climate researchers and has been the focus of numerous studies. Therefore, while the predictions demonstrate a consistency with their own trends, it does not necessarily equate to accuracy with the real-world data. The LSTM model’s performance is indicative of this: minor inaccuracies can accumulate over time, leading to a model that eventually falls short in accurately predicting CO2 concentration towards the end of the test data period. It serves as a reminder that these models must be periodically recalibrated and retrained to adapt to the changing trends in our ever-dynamic climate.\n\n\nHow does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\nThe deep learning models, specifically the LSTM and GRU models, significantly outperformed the traditional single-variable time-series ARMA/ARIMA models in predicting atmospheric CO2 concentrations. Initially, I applied the ARMA/ARIMA models to SF6 concentration data, as it did not exhibit seasonality. However, upon revisiting the analysis with CO2 concentration data, the deep learning models demonstrated superior performance.\nThe best CO2 models using the ARMA/ARIMA methods achieved an RMSE of 0.6, while the LSTM model had an RMSE of 0.1 and the GRU model an RMSE of 0.17 for the test data. It is important to note that training ARIMA and deep learning models is a complex and challenging task. It is highly probable that alternative hyperparameter configurations for both methods could yield more accurate predictions. Nonetheless, based on the models we have, it is evident that the deep learning models are substantially more effective in modeling CO2 data and making accurate predictions.\n\n\nCompare your models (use RMSE) and forecasts from these sections with your Deep Learning Models: ARIMA/SARIMA/VAR\nWhen comparing the deep learning models with the SARIMA model, the outcomes echo those of the ARIMA model. The deep learning models, specifically LSTM and GRU, substantially outperform the traditional time series models in modeling and predicting CO2 concentrations. While the SARIMA model did present improved performance over ARIMA, its best RMSE was 0.4, compared to the LSTM model with an RMSE of 0.1, and the GRU model with an RMSE of 0.17 for the test data. Just like with the ARIMA model, the SARIMA model’s performance may have been hindered by the lack of extensive hyperparameter tuning. It’s challenging to definitively state which model is superior without further optimization. Nonetheless, the preliminary results indicate that the deep learning models significantly outperform the traditional time series models.\nIn the VAR section, I employed data from four greenhouse gases to construct a multivariate model, aiming to predict future concentrations of all four gases concurrently. I did my best to implement a deep learning model for this task but encountered challenges during the data wrangling stage. As Professor James suggested, the most challenging aspect of the deep learning process (and data science in general) is ensuring the data is appropriately prepared and in a format that TensorFlow likes. I am still working on the multivariate deep learning model and aim to have it ready for the final deliverable. Comparing just the prediciting of the CO2 concentration from the VAR model, we find the same thing as the SARIMA model. The VAR model’s best RSME for the CO2 is 0.4, which is not even close to the deep learning model."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to my (Sam LeBlanc, nice to meet you 👋) class portfolio for ANLY 560 - Time Series Analysis at Georgetown University. The course is an in-depth exploration of time series data analysis, and provides a critical approach for using statistics and machine to learn about sequential time data, which holds unique challenges due to the inherent correlation of adjacent observations. If that sounds like a lot, don’t worry, it is 🙃. This project is titled, Exploring Atmospheric Concentrations of Greenhouse Gases with Time Series Analysis, and I will let you (dear reader) figure out what that means…"
  },
  {
    "objectID": "intro.html#what-is-a-time-series",
    "href": "intro.html#what-is-a-time-series",
    "title": "Introduction",
    "section": "What is a Time Series?",
    "text": "What is a Time Series?\n\nAny metric that is consistently measured over regular time intervals. Some examples of time series data include: weather, stocks, industry forecasts, traffic, and energy prices.\nWhat makes a time series special of the auto-correlation in the data. Successive observations are not independent as they strictly rely on data from directly before the observation.\nThis can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "intro.html#what-are-greenhouse-gases",
    "href": "intro.html#what-are-greenhouse-gases",
    "title": "Introduction",
    "section": "What are Greenhouse Gases?",
    "text": "What are Greenhouse Gases?\n\nNature and Composition: Greenhouse gases are atmospheric gases that have the capacity to absorb and emit infrared radiation, contributing to the greenhouse effect. Principal greenhouse gases include carbon dioxide (\\(CO_2\\)), methane (\\(CH_4\\)), nitrous oxide (\\(N2_O\\)), and fluorinated gases such as sulfur hexafluoride (\\(SF_6\\)).\nRole in Climate Change: These gases play a critical role in the warming of our planet. They trap heat in the Earth’s atmosphere by preventing it from escaping back into space, causing global temperatures to rise—a phenomenon known as global warming. This is a major driver of the ongoing changes in our global climate.\nSources: Greenhouse gases are released from both natural and human-made sources. Natural sources include decomposition, respiration, ocean release, and volcanic eruptions. Human activities, such as burning fossil fuels for electricity, heat, and transportation, deforestation, and industrial processes, significantly contribute to the increase in greenhouse gases in the atmosphere.\n\n\n\n\n\nExample of several important greenhouse gases.\n\n\n\n\n\n\nDiagram of the (natural) Greenhouse Effect. This image does not account for additional anthropogenic greenhouses gas emissions."
  },
  {
    "objectID": "intro.html#my-project",
    "href": "intro.html#my-project",
    "title": "Introduction",
    "section": "My Project",
    "text": "My Project\nThis semester, my project revolves around analyzing the atmospheric concentrations of greenhouse gases over time. Using the tools and techniques I’ve learned in ANLY 560, I’ll be investigating why these concentrations are increasing and how this knowledge can be used to inform policy decisions and contribute to the fight against climate change.\nBased on data from NOAA’s Global Monitoring Lab, in 2020, even with decreased emissions due to the COVID-19 pandemic, the global average atmospheric carbon dioxide still managed to reach a new record high of 414.72 ppm (parts per million). This increase, the 5th-highest yearly increase since NOAA’s record began in 1958, shows the unabated rise in greenhouse gases. The main reason for the rise in carbon dioxide is the burning of fossil fuels, which has tripled from an average of 3 billion tons of carbon (11 billion tons of carbon dioxide) per year in the 1960s to 9.5 billion tons of carbon (35 billion tons of carbon dioxide) per year in the 2010s.\n\nWhile carbon dioxide often takes the spotlight in discussions about greenhouse gases, it’s crucial to remember that it’s not the only player. Methane, for example, which is heavily produced by livestock such as cows, has a larger effect on global warming. As part of this project, I will investigate the correlation between methane and carbon dioxide emissions and aim to raise awareness about the environmental impact of methane."
  },
  {
    "objectID": "intro.html#guiding-questions",
    "href": "intro.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions",
    "text": "Guiding Questions\nTo navigate through the complex world of greenhouse gases and their impact on our climate, I have outlined several guiding questions for this study:\n\nIs there a greater correlation between carbon-based greenhouse gases (carbon dioxide and methane) versus non-carbon based greenhouse gases (nitrous oxide and sulfur hexafluoride)?\nTo what degree is the trend of the four greenhouse gas concentrations correlated?\nTo what degree is the seasonality of the four greenhouse gas concentrations correlated?\nAre any of the greenhouse gas concentrations strongly or weakly stationary?\nWhat is the daily predictive accuracy of the four greenhouse gases?\nAre there significant differences in seasonal trends between the northern and southern hemispheres, or at the equator?\nIs there a correlation between the proximity of a station to large greenhouse gas emitters and the magnitude of seasonal variation in concentrations?\nHow do the temporal components of the data vary by station or region?\nTo what extent are greenhouse gas concentrations affected by meteorological conditions, specifically temperature?\nDo the data indicate any potential avenues of mitigation of climate change?\n\nThese questions will steer my exploration and analysis, as I endeavor to unravel the secrets that the time series data of atmospheric greenhouse gases hold. Join me on this fascinating journey, where every discovery brings us one step closer to understanding and mitigating the effects of climate change."
  },
  {
    "objectID": "intro.html#project-goals-and-structure",
    "href": "intro.html#project-goals-and-structure",
    "title": "Introduction",
    "section": "Project Goals and Structure",
    "text": "Project Goals and Structure\nThe overarching goal of this project is to utilize time series analysis to dissect the complex patterns in greenhouse gas concentrations over time, and to understand their implications on our climate. To accomplish this, the project will be structured into the following sections:\nData Sources: The foundation of any data analysis project is the data itself. I will begin by sourcing reliable and comprehensive datasets on greenhouse gas concentrations, from reputable sources such as NOAA’s Global Monitoring Lab.\nData Visualization: One of the most effective ways to understand data is to see it. Here, I’ll apply data storytelling techniques to create compelling and intuitive visual representations of the data, which will help us discern patterns, trends, and anomalies.\nExploratory Data Analysis: The initial step in any analysis is to explore the data. This involves checking for missing values, outliers, and making sure the data is clean and ready for further analysis.\nARMA/ARIMA/SARIMA Models: I’ll apply these models to understand the data’s underlying structure. These models can help identify if there’s an autoregressive or moving average process occurring, and whether there are seasonal components that need to be accounted for.\nARIMAX/SARIMAX/VAR: These models allow for the inclusion of exogenous variables, which can offer more context to the analysis. This can be particularly useful in understanding how different types of greenhouse gases interact with each other.\nFinancial Time Series Models (ARCH/GARCH): These models, traditionally used in financial forecasting, can be quite effective in modelling volatility, which may be relevant in examining fluctuations in greenhouse gas concentrations.\nDeep Learning for Time Series: Finally, I’ll employ deep learning techniques, which can capture complex patterns and relationships in the data that may be missed by traditional methods.\nThroughout each of these sections, the primary themes will be the impacts and implications of rising greenhouse gas concentrations, the role of human activities in these increases, and the potential avenues for mitigating climate change. By applying these techniques and maintaining these themes, I aim to generate insights that could inform policy decisions and contribute to the global conversation about climate change."
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Sam LeBlanc and I am an aspiring data scientist with a deep passion for public policy, social justice, and the transformative power of data! Currently, I am one week away from graduating from Georgetown University’s McCourt School of Public Policy with a Master’s degree in Data Science for Public Policy. My research is focused on the compelling intersection of data science and public policy, with a particular emphasis on issues related to climate and energy.\nI believe that data, when interpreted correctly, can reveal insights that drive meaningful social change. One area where this is especially crucial is in our understanding of climate change, and I am particularly interested in the role time series analysis can play in this context. The ability to track and analyze weather data over time, for instance, has vast implications for policy decisions, and I’m eager to contribute to this field.\nI am a firm advocate for data literacy. In an era increasingly defined by data, it is imperative that we not only understand how to interpret data, but also how to use it responsibly and effectively to inform discourse and decision-making. This is especially true for data communication inn climate science. This is a field where the top-echelon of researchers are incredibly data fluent, but most people in the green-climate industry are woefully inept at using data to their advantage."
  }
]